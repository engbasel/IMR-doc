# Ø´Ø±Ø­ ØªÙØµÙŠÙ„ÙŠ ÙˆØ¹Ù…ÙŠÙ‚ - Ø³Ø·Ø± Ø¨Ø³Ø·Ø± | Deep Line-by-Line Explanation

## ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | Project Structure Analysis

Ø¯Ø¹Ù†ÙŠ Ø£ÙˆÙ„Ø§Ù‹ Ø£ÙØªØ­ Ø§Ù„Ù…Ù„Ù ÙˆØ£ÙØ­Øµ Ù…Ø­ØªÙˆÙŠØ§ØªÙ‡ Ù„Ø£Ø¹Ø·ÙŠÙƒ Ø´Ø±Ø­Ø§Ù‹ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…ÙØµÙ„Ø§Ù‹.Ù…Ù…ØªØ§Ø²! Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠ ØµÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ù€ Notebook. Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­ Ù„Ùƒ ÙƒÙ„ Ø®Ù„ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ø§Ù„Ù…Ù…Ù„.

---

# ğŸ“Š Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ø§Ù„Ù…ØªØ¹Ù…Ù‚ | Deep Detailed Explanation

## ğŸ¯ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ | Overall Project Structure

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

Ù‡Ø°Ø§ Ø§Ù„Ù€ Notebook ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ø³Ø§Ø¨Ù‚Ø© **RSNA-MICCAI Brain Tumor Radiogenomic Classification**. Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ ØªØ­Ù„ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ± MRI Ù„Ù„Ø¯Ù…Ø§Øº Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø­Ø§Ù„Ø© Ø¬ÙŠÙ† MGMT (ÙˆÙ‡Ùˆ Ù…Ø¤Ø´Ø± Ù…Ù‡Ù… ÙÙŠ ØªØ´Ø®ÙŠØµ ÙˆØ¹Ù„Ø§Ø¬ Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø¯Ù…Ø§Øº).

**Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©:**
1. **Data Loading Layer**: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­ØªÙ‡Ø§
2. **Exploration Layer**: Ø§Ø³ØªÙƒØ´Ø§Ù Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
3. **Visualization Layer**: Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
4. **Preprocessing Layer**: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± (ØªØ·Ø¨ÙŠØ¹ØŒ Ù‚ØµØŒ ØªØºÙŠÙŠØ± Ø­Ø¬Ù…)
5. **Augmentation Layer**: Ø²ÙŠØ§Ø¯Ø© ØªÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

**âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹:**
Ù‡Ø°Ø§ Ø§Ù„Ù€ Notebook **Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ Ø£Ùˆ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ©**. Ø¥Ù†Ù‡ ÙŠØ±ÙƒØ² ÙÙ‚Ø· Ø¹Ù„Ù‰:
- Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (EDA - Exploratory Data Analysis)
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± (Image Preprocessing)
- ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©

**In English:**

This notebook deals with the **RSNA-MICCAI Brain Tumor Radiogenomic Classification** competition. The goal is to analyze and process brain MRI images to predict MGMT gene status (an important indicator in brain tumor diagnosis and treatment).

**Logical Layers:**
1. **Data Loading Layer**: Reading and validating data
2. **Exploration Layer**: Exploring data characteristics
3. **Visualization Layer**: Displaying images and statistics
4. **Preprocessing Layer**: Processing images (normalization, cropping, resizing)
5. **Augmentation Layer**: Increasing data variety

**âš ï¸ Very Important Note:**
This notebook **contains NO machine learning model or neural network**. It focuses only on:
- Exploratory Data Analysis (EDA)
- Image Preprocessing
- Preparing data for future modeling

---

## ğŸ“ Ø´Ø±Ø­ ÙƒÙ„ Ø®Ù„ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ | Cell-by-Cell Detailed Explanation

### âœ… Cell 1: ÙØ­Øµ GPU | GPU Check

```python
!nvidia-smi
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±ØŸ**
- ÙŠØ³ØªØ¯Ø¹ÙŠ Ø£Ø¯Ø§Ø© `nvidia-smi` (NVIDIA System Management Interface)
- ÙŠØ¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† GPU Ø§Ù„Ù…ØªØ§Ø­ ÙÙŠ Ø§Ù„Ø¬Ù‡Ø§Ø²

**Ù„Ù…Ø§Ø°Ø§ ÙŠÙˆØ¬Ø¯ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±ØŸ**
- Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† GPU Ù…ØªØ§Ø­ ÙˆÙŠØ¹Ù…Ù„
- Ù„Ù…Ø¹Ø±ÙØ© Ù†ÙˆØ¹ GPU ÙˆØ°Ø§ÙƒØ±ØªÙ‡ Ø§Ù„Ù…ØªØ§Ø­Ø©
- Ù…ÙÙŠØ¯ ÙÙŠ Kaggle Ù„Ø£Ù†Ù‡Ø§ ØªÙˆÙØ± GPU Ù…Ø¬Ø§Ù†ÙŠ

**Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø­Ø°ÙÙ†Ø§Ù‡ØŸ**
- Ù„Ù† ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙƒÙˆØ¯
- Ù„ÙƒÙ†Ù†Ø§ Ù„Ù† Ù†Ø¹Ø±Ù Ù…ÙˆØ§ØµÙØ§Øª GPU
- Ù‚Ø¯ Ù†ÙˆØ§Ø¬Ù‡ Ù…Ø´Ø§ÙƒÙ„ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù† GPU ØºÙŠØ± Ù…ØªØ§Ø­

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.xx       Driver Version: 450.xx       CUDA Version: 11.0    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

**Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©:**
- Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† GPU Ù…ØªØ§Ø­Ø§Ù‹ØŒ Ø³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø®Ø·Ø£ "command not found"
- ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ø¨Ø¯ÙˆÙ† GPUØŒ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU ÙÙ‚Ø·

**In English:**

**What does this line do?**
- Calls the `nvidia-smi` tool (NVIDIA System Management Interface)
- Displays information about available GPU

**Why does this line exist?**
- To verify that GPU is available and working
- To know GPU type and available memory
- Useful on Kaggle as it provides free GPU

**What if we remove it?**
- Won't affect the rest of the code
- But we won't know GPU specifications
- May face issues later if GPU is unavailable

**Expected output:**
Shows GPU name, memory, temperature, and utilization.

**Common errors:**
- If GPU is unavailable, you'll get "command not found" error
- In environments without GPU, must use CPU only

---

### âœ… Cell 2: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª | Import Libraries

```python
import os
import json
import glob
import random
import collections
from tqdm import tqdm

import numpy as np
import pandas as pd
import pydicom
from pydicom.pixel_data_handlers.util import apply_voi_lut
import cv2
import matplotlib.pyplot as plt
from matplotlib import animation, rc
import seaborn as sns
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

Ù‡Ø°Ù‡ Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ **Ø§Ù„Ø£Ø³Ø§Ø³** Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ ÙƒÙ„Ù‡. Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­ ÙƒÙ„ Ù…ÙƒØªØ¨Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„:

#### Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (System Libraries):

**1. `import os`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªÙˆÙØ± ÙˆØ¸Ø§Ø¦Ù Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** 
  - Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª (`os.listdir()`)
  - Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª (`os.path.join()`)
  - ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª (`os.path.getsize()`)
- **Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `os.listdir(TRAIN_DATA_PATH)` â†’ ÙŠØ¹Ø·ÙŠÙƒ Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰

**2. `import json`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª JSON
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© JSON
- **ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:** Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ù„Ø§Ø­ØªÙŠØ§Ø·

**3. `import glob`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªØ¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø¨Ù†Ù…Ø· Ù…Ø¹ÙŠÙ†
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª DICOM ÙÙŠ Ù…Ø¬Ù„Ø¯
- **Ù…Ø«Ø§Ù„:** `glob.glob('*.dcm')` â†’ ÙŠØ¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ†ØªÙ‡ÙŠ Ø¨Ù€ .dcm
- **âš ï¸ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹:** Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø£Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¶Ø®Ù… (350,000+ Ù…Ù„Ù)

**4. `import random`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªÙˆÙ„ÙŠØ¯ Ø£Ø±Ù‚Ø§Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ù€ Data Augmentation (ØªØ¯ÙˆÙŠØ± Ø§Ù„ØµÙˆØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹)
- **Ù…Ø«Ø§Ù„:** `random.randint(0, 3)` â†’ Ø±Ù‚Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ù† 0 Ø¥Ù„Ù‰ 3

**5. `import collections`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªÙˆÙØ± Ù‡ÙŠØ§ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù…ÙÙŠØ¯Ø© Ù„Ù€ Counter Ùˆ defaultdict

**6. `from tqdm import tqdm`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªØ¹Ø±Ø¶ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… (progress bar)
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ø¹Ù†Ø¯Ù…Ø§ Ù†Ø¹Ø§Ù„Ø¬ Ø¢Ù„Ø§Ù Ø§Ù„Ù…Ù„ÙØ§ØªØŒ Ù†Ø±ÙŠØ¯ Ø£Ù† Ù†Ø±Ù‰ Ø§Ù„ØªÙ‚Ø¯Ù…
- **Ù…Ø«Ø§Ù„:** `for i in tqdm(range(1000))` â†’ ÙŠØ¹Ø±Ø¶: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80%

#### Ù…ÙƒØªØ¨Ø§Øª Ø¹Ù„Ù…ÙŠØ© (Scientific Libraries):

**7. `import numpy as np`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ù…ÙƒØªØ¨Ø© Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ø§Ù„ØµÙˆØ± = Ù…ØµÙÙˆÙØ§Øª numpy
- **Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©:**
  - `np.array()` â†’ ØªØ­ÙˆÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ©
  - `np.mean()` â†’ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·
  - `np.max()` â†’ Ø£ÙƒØ¨Ø± Ù‚ÙŠÙ…Ø©
  - `np.min()` â†’ Ø£ØµØºØ± Ù‚ÙŠÙ…Ø©
  - `np.std()` â†’ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ

**8. `import pandas as pd`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ù…ÙƒØªØ¨Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ labels
- **Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©:**
  - `pd.read_csv()` â†’ Ù‚Ø±Ø§Ø¡Ø© CSV
  - `df.head()` â†’ Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ
  - `df['column'].value_counts()` â†’ Ø¹Ø¯ Ø§Ù„Ù‚ÙŠÙ…

#### Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© (Medical Imaging Libraries):

**9. `import pydicom`**
- **â­ Ø£Ù‡Ù… Ù…ÙƒØªØ¨Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹!**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª DICOM (ØµÙŠØºØ© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©)
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** ÙƒÙ„ ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨ØµÙŠØºØ© .dcm (DICOM)
- **Ù…Ø§ Ù‡Ùˆ DICOMØŸ**
  - Digital Imaging and Communications in Medicine
  - ØµÙŠØºØ© Ù‚ÙŠØ§Ø³ÙŠØ© Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©
  - ØªØ­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© + Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (metadata)

**10. `from pydicom.pixel_data_handlers.util import apply_voi_lut`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** ØªØ·Ø¨Ù‚ Value of Interest Look-Up Table
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„ØªØ­ÙˆÙŠÙ„ Ù‚ÙŠÙ… Ø§Ù„Ø¨ÙƒØ³Ù„ Ø¥Ù„Ù‰ Ù†Ø·Ø§Ù‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ø±Ø¶
- **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

#### Ù…ÙƒØªØ¨Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± (Image Processing Libraries):

**11. `import cv2`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** OpenCV - Ù…ÙƒØªØ¨Ø© Ù‚ÙˆÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ù€:
  - `cv2.resize()` â†’ ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±
  - `cv2.rotate()` â†’ ØªØ¯ÙˆÙŠØ± Ø§Ù„ØµÙˆØ±
  - `cv2.INTER_AREA` â†’ Ù†ÙˆØ¹ interpolation Ø¹Ù†Ø¯ ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
- **âš ï¸ Ù…Ù‡Ù…:** OpenCV ÙŠÙ‚Ø±Ø£ Ø§Ù„ØµÙˆØ± Ø¨ØµÙŠØºØ© BGR ÙˆÙ„ÙŠØ³ RGB

#### Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØµÙˆØ± (Visualization Libraries):

**12. `import matplotlib.pyplot as plt`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙˆØ¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ø¹Ø±Ø¶ ØµÙˆØ± MRI ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
- **Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª:**
  - `plt.imshow()` â†’ Ø¹Ø±Ø¶ ØµÙˆØ±Ø©
  - `plt.hist()` â†’ Ø±Ø³Ù… histogram
  - `plt.figure()` â†’ Ø¥Ù†Ø´Ø§Ø¡ figure Ø¬Ø¯ÙŠØ¯

**13. `from matplotlib import animation, rc`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³ÙˆÙ… Ù…ØªØ­Ø±ÙƒØ©
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ slices ÙÙŠ MRI ÙƒÙ€ animation
- **Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `animation.FuncAnimation()` â†’ ÙŠÙ†Ø´Ø¦ GIF Ù…ØªØ­Ø±Ùƒ

**14. `import seaborn as sns`**
- **Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ** Ù…ÙƒØªØ¨Ø© ØªØµÙˆØ± Ø¥Ø­ØµØ§Ø¦ÙŠ Ù…ØªÙ‚Ø¯Ù…Ø©
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ØŸ** Ù„Ø±Ø³Ù… Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ø£Ø¬Ù…Ù„ Ù…Ù† matplotlib
- **Ø§Ø³ØªØ®Ø¯Ø§Ù…:** `sns.countplot()` â†’ Ø±Ø³Ù… Ø¹Ø¯Ø¯ ÙƒÙ„ ÙØ¦Ø©

**Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø­Ø°ÙÙ†Ø§ Ø£ÙŠ Ù…ÙƒØªØ¨Ø©ØŸ**
- Ø­Ø°Ù `numpy` â†’ âŒ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ„Ù‡ ÙŠØªØ¹Ø·Ù„ (Ø§Ù„ØµÙˆØ± = numpy arrays)
- Ø­Ø°Ù `pydicom` â†’ âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© DICOM
- Ø­Ø°Ù `cv2` â†’ âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±
- Ø­Ø°Ù `pandas` â†’ âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© labels
- Ø­Ø°Ù `matplotlib` â†’ âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±
- Ø­Ø°Ù `tqdm` â†’ âœ… ÙŠØ¹Ù…Ù„ Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† progress bar
- Ø­Ø°Ù `seaborn` â†’ âœ… ÙŠØ¹Ù…Ù„ Ù„ÙƒÙ† Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ø£Ù‚Ù„ Ø¬Ù…Ø§Ù„Ø§Ù‹

**In English:**

This cell is the **foundation** of the entire project. Let me explain each library in detail:

#### System Libraries:

**1. `import os`**
- **What does it do?** Provides functions for file system operations
- **Why do we need it?** 
  - Reading folder contents (`os.listdir()`)
  - Creating file paths (`os.path.join()`)
  - Checking file sizes (`os.path.getsize()`)

**2. `import json`**
- **What does it do?** Handles JSON files
- **Why do we need it?** Some data might be in JSON format

**3. `import glob`**
- **What does it do?** Searches for files with specific patterns
- **Why do we need it?** To find all DICOM files in a folder
- **Example:** `glob.glob('*.dcm')` â†’ gets all files ending with .dcm
- **âš ï¸ Very important:** Essential because there are 350,000+ files

**4. `import random`**
- **What does it do?** Generates random numbers
- **Why do we need it?** For Data Augmentation (rotating images randomly)

**5. `import collections`**
- **What does it do?** Provides additional data structures
- **Why do we need it?** Not explicitly used, but useful for Counter and defaultdict

**6. `from tqdm import tqdm`**
- **What does it do?** Displays progress bar
- **Why do we need it?** When processing thousands of files, we want to see progress

#### Scientific Libraries:

**7. `import numpy as np`**
- **What does it do?** Library for mathematical operations on arrays
- **Why do we need it?** Images = numpy arrays
- **Main uses:**
  - `np.array()` â†’ convert list to array
  - `np.mean()` â†’ calculate mean
  - `np.max()` â†’ maximum value
  - `np.min()` â†’ minimum value
  - `np.std()` â†’ standard deviation

**8. `import pandas as pd`**
- **What does it do?** Library for tabular data
- **Why do we need it?** To read CSV file containing labels

#### Medical Imaging Libraries:

**9. `import pydicom`**
- **â­ Most important library in the project!**
- **What does it do?** Reads DICOM files (medical image format)
- **Why do we need it?** Every image in the project is .dcm (DICOM) format
- **What is DICOM?**
  - Digital Imaging and Communications in Medicine
  - Global standard format for medical images
  - Stores image + additional metadata

**10. `from pydicom.pixel_data_handlers.util import apply_voi_lut`**
- **What does it do?** Applies Value of Interest Look-Up Table
- **Why do we need it?** To convert pixel values to appropriate display range
- **Note:** Not used in this code, but useful for advanced processing

#### Image Processing Libraries:

**11. `import cv2`**
- **What does it do?** OpenCV - powerful image processing library
- **Why do we need it?** For:
  - `cv2.resize()` â†’ resize images
  - `cv2.rotate()` â†’ rotate images
  - `cv2.INTER_AREA` â†’ interpolation type when resizing
- **âš ï¸ Important:** OpenCV reads images in BGR format, not RGB

#### Visualization Libraries:

**12. `import matplotlib.pyplot as plt`**
- **What does it do?** Plots charts and displays images
- **Why do we need it?** To display MRI images and statistics

**13. `from matplotlib import animation, rc`**
- **What does it do?** Creates animations
- **Why do we need it?** To display all MRI slices as animation

**14. `import seaborn as sns`**
- **What does it do?** Advanced statistical visualization library
- **Why do we need it?** To draw prettier charts than matplotlib

**What if we remove any library?**
- Remove `numpy` â†’ âŒ Entire code breaks (images = numpy arrays)
- Remove `pydicom` â†’ âŒ Cannot read DICOM
- Remove `cv2` â†’ âŒ Cannot process images
- Remove `pandas` â†’ âŒ Cannot read labels
- Remove `matplotlib` â†’ âŒ Cannot display images
- Remove `tqdm` â†’ âœ… Works but without progress bar
- Remove `seaborn` â†’ âœ… Works but less beautiful charts

---

### âœ… Cell 3-4: Ø¹Ù†Ø§ÙˆÙŠÙ† ØªÙˆØ¶ÙŠØ­ÙŠØ© | Markdown Headers

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**
Ø®Ù„Ø§ÙŠØ§ markdown Ø¨Ø³ÙŠØ·Ø© Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù€ Notebook:
- Cell 3: "## Data Display"
- Cell 4: ØªØ­Ø°ÙŠØ± Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø³Ø¯Ø©

**Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø§Ù„Ù…Ù‡Ù…Ø©:** Ù‡Ù†Ø§Ùƒ 3 Ø­Ø§Ù„Ø§Øª Ù…Ø±Ø¶Ù‰ Ø¨Ù‡Ø§ Ù…Ø´Ø§ÙƒÙ„:
- Patient ID: 00109
- Patient ID: 00123  
- Patient ID: 00709

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ**
- Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø³Ø¯Ø© Ø£Ùˆ ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø©
- ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ ÙŠÙ…Ù†Ø¹ errors Ù„Ø§Ø­Ù‚Ø§Ù‹

**In English:**
Simple markdown cells to organize the notebook:
- Cell 3: "## Data Display"
- Cell 4: Warning about corrupted data

**Important info:** There are 3 patient cases with issues:
- Patient ID: 00109
- Patient ID: 00123
- Patient ID: 00709

**Why is this important?**
- This data is corrupted or incomplete
- Must be excluded from training
- Ignoring them prevents later errors

---

### âœ… Cell 5-6: Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Labels | Reading Labels File

```python
# Cell 5
TRAIN_LABELS_PATH = "../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv"

# Cell 6
train_labels = pd.read_csv(TRAIN_LABELS_PATH)
train_labels
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 5: ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø±

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ**
- ÙŠÙ†Ø´Ø¦ Ù…ØªØºÙŠØ± Ù†ØµÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ù„Ù CSV

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù…ØªØºÙŠØ±ØŸ**
- **Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… `TRAIN_LABELS_PATH` ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ù…ØªØ¹Ø¯Ø¯Ø©
- **Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ¹Ø¯ÙŠÙ„:** Ø¥Ø°Ø§ ØªØºÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø±ØŒ Ù†Ø¹Ø¯Ù„ Ù…ÙƒØ§Ù† ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
- **Ø§Ù„ÙˆØ¶ÙˆØ­:** Ø§Ù„Ø§Ø³Ù… Ø¨Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ **Ø«Ø§Ø¨Øª** (constant)

**Ù…Ø§Ø°Ø§ Ù„Ùˆ ÙƒØªØ¨Ù†Ø§ Ø§Ù„Ù…Ø³Ø§Ø± Ù…Ø¨Ø§Ø´Ø±Ø©ØŸ**
```python
# Ø³ÙŠØ¡ âŒ
train_labels = pd.read_csv("../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv")
```
- ÙŠØ¹Ù…Ù„ØŒ Ù„ÙƒÙ† ØµØ¹Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
- Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ø§Ù„Ù…Ø³Ø§Ø± Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ù†ÙƒØªØ¨Ù‡ Ù…Ù† Ø¬Ø¯ÙŠØ¯
- Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©

#### Cell 6: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ `pd.read_csv()`ØŸ**

```python
train_labels = pd.read_csv(TRAIN_LABELS_PATH)
```

**Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**
1. ÙŠÙØªØ­ Ù…Ù„Ù CSV
2. ÙŠÙ‚Ø±Ø£ Ø§Ù„ØµÙÙˆÙ ÙˆØ§Ù„Ø£Ø¹Ù…Ø¯Ø©
3. ÙŠØ­ÙˆÙ„Ù‡Ø§ Ø¥Ù„Ù‰ DataFrame (Ø¬Ø¯ÙˆÙ„ pandas)
4. ÙŠØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ù…ØªØºÙŠØ± `train_labels`

**Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
   BraTS21ID  MGMT_value
0      00000           1
1      00002           1
2      00003           1
3      00005           0
...      ...         ...
```

**Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:**
- **BraTS21ID:** Ø±Ù‚Ù… ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø±ÙŠØ¶ (Ù…Ø«Ù„: 00000, 00002)
- **MGMT_value:** Ø§Ù„ØªØµÙ†ÙŠÙ (0 Ø£Ùˆ 1)
  - 0 = MGMT promoter **not methylated** (Ø³Ù„Ø¨ÙŠ)
  - 1 = MGMT promoter **methylated** (Ø¥ÙŠØ¬Ø§Ø¨ÙŠ)

**Ù…Ø§ Ù‡Ùˆ MGMTØŸ**
- MGMT = O6-Methylguanine-DNA Methyltransferase
- Ø¬ÙŠÙ† Ù…Ø±ØªØ¨Ø· Ø¨Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø¯Ù…Ø§Øº
- Ø­Ø§Ù„ØªÙ‡ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±ÙŠØ¶ Ù„Ù„Ø¹Ù„Ø§Ø¬
- **methylated** â†’ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£ÙØ¶Ù„ Ù„Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠ
- **not methylated** â†’ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ø¶Ø¹Ù

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù†Ù‡:**
"Ù…Ù† Ù‡Ù… Ø§Ù„Ù…Ø±Ø¶Ù‰ØŸ ÙˆÙ…Ø§ Ù‡ÙŠ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª (labels) Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶ØŸ"

**Ù…Ø§Ø°Ø§ Ù„Ùˆ ÙØ´Ù„Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ**
- **File not found:** Ø§Ù„Ù…Ø³Ø§Ø± Ø®Ø§Ø·Ø¦
- **Encoding error:** ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù ØºÙŠØ± ØµØ­ÙŠØ­
- **Memory error:** Ø§Ù„Ù…Ù„Ù ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ù†Ø§Ø¯Ø± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©)

**Ø§Ù„Ø§ÙØªØ±Ø§Ø¶Ø§Øª:**
- Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯
- Ø§Ù„Ù…Ù„Ù Ø¨ØµÙŠØºØ© CSV ØµØ­ÙŠØ­Ø©
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„

**In English:**

#### Cell 5: Path Definition

**What does it do?**
- Creates a string variable containing CSV file path

**Why use a variable?**
- **Reusability:** Can use `TRAIN_LABELS_PATH` in multiple places
- **Easy modification:** If path changes, edit one place only
- **Clarity:** Uppercase name indicates it's a **constant**

**What if we write the path directly?**
```python
# Bad âŒ
train_labels = pd.read_csv("../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv")
```
- Works, but hard to read
- If we need the path again, we rewrite it
- Risk of typos

#### Cell 6: Reading Data

**What does `pd.read_csv()` do?**

**Step by step:**
1. Opens CSV file
2. Reads rows and columns
3. Converts to DataFrame (pandas table)
4. Saves in `train_labels` variable

**Expected data shape:**
```
   BraTS21ID  MGMT_value
0      00000           1
1      00002           1
2      00003           1
3      00005           0
...      ...         ...
```

**Columns:**
- **BraTS21ID:** Patient identification number
- **MGMT_value:** Classification (0 or 1)
  - 0 = MGMT promoter **not methylated** (negative)
  - 1 = MGMT promoter **methylated** (positive)

**What is MGMT?**
- MGMT = O6-Methylguanine-DNA Methyltransferase
- Gene associated with brain tumors
- Its status affects patient response to treatment
- **methylated** â†’ better response to chemotherapy
- **not methylated** â†’ weaker response

**Question it answers:**
"Who are the patients? What are the labels for each patient?"

**What if reading fails?**
- **File not found:** Wrong path
- **Encoding error:** Incorrect file encoding
- **Memory error:** File too large (rare in this case)

**Assumptions:**
- File exists at specified path
- File is valid CSV format
- Contains at least two columns

---

### âœ… Cell 7-8: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Data Cleaning

```python
# Cell 7
bad_ids = [109, 123, 709]
train_labels = train_labels[~train_labels.BraTS21ID.isin(bad_ids)]
train_labels

# Cell 8
print(train_labels.shape)
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 7: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ§Ø³Ø¯Ø©

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„:**
```python
bad_ids = [109, 123, 709]
```
- ÙŠÙ†Ø´Ø¦ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„ÙØ§Ø³Ø¯Ø©
- **âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù‡Ù†Ø§ integers (109) ÙˆÙ„ÙŠØ³ strings ("00109")

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ (Ø§Ù„Ø£Ù‡Ù…!):**
```python
train_labels = train_labels[~train_labels.BraTS21ID.isin(bad_ids)]
```

**Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­Ù‡ Ù‚Ø·Ø¹Ø© Ù‚Ø·Ø¹Ø©:**

**1. `train_labels.BraTS21ID`**
- ÙŠØ®ØªØ§Ø± Ø¹Ù…ÙˆØ¯ BraTS21ID Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: Series Ù…Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø¶Ù‰

**2. `.isin(bad_ids)`**
- ÙŠÙØ­Øµ: Ù‡Ù„ ÙƒÙ„ Ø±Ù‚Ù… Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© `bad_ids`ØŸ
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: Series Ù…Ù† True/False
- Ù…Ø«Ø§Ù„:
  ```python
  BraTS21ID    isin(bad_ids)
  00000        False
  00109        True   â† Ø³ÙŠØªÙ… Ø­Ø°ÙÙ‡
  00123        True   â† Ø³ÙŠØªÙ… Ø­Ø°ÙÙ‡
  00200        False
  ```

**3. `~` (Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù†ÙÙŠ)**
- ØªØ¹ÙƒØ³ Ø§Ù„Ù‚ÙŠÙ…: True â†’ False Ùˆ False â†’ True
- Ø§Ù„Ù‡Ø¯Ù: Ù†Ø±ÙŠØ¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ **Ù„ÙŠØ³Øª** ÙÙŠ bad_ids
- Ø¨Ø¹Ø¯ `~`:
  ```python
  BraTS21ID    ~isin(bad_ids)
  00000        True    â† Ù†Ø¨Ù‚ÙŠÙ‡
  00109        False   â† Ù†Ø­Ø°ÙÙ‡
  00123        False   â† Ù†Ø­Ø°ÙÙ‡
  00200        True    â† Ù†Ø¨Ù‚ÙŠÙ‡
  ```

**4. `train_labels[...]`**
- ÙŠØ³ØªØ®Ø¯Ù… mask (True/False) Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙÙˆÙ
- ÙŠØ¨Ù‚ÙŠ ÙÙ‚Ø· Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ Ù‚ÙŠÙ…ØªÙ‡Ø§ True

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
- Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ: 585 ØµÙ
- Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: 582 ØµÙ
- ØªÙ… Ø­Ø°Ù 3 ØµÙÙˆÙ (Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„ÙØ§Ø³Ø¯ÙŠÙ†)

**Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø¯ÙŠÙ„Ø© (Ù†ÙØ³ Ø§Ù„Ù†ØªÙŠØ¬Ø©):**
```python
# Ø·Ø±ÙŠÙ‚Ø© 1 (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©)
train_labels = train_labels[~train_labels.BraTS21ID.isin(bad_ids)]

# Ø·Ø±ÙŠÙ‚Ø© 2 (Ø£Ø·ÙˆÙ„ Ù„ÙƒÙ† Ø£ÙˆØ¶Ø­)
mask = ~train_labels.BraTS21ID.isin(bad_ids)
train_labels = train_labels[mask]

# Ø·Ø±ÙŠÙ‚Ø© 3 (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… query)
train_labels = train_labels.query('BraTS21ID not in @bad_ids')
```

#### Cell 8: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ù…

```python
print(train_labels.shape)
```

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ**
- `.shape` ÙŠØ¹Ø·ÙŠ tuple: `(Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ, Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©)`
- Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: `(582, 2)`
  - 582 Ù…Ø±ÙŠØ¶
  - 2 Ø¹Ù…ÙˆØ¯ (BraTS21ID, MGMT_value)

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø·Ø¨Ø¹Ù‡ØŸ**
- **Ø§Ù„ØªØ­Ù‚Ù‚:** Ù‡Ù„ ØªÙ… Ø§Ù„Ø­Ø°Ù Ø¨Ù†Ø¬Ø§Ø­ØŸ
- **Ø§Ù„ØªÙˆØ«ÙŠÙ‚:** Ù†Ø¹Ø±Ù Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
- **Debug:** Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… Ø®Ø§Ø·Ø¦ØŒ Ù†Ø¹Ø±Ù Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø©

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù†Ù‡:**
"ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø¨Ø¹Ø¯ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ"

**Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­ØªÙ…Ù„Ø©:**
```python
# Ø®Ø·Ø£ Ø´Ø§Ø¦Ø¹ âŒ: Ù†Ø³ÙŠØ§Ù† Ø¹Ù„Ø§Ù…Ø© ~
train_labels = train_labels[train_labels.BraTS21ID.isin(bad_ids)]
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: ÙŠØ¨Ù‚ÙŠ ÙÙ‚Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ§Ø³Ø¯Ø©! (3 ØµÙÙˆÙ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 582)

# Ø®Ø·Ø£ Ø´Ø§Ø¦Ø¹ âŒ: Ø¹Ø¯Ù… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
train_labels[~train_labels.BraTS21ID.isin(bad_ids)]
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: ÙŠØ¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ© Ù„ÙƒÙ† Ù„Ø§ ÙŠØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±
```

**In English:**

#### Cell 7: Removing Corrupted Data

**Line 1 analysis:**
```python
bad_ids = [109, 123, 709]
```
- Creates list of corrupted patient IDs
- **âš ï¸ Note:** Numbers here are integers (109) not strings ("00109")

**Line 2 analysis (Most Important!):**
```python
train_labels = train_labels[~train_labels.BraTS21ID.isin(bad_ids)]
```

**Let me explain piece by piece:**

**1. `train_labels.BraTS21ID`**
- Selects BraTS21ID column from table
- Result: Series of patient numbers

**2. `.isin(bad_ids)`**
- Checks: is each number in `bad_ids` list?
- Result: Series of True/False

**3. `~` (negation operator)**
- Reverses values: True â†’ False and False â†’ True
- Goal: we want rows that are **not** in bad_ids

**4. `train_labels[...]`**
- Uses mask (True/False) to select rows
- Keeps only rows where value is True

**Final result:**
- Original table: 585 rows
- After deletion: 582 rows
- Deleted 3 rows (corrupted patients)

#### Cell 8: Size Verification

```python
print(train_labels.shape)
```

**What does it do?**
- `.shape` gives tuple: `(number of rows, number of columns)`
- Expected output: `(582, 2)`
  - 582 patients
  - 2 columns (BraTS21ID, MGMT_value)

**Why print it?**
- **Verification:** Was deletion successful?
- **Documentation:** Know data size for future reference
- **Debug:** If number is wrong, we know there's a problem

**Question it answers:**
"How many patients after data cleaning?"

**Possible errors:**
```python
# Common mistake âŒ: forgetting ~ sign
train_labels = train_labels[train_labels.BraTS21ID.isin(bad_ids)]
# Result: keeps only corrupted data! (3 rows instead of 582)

# Common mistake âŒ: not saving result
train_labels[~train_labels.BraTS21ID.isin(bad_ids)]
# Result: displays cleaned data but doesn't save it in variable
```

---
# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ | Continuation of Detailed Explanation

---

### âœ… Cell 9-12: ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª | Label Distribution Analysis

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 9: Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ
```markdown
As shown the size of training data is 582
```
- Ù…Ø¬Ø±Ø¯ Ù…Ù„Ø§Ø­Ø¸Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© ØªØ¤ÙƒØ¯ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

#### Cell 10: Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª

```python
train_labels['MGMT_value'].value_counts()
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ `value_counts()`ØŸ**
- ÙŠØ­Ø³Ø¨ ÙƒÙ… Ù…Ø±Ø© Ø¸Ù‡Ø±Øª ÙƒÙ„ Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯
- ÙŠØ±ØªØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ØªÙƒØ±Ø§Ø±Ø§Ù‹
- ÙŠÙØ³ØªØ®Ø¯Ù… **Ø¨ÙƒØ«Ø±Ø©** ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```python
0    291  # Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ© (not methylated)
1    291  # Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© (methylated)
Name: MGMT_value, dtype: int64
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ØŸ**

**1. ÙØ­Øµ Ø§Ù„ØªÙˆØ§Ø²Ù† (Class Balance):**
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø©:
  ```python
  0    500  # 86%
  1     82  # 14%
  ```
  Ù‡Ø°Ø§ **ØºÙŠØ± Ù…ØªÙˆØ§Ø²Ù†** (imbalanced)! Ø§Ù„Ù…Ø´Ø§ÙƒÙ„:
  - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³ÙŠØªØ¹Ù„Ù… Ø§Ù„ØªØ­ÙŠØ² Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø£ÙƒØ¨Ø±
  - Ø³ÙŠØªÙ†Ø¨Ø£ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù€ 0 ÙˆÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© 86%!
  - Ù„ÙƒÙ†Ù‡ ÙØ§Ø´Ù„ ÙÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ¦Ø© 1

- Ù„ÙƒÙ† ÙÙŠ Ø­Ø§Ù„ØªÙ†Ø§:
  ```python
  0    291  # 50%
  1    291  # 50%
  ```
  **Ù…ØªÙˆØ§Ø²Ù† ØªÙ…Ø§Ù…Ø§Ù‹!** âœ… Ù‡Ø°Ø§ Ù…Ù…ØªØ§Ø² Ù„Ù„ØªØ¯Ø±ÙŠØ¨

**2. ØªØ®Ø·ÙŠØ· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:**
- Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø© â†’ Ù†Ø³ØªØ®Ø¯Ù… accuracy ÙƒÙ…Ù‚ÙŠØ§Ø³
- Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆØ§Ø²Ù†Ø© â†’ Ù†Ø­ØªØ§Ø¬ F1-score, AUC-ROC, weighted loss

**3. Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø©:**
- 291 Ø¹ÙŠÙ†Ø© Ù„ÙƒÙ„ ÙØ¦Ø© = 582 Ø¹ÙŠÙ†Ø© ÙƒÙ„ÙŠØ§Ù‹
- **âš ï¸ Ù‡Ø°Ø§ Ø±Ù‚Ù… ØµØºÙŠØ± Ù†Ø³Ø¨ÙŠØ§Ù‹!**
- Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬:
  - Data augmentation Ù‚ÙˆÙŠØ©
  - Transfer learning Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
  - Cross-validation Ø¯Ù‚ÙŠÙ‚

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù†Ù‡:**
"Ù‡Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø©ØŸ Ù‡Ù„ Ù†Ø­ØªØ§Ø¬ Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø®Ø§ØµØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†ØŸ"

**Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©:**
```python
# Ø®Ø·Ø£ âŒ: Ø§Ø³ØªØ®Ø¯Ø§Ù… count() Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† value_counts()
train_labels['MGMT_value'].count()  # ÙŠØ¹Ø·ÙŠ ÙÙ‚Ø· Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ (582)

# ØµØ­ÙŠØ­ âœ…:
train_labels['MGMT_value'].value_counts()  # ÙŠØ¹Ø·ÙŠ Ø¹Ø¯Ø¯ ÙƒÙ„ ÙØ¦Ø©
```

#### Cell 11: Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„ØªÙˆØ²ÙŠØ¹

```python
plt.figure(figsize=(5, 5))
sns.countplot(data=train_labels, x="MGMT_value");
```

**ØªØ­Ù„ÙŠÙ„ Ø³Ø·Ø± Ø¨Ø³Ø·Ø±:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
plt.figure(figsize=(5, 5))
```
- **Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ** ÙŠÙ†Ø´Ø¦ figure Ø¬Ø¯ÙŠØ¯Ø©
- **`figsize=(5, 5)`:** Ø­Ø¬Ù… Ø§Ù„Ø±Ø³Ù… Ø¨Ø§Ù„Ø¥Ù†Ø´ (Ø¹Ø±Ø¶ 5ØŒ Ø§Ø±ØªÙØ§Ø¹ 5)
- **Ù„Ù…Ø§Ø°Ø§ (5, 5)ØŸ** Ù…Ø±Ø¨Ø¹ Ù„Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
- **Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø­Ø°ÙÙ†Ø§Ù‡ØŸ** Ø³ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (6.4, 4.8)

**Ø§Ù„Ø³Ø·Ø± 2:**
```python
sns.countplot(data=train_labels, x="MGMT_value");
```
- **`sns.countplot()`:** ÙŠØ±Ø³Ù… Ø¹Ø¯Ø¯ ÙƒÙ„ ÙØ¦Ø© ÙƒÙ€ bar chart
- **`data=train_labels`:** Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- **`x="MGMT_value"`:** Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø±Ø³Ù…Ù‡
- **`;` ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©:** ÙŠÙ…Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø© return value (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

**Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† countplot Ùˆ hist:**
```python
# countplot: Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© (categorical)
sns.countplot(x="MGMT_value")  # ÙŠØ±Ø³Ù… Ø¹Ø¯Ø¯ 0 ÙˆØ¹Ø¯Ø¯ 1

# histogram: Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© (continuous)
plt.hist(ages)  # ÙŠØ±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø§Ø±
```

**Ù…Ø§ ÙŠØ¸Ù‡Ø±Ù‡ Ø§Ù„Ø±Ø³Ù…:**
- Ø¹Ù…ÙˆØ¯ÙŠÙ† Ù…ØªØ³Ø§ÙˆÙŠÙŠ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹
- Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ (0): Ø­ÙˆØ§Ù„ÙŠ 291
- Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø«Ø§Ù†ÙŠ (1): Ø­ÙˆØ§Ù„ÙŠ 291
- **Ø§Ù„Ù†ØªÙŠØ¬Ø©:** ØªØ£ÙƒÙŠØ¯ Ø¨ØµØ±ÙŠ Ù„Ù„ØªÙˆØ§Ø²Ù†

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø³Ù… Ù…Ø¹ Ø£Ù† `value_counts()` ÙŠØ¹Ø·ÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŸ**
- **Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø£ÙˆØ¶Ø­ Ø¨ØµØ±ÙŠØ§Ù‹**
- Ø³Ù‡Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„ÙØ±Ù‚ Ø¨Ù†Ø¸Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
- Ù…ÙÙŠØ¯ ÙÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…ÙŠØ©
- ÙŠÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ù‚Ø¯ Ù„Ø§ ØªØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…

#### Cell 12: ØªØ¹Ù„ÙŠÙ‚ ØªÙˆØ¶ÙŠØ­ÙŠ
```markdown
The train labels seem balanced! Great!
```
- ÙŠØ¤ÙƒØ¯ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø© âœ…

**In English:**

#### Cell 10: Counting Labels

```python
train_labels['MGMT_value'].value_counts()
```

**Deep Analysis:**

**What does `value_counts()` do?**
- Counts how many times each value appeared in the column
- Sorts results from most to least frequent
- Used **extensively** in data analysis

**Expected output:**
```python
0    291  # number of negative cases (not methylated)
1    291  # number of positive cases (methylated)
```

**Why is this very important?**

**1. Checking Balance (Class Balance):**
- If result was:
  ```python
  0    500  # 86%
  1     82  # 14%
  ```
  This is **imbalanced**! Problems:
  - Model will learn bias toward larger class
  - Will always predict 0 and get 86% accuracy!
  - But fails to detect class 1

- But in our case:
  ```python
  0    291  # 50%
  1    291  # 50%
  ```
  **Perfectly balanced!** âœ… This is excellent for training

**2. Strategy Planning:**
- Balanced data â†’ use accuracy as metric
- Imbalanced data â†’ need F1-score, AUC-ROC, weighted loss

**3. Sample Size:**
- 291 samples per class = 582 total
- **âš ï¸ This is relatively small!**
- May need:
  - Strong data augmentation
  - Transfer learning from pre-trained models
  - Careful cross-validation

**Question it answers:**
"Is the data balanced? Do we need special techniques for handling imbalance?"

#### Cell 11: Distribution Plot

```python
plt.figure(figsize=(5, 5))
sns.countplot(data=train_labels, x="MGMT_value");
```

**Line-by-line analysis:**

**Line 1:**
```python
plt.figure(figsize=(5, 5))
```
- **What does it do?** Creates new figure
- **`figsize=(5, 5)`:** Plot size in inches (width 5, height 5)
- **Why (5, 5)?** Square because we have only one column
- **What if we remove it?** Will use default size (6.4, 4.8)

**Line 2:**
```python
sns.countplot(data=train_labels, x="MGMT_value");
```
- **`sns.countplot()`:** Draws count of each category as bar chart
- **`data=train_labels`:** Data source
- **`x="MGMT_value"`:** Column to plot
- **`;` at end:** Prevents printing return value (optional)

**What the plot shows:**
- Two bars of equal height
- First bar (0): about 291
- Second bar (1): about 291
- **Result:** Visual confirmation of balance

**Why use plot when `value_counts()` gives same info?**
- **Plot is visually clearer**
- Easy to see difference at a glance
- Useful in presentations
- Reveals patterns that may not appear in numbers

---

### âœ… Cell 13-15: Ø§Ø³ØªÙƒØ´Ø§Ù Ø¨Ù†ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Data Structure Exploration

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 13: Ø£Ø³Ø¦Ù„Ø© Ø§Ø³ØªÙƒØ´Ø§ÙÙŠØ©
```markdown
Let's discover the train data and how is it.
What is number of DICOM slices available for each MRI modality (FLAIR, T1w, T1wCE, T2w) across all patients in the dataset
```

**Ø§Ù„Ù‡Ø¯Ù:**
- Ù†Ø±ÙŠØ¯ Ø£Ù† Ù†Ø¹Ø±Ù: ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± (slices) Ù„ÙƒÙ„ Ù†ÙˆØ¹ MRIØŸ
- ÙƒÙ„ Ù…Ø±ÙŠØ¶ Ù„Ø¯ÙŠÙ‡ 4 Ø£Ù†ÙˆØ§Ø¹ MRI:
  1. **FLAIR** (Fluid-Attenuated Inversion Recovery)
  2. **T1w** (T1-weighted)
  3. **T1wCE** (T1-weighted with Contrast Enhancement)
  4. **T2w** (T2-weighted)

**Ù„Ù…Ø§Ø°Ø§ 4 Ø£Ù†ÙˆØ§Ø¹ØŸ**
- ÙƒÙ„ Ù†ÙˆØ¹ ÙŠÙØ¸Ù‡Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø§Ù„Ø¯Ù…Ø§Øº:
  - **FLAIR:** ÙŠÙØ¸Ù‡Ø± Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ ÙˆØ§Ù„ÙˆØ°Ù…Ø§Øª
  - **T1w:** ÙŠÙØ¸Ù‡Ø± Ø§Ù„ØªØ´Ø±ÙŠØ­ (anatomy)
  - **T1wCE:** ÙŠÙØ¸Ù‡Ø± Ø§Ù„Ø£ÙˆØ±Ø§Ù… Ø¨Ø¹Ø¯ Ø­Ù‚Ù† Ù…Ø§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ†
  - **T2w:** ÙŠÙØ¸Ù‡Ø± Ø§Ù„Ø§Ù„ØªÙ‡Ø§Ø¨Ø§Øª ÙˆØ§Ù„Ù†Ø²ÙŠÙ

#### Cell 14: ØªØ¹Ø±ÙŠÙ Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
TRAIN_DATA_PATH = "/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/"
```

**ØªØ­Ù„ÙŠÙ„:**
- ÙŠØ­Ø¯Ø¯ Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:
  ```
  train/
  â”œâ”€â”€ 00000/
  â”‚   â”œâ”€â”€ FLAIR/
  â”‚   â”‚   â”œâ”€â”€ Image-1.dcm
  â”‚   â”‚   â”œâ”€â”€ Image-2.dcm
  â”‚   â”‚   â””â”€â”€ ...
  â”‚   â”œâ”€â”€ T1w/
  â”‚   â”œâ”€â”€ T1wCE/
  â”‚   â””â”€â”€ T2w/
  â”œâ”€â”€ 00002/
  â”œâ”€â”€ 00003/
  â””â”€â”€ ...
  ```

**Ù…Ù„Ø§Ø­Ø¸Ø©:**
- ÙƒÙ„ Ù…Ø±ÙŠØ¶ = Ù…Ø¬Ù„Ø¯
- ÙƒÙ„ Ù…Ø¬Ù„Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 4 Ù…Ø¬Ù„Ø¯Ø§Øª ÙØ±Ø¹ÙŠØ© (Ù„Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©)
- ÙƒÙ„ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© ØµÙˆØ± .dcm

#### Cell 15: Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ù„ÙƒÙ„ Ù†ÙˆØ¹

```python
# filter patient_ids from bad ids, [00109, 00123, 00709].
patient_ids = sorted(os.listdir(TRAIN_DATA_PATH))
bad_ids = ['00109', '00123', '00709']
patient_ids = [i for i in patient_ids if i not in bad_ids]

Flair_files = []
T1w_files = []
T1wCE_files = []
T2w_files = []

for i in tqdm(range(len(patient_ids))):
      patient_path = os.path.join(TRAIN_DATA_PATH, patient_ids[i])
      Flair_files.append(len(os.listdir(os.path.join(patient_path, "FLAIR"))))
      T1w_files.append(len(os.listdir(os.path.join(patient_path, "T1w"))))
      T1wCE_files.append(len(os.listdir(os.path.join(patient_path, "T1wCE"))))
      T2w_files.append(len(os.listdir(os.path.join(patient_path, "T2w"))))
```

**ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ø¬Ø¯Ø§Ù‹:**

**Ø§Ù„Ø³Ø·Ø± 1-2:**
```python
patient_ids = sorted(os.listdir(TRAIN_DATA_PATH))
bad_ids = ['00109', '00123', '00709']
```
- `os.listdir()` ÙŠØ¹Ø·ÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
- `sorted()` ÙŠØ±ØªØ¨Ù‡Ø§ Ø£Ø¨Ø¬Ø¯ÙŠØ§Ù‹ (00000, 00002, 00003...)
- **âš ï¸ Ù…Ù‡Ù…:** Ù‡Ù†Ø§ `bad_ids` strings ÙˆÙ„ÙŠØ³ integers!

**Ø§Ù„Ø³Ø·Ø± 3:**
```python
patient_ids = [i for i in patient_ids if i not in bad_ids]
```
- **List comprehension** (Ø£Ø³Ù„ÙˆØ¨ pythonic)
- ÙŠØ­ÙØ¸ ÙÙ‚Ø· Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø°ÙŠÙ† **Ù„ÙŠØ³ÙˆØ§** ÙÙŠ bad_ids
- Ù…ÙƒØ§ÙØ¦Ø© Ù„Ù€:
  ```python
  clean_ids = []
  for i in patient_ids:
      if i not in bad_ids:
          clean_ids.append(i)
  patient_ids = clean_ids
  ```

**Ø§Ù„Ø³Ø·Ø± 4-7:**
```python
Flair_files = []
T1w_files = []
T1wCE_files = []
T2w_files = []
```
- ÙŠÙ†Ø´Ø¦ 4 Ù‚ÙˆØ§Ø¦Ù… ÙØ§Ø±ØºØ©
- ÙƒÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø³ØªØ­ÙØ¸ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ù„Ù†ÙˆØ¹ Ù…Ø¹ÙŠÙ†

**Ø§Ù„Ø³Ø·Ø± 9-14 (Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©):**

```python
for i in tqdm(range(len(patient_ids))):
```
- `range(len(patient_ids))` ÙŠÙ†Ø´Ø¦: 0, 1, 2, ..., 581
- `tqdm()` ÙŠØ¶ÙŠÙ progress bar
- **Ù„Ù…Ø§Ø°Ø§ `range(len())` ÙˆÙ„ÙŠØ³ `for patient in patient_ids`ØŸ**
  - Ù„Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ index (i) Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£Ø®Ø±Ù‰

```python
patient_path = os.path.join(TRAIN_DATA_PATH, patient_ids[i])
```
- ÙŠÙ†Ø´Ø¦ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø±ÙŠØ¶
- Ù…Ø«Ø§Ù„: `/kaggle/input/.../train/00000`
- **Ù„Ù…Ø§Ø°Ø§ `os.path.join()` ÙˆÙ„ÙŠØ³ `/` Ø¹Ø§Ø¯ÙŠØŸ**
  - ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Windows Ùˆ Linux
  - ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­

```python
Flair_files.append(len(os.listdir(os.path.join(patient_path, "FLAIR"))))
```
**Ø¯Ø¹Ù†ÙŠ Ø£ÙÙƒÙƒÙ‡:**

1. `os.path.join(patient_path, "FLAIR")`
   - ÙŠÙ†Ø´Ø¦: `/kaggle/input/.../train/00000/FLAIR`

2. `os.listdir(...)`
   - ÙŠØ¹Ø·ÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ FLAIR
   - Ù…Ø«Ø§Ù„: ['Image-1.dcm', 'Image-2.dcm', ..., 'Image-400.dcm']

3. `len(...)`
   - ÙŠØ­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
   - Ù…Ø«Ø§Ù„: 400

4. `.append(...)`
   - ÙŠØ¶ÙŠÙ Ø§Ù„Ø¹Ø¯Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
   - Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø§Ù„Ø£ÙˆÙ„: `Flair_files = [400]`
   - Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø§Ù„Ø«Ø§Ù†ÙŠ: `Flair_files = [400, 385]`
   - ÙˆÙ‡ÙƒØ°Ø§...

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
```python
Flair_files = [400, 385, 392, ...]  # 582 Ø¹Ù†ØµØ±
T1w_files = [400, 385, 392, ...]
T1wCE_files = [400, 385, 392, ...]
T2w_files = [400, 385, 392, ...]
```

**Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø© (Ø§Ù„Ø³Ø·Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ù„ÙƒÙ† Ù…Ù†Ø·Ù‚ÙŠØ§Ù‹):**
```python
no_frame_df = pd.DataFrame({
    'Flair': Flair_files,
    'T1w': T1w_files,
    'T1wCE': T1wCE_files,
    'T2w': T2w_files
})
```

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù†Ù‡:**
"ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± (slices) Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ÙƒÙ„ Ù†ÙˆØ¹ MRI Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶ØŸ"

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ**
1. **Ù…Ø¹Ø±ÙØ© Ø§Ù„ØªØ¨Ø§ÙŠÙ†:** Ù‡Ù„ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù„Ø¯ÙŠÙ‡Ù… Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø¯ØŸ
2. **ØªØ®Ø·ÙŠØ· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:** ÙƒÙŠÙ Ù†ÙˆØ­Ø¯ Ø§Ù„Ø¹Ø¯Ø¯ØŸ
3. **Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ°:** Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù…Ø±Ø¶Ù‰ Ø¨Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„ØµÙˆØ±ØŸ

**Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­ØªÙ…Ù„Ø©:**
```python
# Ø®Ø·Ø£ âŒ: Ù†Ø³ÙŠØ§Ù† Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ bad_ids
patient_ids = sorted(os.listdir(TRAIN_DATA_PATH))
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ø³ÙŠØ­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ§Ø³Ø¯Ø© â†’ Ø®Ø·Ø£

# Ø®Ø·Ø£ âŒ: Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… os.path.join
path = TRAIN_DATA_PATH + patient_ids[i] + "/FLAIR"
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù‚Ø¯ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Linux Ù„ÙƒÙ† ÙŠÙØ´Ù„ Ø¹Ù„Ù‰ Windows

# ØµØ­ÙŠØ­ âœ…:
path = os.path.join(TRAIN_DATA_PATH, patient_ids[i], "FLAIR")
```

**ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ù…ÙƒÙ†Ø©:**
```python
# Ø£Ø³Ø±Ø¹ ÙˆØ£Ù†Ø¸Ù:
from pathlib import Path

patient_ids = [p.name for p in Path(TRAIN_DATA_PATH).iterdir() 
               if p.name not in bad_ids]

# Ø£Ùˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… dictionary:
modality_counts = {mod: [] for mod in ['FLAIR', 'T1w', 'T1wCE', 'T2w']}
for patient in tqdm(patient_ids):
    patient_path = Path(TRAIN_DATA_PATH) / patient
    for mod in modality_counts:
        modality_counts[mod].append(len(list((patient_path / mod).iterdir())))
```

**In English:**

#### Cell 15: Counting Images per Type

**Detailed Analysis:**

**Lines 1-3:**
- Gets list of patient folders
- Excludes bad patient IDs
- **âš ï¸ Important:** Here `bad_ids` are strings, not integers!

**Lines 4-7:**
- Creates 4 empty lists
- Each list will store image counts for a specific type

**Lines 9-14 (Main Loop):**

```python
for i in tqdm(range(len(patient_ids))):
    patient_path = os.path.join(TRAIN_DATA_PATH, patient_ids[i])
    Flair_files.append(len(os.listdir(os.path.join(patient_path, "FLAIR"))))
```

**Breaking it down:**

1. `os.path.join(patient_path, "FLAIR")`
   - Creates: `/kaggle/input/.../train/00000/FLAIR`

2. `os.listdir(...)`
   - Returns list of all files in FLAIR
   - Example: ['Image-1.dcm', 'Image-2.dcm', ..., 'Image-400.dcm']

3. `len(...)`
   - Counts number of files
   - Example: 400

4. `.append(...)`
   - Adds count to list
   - After first patient: `Flair_files = [400]`
   - After second patient: `Flair_files = [400, 385]`
   - And so on...

**Final Result:**
```python
Flair_files = [400, 385, 392, ...]  # 582 elements
T1w_files = [400, 385, 392, ...]
T1wCE_files = [400, 385, 392, ...]
T2w_files = [400, 385, 392, ...]
```

**Question it answers:**
"How many images (slices) are available for each MRI type for each patient?"

**Why is this important?**
1. **Know variation:** Do all patients have same count?
2. **Plan processing:** How to standardize count?
3. **Detect anomalies:** Are there patients with very few images?

---

### âœ… Cell 17-23: ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØµÙˆØ± | Image Distribution Analysis

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 17: Ø¹Ø±Ø¶ DataFrame

```python
no_frame_df
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ù‡Ù†Ø§ØŸ**
- ÙŠØ¹Ø±Ø¶ DataFrame Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ù„ÙƒÙ„ Ù†ÙˆØ¹
- Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:

```
    Flair  T1w  T1wCE  T2w
0     400  400    400  400
1     385  385    385  385
2     392  392    392  392
...   ...  ...    ...  ...
581   420  420    420  420
```

**Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©:**
1. **Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø¯ Ø¹Ø¨Ø± Ø§Ù„Ø£Ù†ÙˆØ§Ø¹:** Ù„Ø§Ø­Ø¸ Ø£Ù† Flair = T1w = T1wCE = T2w Ù„Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶
   - Ù‡Ø°Ø§ Ù…Ù†Ø·Ù‚ÙŠ! Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶ = Ù†ÙØ³ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ slices
   - ÙƒÙ„ slice ØªÙ… ØªØµÙˆÙŠØ±Ù‡ Ø¨Ù€ 4 Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ©

2. **Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø¶Ù‰:** 
   - Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø±Ø¶Ù‰: 385 ØµÙˆØ±Ø©
   - Ø¨Ø¹Ø¶Ù‡Ù…: 400 ØµÙˆØ±Ø©
   - Ø¨Ø¹Ø¶Ù‡Ù…: 420 ØµÙˆØ±Ø©
   - **Ù„Ù…Ø§Ø°Ø§ØŸ** Ø³ÙÙ…Ùƒ Ø§Ù„Ù€ slices ÙŠØ®ØªÙ„ÙØŒ Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ®ØªÙ„Ù ÙÙŠ Ø§Ù„Ø­Ø¬Ù…

#### Cell 19: Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±

```python
modalities = ["Flair", "T1w", "T1wCE", "T2w"]

fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes = axes.flatten()  # flatten to iterate easily

for i, m in enumerate(modalities):
    axes[i].hist(no_frame_df[m], bins=30, color='steelblue', edgecolor='black')
    axes[i].set_title(f"Distribution of {m} slice counts", fontsize=10)
    axes[i].set_xlabel("Number of slices")
    axes[i].set_ylabel("Number of patients")

plt.tight_layout()
plt.show()
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
modalities = ["Flair", "T1w", "T1wCE", "T2w"]
```
- Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
- Ø³Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù„Ù„ØªÙƒØ±Ø§Ø±

**Ø§Ù„Ø³Ø·Ø± 3:**
```python
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
```
**Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­Ù‡ Ø¨Ø§Ù„ØªÙØµÙŠÙ„:**

- **`plt.subplots(2, 2)`:** ÙŠÙ†Ø´Ø¦ Ø´Ø¨ÙƒØ© 2Ã—2 Ù…Ù† Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ axes[0] â”‚ axes[1] â”‚  â† Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ axes[2] â”‚ axes[3] â”‚  â† Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ```

- **`figsize=(10, 8)`:** 
  - Ø¹Ø±Ø¶ 10 Ø¥Ù†Ø´ØŒ Ø§Ø±ØªÙØ§Ø¹ 8 Ø¥Ù†Ø´
  - Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ø£Ù†Ù†Ø§ Ù†Ø¹Ø±Ø¶ 4 Ø±Ø³ÙˆÙ…

- **Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙØ±Ø¬Ø¹Ø©:**
  - `fig`: Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ (container)
  - `axes`: Ù…ØµÙÙˆÙØ© 2Ã—2 Ù…Ù† Ø§Ù„Ù…Ø­Ø§ÙˆØ±

**Ø§Ù„Ø³Ø·Ø± 4:**
```python
axes = axes.flatten()
```
**Ù„Ù…Ø§Ø°Ø§ flattenØŸ**

- Ù‚Ø¨Ù„: `axes` Ø´ÙƒÙ„Ù‡ (2, 2) â†’ Ù…ØµÙÙˆÙØ© Ø«Ù†Ø§Ø¦ÙŠØ©
  ```python
  axes = [[ax0, ax1],
          [ax2, ax3]]
  ```
  Ù„Ù„ÙˆØµÙˆÙ„: `axes[0][0]`, `axes[0][1]`, `axes[1][0]`, `axes[1][1]`

- Ø¨Ø¹Ø¯ flatten: Ø´ÙƒÙ„Ù‡ (4,) â†’ Ù…ØµÙÙˆÙØ© Ø£Ø­Ø§Ø¯ÙŠØ©
  ```python
  axes = [ax0, ax1, ax2, ax3]
  ```
  Ù„Ù„ÙˆØµÙˆÙ„: `axes[0]`, `axes[1]`, `axes[2]`, `axes[3]`

- **Ø§Ù„ÙØ§Ø¦Ø¯Ø©:** ÙŠØ³Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±!

**Ø§Ù„Ø³Ø·Ø± 6-10 (Ø§Ù„Ø­Ù„Ù‚Ø©):**

```python
for i, m in enumerate(modalities):
```
- `enumerate()` ÙŠØ¹Ø·ÙŠ (index, value)
- Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª:
  - i=0, m="Flair"
  - i=1, m="T1w"
  - i=2, m="T1wCE"
  - i=3, m="T2w"

```python
axes[i].hist(no_frame_df[m], bins=30, color='steelblue', edgecolor='black')
```
**ØªÙÙƒÙŠÙƒ Ø§Ù„Ø¯Ø§Ù„Ø©:**

- **`axes[i]`:** Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø­Ø§Ù„ÙŠ (ax0, ax1, ax2, or ax3)
- **`.hist()`:** ÙŠØ±Ø³Ù… histogram
- **`no_frame_df[m]`:** Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¹Ù…ÙˆØ¯ Flair Ø£Ùˆ T1w...)
- **`bins=30`:** Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (bars) ÙÙŠ Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
  - ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ bins â†’ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ù„ÙƒÙ† Ø£Ø¹Ù…Ø¯Ø© Ø£Ø¶ÙŠÙ‚
  - ÙƒÙ„Ù…Ø§ Ù‚Ù„ bins â†’ Ø£Ø¹Ù…Ø¯Ø© Ø£Ø¹Ø±Ø¶ Ù„ÙƒÙ† Ø¯Ù‚Ø© Ø£Ù‚Ù„
  - 30 Ø±Ù‚Ù… Ù…Ø¹Ù‚ÙˆÙ„ Ù„Ù„ØªÙˆØ§Ø²Ù†

- **`color='steelblue'`:** Ù„ÙˆÙ† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
- **`edgecolor='black'`:** Ù„ÙˆÙ† Ø­ÙˆØ§Ù Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Ù„ØªÙ…ÙŠÙŠØ²Ù‡Ø§)

```python
axes[i].set_title(f"Distribution of {m} slice counts", fontsize=10)
```
- ÙŠØ¶Ø¹ Ø¹Ù†ÙˆØ§Ù† Ù„Ù„Ù…Ø®Ø·Ø·
- `f"...{m}..."` â†’ f-string (Python 3.6+)
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: "Distribution of Flair slice counts"

```python
axes[i].set_xlabel("Number of slices")
axes[i].set_ylabel("Number of patients")
```
- ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø§ÙˆØ±
- x-axis: Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±
- y-axis: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø¶Ù‰

**Ø§Ù„Ø³Ø·Ø± 12:**
```python
plt.tight_layout()
```
- **Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ** ÙŠØ¶Ø¨Ø· Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- **Ù„Ù…Ø§Ø°Ø§ØŸ** Ø¨Ø¯ÙˆÙ†Ù‡ØŒ Ù‚Ø¯ ØªØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§ÙˆØ±
- **Ù‚Ø¨Ù„ tight_layout:**
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚      â”‚â”‚      â”‚ â† Ù…ØªØ¯Ø§Ø®Ù„!
  â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜
  ```
- **Ø¨Ø¹Ø¯ tight_layout:**
  ```
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚      â”‚  â”‚      â”‚ â† Ù…Ù†Ø¸Ù…!
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
  ```

**Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙƒØ´ÙÙ‡ Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…ØŸ**

Ù…Ù† Ø§Ù„Ù†Ø¸Ø± Ù„Ù„Ø±Ø³Ù…ØŒ Ù†Ø±Ù‰:
- **Ø§Ù„ØªÙˆØ²ÙŠØ¹:** Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù„Ø¯ÙŠÙ‡Ù… Ø¨ÙŠÙ† 120-160 ØµÙˆØ±Ø©
- **Ø§Ù„Ø°Ø±ÙˆØ© (peak):** Ø­ÙˆØ§Ù„ÙŠ 130 ØµÙˆØ±Ø©
- **Ø§Ù„Ù†Ø·Ø§Ù‚:** Ù…Ù† Ø­ÙˆØ§Ù„ÙŠ 100 Ø¥Ù„Ù‰ 180 ØµÙˆØ±Ø©
- **Ø§Ù„Ø´ÙƒÙ„:** ÙŠØ´Ø¨Ù‡ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ (bell curve)

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ**
- Ù†Ø¹Ø±Ù Ø§Ù„Ù…Ø¯Ù‰ (range) Ù„Ù„ØªØ®Ø·ÙŠØ· Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
- Ù†ÙƒØªØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ outliers (Ù‚ÙŠÙ… Ø´Ø§Ø°Ø©)
- Ù†Ù‚Ø±Ø±: Ù‡Ù„ Ù†Ø­ØªØ§Ø¬ padding/truncation Ù„Ù„ØªÙˆØ­ÙŠØ¯ØŸ

#### Cell 21: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰

```python
print("The minimum number of slices with **Flair** modalities", no_frame_df['Flair'].values.min())
print("The minimum number of slices with **T1w** modalities", no_frame_df['T1w'].values.min())
print("The minimum number of slices with **T1wCE** modalities", no_frame_df['T1wCE'].values.min())
print("The minimum number of slices with **T2w** modalities", no_frame_df['T2w'].values.min())
```

**ØªØ­Ù„ÙŠÙ„:**

```python
no_frame_df['Flair'].values.min()
```
- **`['Flair']`:** ÙŠØ®ØªØ§Ø± Ø§Ù„Ø¹Ù…ÙˆØ¯
- **`.values`:** ÙŠØ­ÙˆÙ„ Ù…Ù† pandas Series Ø¥Ù„Ù‰ numpy array
- **`.min()`:** ÙŠØ¬Ø¯ Ø£ØµØºØ± Ù‚ÙŠÙ…Ø©

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
The minimum number of slices with **Flair** modalities 99
The minimum number of slices with **T1w** modalities 99
The minimum number of slices with **T1wCE** modalities 99
The minimum number of slices with **T2w** modalities 99
```

**Ù„Ù…Ø§Ø°Ø§ Ù†ÙØ³ Ø§Ù„Ø±Ù‚Ù… (99) Ù„Ù„Ø¬Ù…ÙŠØ¹ØŸ**
- Ù„Ø£Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶ Ù„Ø¯ÙŠÙ‡ Ù†ÙØ³ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ slices Ù„ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
- Ø§Ù„Ù…Ø±ÙŠØ¶ Ø§Ù„Ø°ÙŠ Ù„Ø¯ÙŠÙ‡ Ø£Ù‚Ù„ Ø¹Ø¯Ø¯ ØµÙˆØ± = 99 ØµÙˆØ±Ø©

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ**
- **Padding:** Ø¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„ØŒ ÙŠØ¬Ø¨ Ø£Ù† Ù†Ø¹Ø±Ù Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰
- **Truncation:** Ø£Ùˆ Ù†Ù‚Øµ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ (99) Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
- **Planning:** Ù†Ø¹Ø±Ù Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù„Ø¯ÙŠÙ‡Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 99 ØµÙˆØ±Ø©

**Ø·Ø±ÙŠÙ‚Ø© Ø£ÙØ¶Ù„:**
```python
# Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 4 Ø£Ø³Ø·Ø±ØŒ ÙŠÙ…ÙƒÙ†:
print(no_frame_df.min())

# Ø£Ùˆ:
for mod in modalities:
    print(f"{mod}: {no_frame_df[mod].min()}")
```

#### Cell 23: Ø¹Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙ„ÙŠ

```python
filenames = glob.glob('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/*/*/*')
print(f'Total number of files: {len(filenames)}')
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Pattern ÙÙŠ glob:**
```python
'train/*/*/*'
```
- `*` Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ (00000, 00002...)
- `*` Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆØ¯Ø§Ù„ØªÙŠ (FLAIR, T1w...)
- `*` Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù (Image-1.dcm...)

**Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª:**
```
train/00000/FLAIR/Image-1.dcm
train/00000/FLAIR/Image-2.dcm
train/00000/T1w/Image-1.dcm
train/00002/FLAIR/Image-1.dcm
...
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
Total number of files: 350000+ 
```
(Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶)

**Ø§Ù„Ø­Ø³Ø§Ø¨:**
- 582 Ù…Ø±ÙŠØ¶
- ÙƒÙ„ Ù…Ø±ÙŠØ¶: Ù…ØªÙˆØ³Ø· ~130 ØµÙˆØ±Ø© Ù„ÙƒÙ„ Ù†ÙˆØ¹
- 4 Ø£Ù†ÙˆØ§Ø¹
- Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 582 Ã— 130 Ã— 4 â‰ˆ 302,640 Ù…Ù„Ù

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠØŸ**
1. **ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©:** ÙƒÙ„ Ù…Ù„Ù ~100 KB â†’ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ~30 GB
2. **ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆÙ‚Øª:** Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
3. **Ø§Ù„ØªØ­Ù‚Ù‚:** Ù‡Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø©ØŸ

**In English:**

#### Cell 19: Plotting Image Distribution

**Deep Analysis:**

```python
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes = axes.flatten()
```

**Why flatten?**
- Before: `axes` shape (2, 2) â†’ 2D array
- After flatten: shape (4,) â†’ 1D array
- **Benefit:** Easier to iterate!

**The Loop:**
```python
for i, m in enumerate(modalities):
    axes[i].hist(no_frame_df[m], bins=30, ...)
```

- **`bins=30`:** Number of bars in histogram
  - More bins â†’ higher precision, narrower bars
  - Fewer bins â†’ wider bars, less precision
  - 30 is reasonable balance

**What does the histogram reveal?**
- **Distribution:** Most patients have 120-160 images
- **Peak:** Around 130 images
- **Range:** From about 100 to 180 images
- **Shape:** Resembles normal distribution (bell curve)

**Why is this important?**
- Know range for processing planning
- Detect if there are outliers
- Decide: do we need padding/truncation for standardization?

#### Cell 21: Finding Minimum

```python
no_frame_df['Flair'].values.min()
```
- **`['Flair']`:** Selects column
- **`.values`:** Converts from pandas Series to numpy array
- **`.min()`:** Finds smallest value

**Expected output:**
```
The minimum number of slices with **Flair** modalities 99
```

**Why same number (99) for all?**
- Same patient has same number of slices for all types
- Patient with fewest images = 99 images

**Why is this important?**
- **Padding:** If we want to standardize length, must know minimum
- **Truncation:** Or cut to minimum (99) to save memory
- **Planning:** Know all patients have at least 99 images

---
# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø« | Continuation Part 3

---

### âœ… Cell 24-26: Ø¨Ø¯Ø§ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ù…Ù‚ | Deep Data Analysis Start

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 24-25: Ø¹Ù†Ø§ÙˆÙŠÙ† ØªÙˆØ¶ÙŠØ­ÙŠØ©
```markdown
## Data Analysis and Discovery
#### 1 - Patient Slices
```

- ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù€ Notebook
- Ù†Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù† Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù†ÙØ³Ù‡Ø§ (Ù„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª)

#### Cell 26: Ø´Ø±Ø­ Ø¯Ø§Ù„Ø© load_dicom

```markdown
Now, to show the image itself, we will create a func to read DICOM files

It extracts the pixel data as a NumPy array (dicom.pixel_array), then **normalizes** the pixel values by subtracting the minimum and dividing by the maximum, ensuring the values fall within the range [0, 1] for preprocessing
```

**Ù…Ø§ ÙŠØ´Ø±Ø­Ù‡:**
- Ø³Ù†Ø¨Ù†ÙŠ Ø¯Ø§Ù„Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© DICOM
- Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (normalization)
- Ø§Ù„Ù‡Ø¯Ù: Ù‚ÙŠÙ… Ù…Ù† 0 Ø¥Ù„Ù‰ 1

---

### âœ… Cell 27: Ø¯Ø§Ù„Ø© load_dicom - Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„Ù†Ø§Ø¨Ø¶ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ | load_dicom Function

```python
def load_dicom(path, visualize=False):
    dicom = pydicom.dcmread(path)
    data = dicom.pixel_array.astype(np.float32)

    #Normalize intensity range to [0, 1]
    data = data - np.min(data)
    if np.max(data) != 0:
        data = data / np.max(data)

    #Optionally scale to [0, 255] for visualization
    if visualize:
        data = (data * 255).astype(np.uint8)
    return data
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù‡ÙŠ **Ø£Ù‡Ù… Ø¯Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù€ Notebook**! Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­Ù‡Ø§ Ø³Ø·Ø±Ø§Ù‹ Ø¨Ø³Ø·Ø± Ø¨ØªÙØµÙŠÙ„ Ù…Ù…Ù„:

#### Ø§Ù„Ø³Ø·Ø± 1: ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¯Ø§Ù„Ø©

```python
def load_dicom(path, visualize=False):
```

**Parameters:**
- **`path`:** Ù…Ø³Ø§Ø± Ù…Ù„Ù DICOM (string)
  - Ù…Ø«Ø§Ù„: `"/kaggle/input/.../train/00000/FLAIR/Image-100.dcm"`
- **`visualize=False`:** Ù…Ø¹Ø§Ù…Ù„ Ø§Ø®ØªÙŠØ§Ø±ÙŠ (boolean)
  - `False` (Ø§ÙØªØ±Ø§Ø¶ÙŠ): Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (values 0-1)
  - `True`: Ù„Ù„Ø¹Ø±Ø¶ (values 0-255)

**Ù„Ù…Ø§Ø°Ø§ Ù…Ø¹Ø§Ù…Ù„ÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†ØŸ**
- **Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (0-1):** 
  - Ø£ÙØ¶Ù„ Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø±ÙŠØ§Ø¶ÙŠØ©
  - ÙŠÙ…Ù†Ø¹ overflow
  - Ù…Ø¹ÙŠØ§Ø± ÙÙŠ deep learning
- **Ù„Ù„Ø¹Ø±Ø¶ (0-255):**
  - matplotlib ÙŠØªÙˆÙ‚Ø¹ 0-255 Ù„Ù„ØµÙˆØ± Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ©
  - Ø£Ø³Ù‡Ù„ Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¨Ø´Ø±ÙŠ

#### Ø§Ù„Ø³Ø·Ø± 2: Ù‚Ø±Ø§Ø¡Ø© DICOM

```python
dicom = pydicom.dcmread(path)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ØŸ**
1. ÙŠÙØªØ­ Ø§Ù„Ù…Ù„Ù .dcm
2. ÙŠÙ‚Ø±Ø£ metadata (Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶ØŒ Ø¬Ù‡Ø§Ø² Ø§Ù„ØªØµÙˆÙŠØ±ØŒ Ø§Ù„ØªØ§Ø±ÙŠØ®...)
3. ÙŠÙ‚Ø±Ø£ pixel data (Ø§Ù„ØµÙˆØ±Ø© Ù†ÙØ³Ù‡Ø§)
4. ÙŠØ®Ø²Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ ÙƒØ§Ø¦Ù† `dicom`

**Ù…Ø­ØªÙˆÙŠØ§Øª ÙƒØ§Ø¦Ù† dicom:**
```python
dicom.PatientID           # Ø±Ù‚Ù… Ø§Ù„Ù…Ø±ÙŠØ¶
dicom.StudyDate           # ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ­Øµ
dicom.Modality            # Ù†ÙˆØ¹ Ø§Ù„ØªØµÙˆÙŠØ± (MR)
dicom.pixel_array         # Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ…ØµÙÙˆÙØ© â­
dicom.Rows                # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ
dicom.Columns             # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# ... ÙˆØ£ÙƒØ«Ø± Ù…Ù† 100 Ø­Ù‚Ù„ Ø¢Ø®Ø±!
```

#### Ø§Ù„Ø³Ø·Ø± 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©

```python
data = dicom.pixel_array.astype(np.float32)
```

**ØªØ­Ù„ÙŠÙ„ Ù‚Ø·Ø¹Ø© Ù‚Ø·Ø¹Ø©:**

**`dicom.pixel_array`:**
- ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„ØµÙˆØ±Ø© ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† metadata)
- Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø£ØµÙ„ÙŠ: Ø¹Ø§Ø¯Ø© `uint16` (0 to 65535)
- Ø§Ù„Ø´ÙƒÙ„: (512, 512) Ø£Ùˆ (256, 256) Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø²

**`.astype(np.float32)`:**
- ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†ÙˆØ¹ Ø¥Ù„Ù‰ float32
- **Ù„Ù…Ø§Ø°Ø§ float32ØŸ**
  - Ù†Ø­ØªØ§Ø¬ float Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© (Ù‚Ø³Ù…Ø©ØŒ Ø·Ø±Ø­)
  - float32 Ø£Ø®Ù Ù…Ù† float64 (ÙŠÙˆÙØ± Ø°Ø§ÙƒØ±Ø©)
  - ÙƒØ§ÙÙŠ Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

**Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
```python
# Ù‚Ø¨Ù„:
data = [[0, 1000, 2000, ...],     # uint16
        [500, 1500, 2500, ...],
        ...]
min = 0, max = 4095

# Ø¨Ø¹Ø¯ astype:
data = [[0.0, 1000.0, 2000.0, ...],  # float32
        [500.0, 1500.0, 2500.0, ...],
        ...]
```

#### Ø§Ù„Ø³Ø·Ø± 5-7: Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Normalization) - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£Ù‡Ù…!

```python
data = data - np.min(data)
if np.max(data) != 0:
    data = data / np.max(data)
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¶Ø±ÙˆØ±ÙŠØŸ**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:**
- ØµÙˆØ± DICOM Ù„Ù‡Ø§ Ù†Ø·Ø§Ù‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©:
  - ØµÙˆØ±Ø© 1: [0, 4095]
  - ØµÙˆØ±Ø© 2: [100, 2000]
  - ØµÙˆØ±Ø© 3: [500, 3500]
- Ù‡Ø°Ø§ ÙŠØ³Ø¨Ø¨ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨

**Ø§Ù„Ø­Ù„ - Min-Max Normalization:**

**Ø§Ù„Ø®Ø·ÙˆØ© 1:**
```python
data = data - np.min(data)
```

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ**
- ÙŠØ·Ø±Ø­ Ø£ØµØºØ± Ù‚ÙŠÙ…Ø© Ù…Ù† ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ…
- **Ø§Ù„Ù‡Ø¯Ù:** Ø¬Ø¹Ù„ Ø£ØµØºØ± Ù‚ÙŠÙ…Ø© = 0

**Ù…Ø«Ø§Ù„:**
```python
# Ù‚Ø¨Ù„:
data = [[100, 200, 300],
        [150, 250, 350]]
min = 100

# Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø±Ø­:
data = [[0, 100, 200],      # 100-100, 200-100, 300-100
        [50, 150, 250]]     # 150-100, 250-100, 350-100
min = 0, max = 250
```

**Ø§Ù„Ø®Ø·ÙˆØ© 2:**
```python
if np.max(data) != 0:
    data = data / np.max(data)
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„Ø´Ø±Ø· `if np.max(data) != 0`ØŸ**
- **Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±!**
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹ (ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… = 0):
  - Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø±Ø­: max = 0
  - Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø´Ø±Ø·: `data / 0` â†’ Ø®Ø·Ø£ Ø£Ùˆ inf
  - Ù…Ø¹ Ø§Ù„Ø´Ø±Ø·: Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù‚Ø³Ù…Ø©ØŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¨Ù‚Ù‰ 0

**Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ Ø§Ù„Ù‚Ø³Ù…Ø©ØŸ**
- ØªÙ‚Ø³Ù… ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø¹Ù„Ù‰ Ø£ÙƒØ¨Ø± Ù‚ÙŠÙ…Ø©
- **Ø§Ù„Ù‡Ø¯Ù:** Ø¬Ø¹Ù„ Ø£ÙƒØ¨Ø± Ù‚ÙŠÙ…Ø© = 1

**Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„:**
```python
# Ø¨ÙŠØ§Ù†Ø§Øª Ø£ØµÙ„ÙŠØ©:
data = [[100, 200, 300],
        [150, 250, 350]]

# Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø±Ø­:
data = [[0, 100, 200],
        [50, 150, 250]]
min = 0, max = 250

# Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ max:
data = [[0/250, 100/250, 200/250],     # [0.0, 0.4, 0.8]
        [50/250, 150/250, 250/250]]    # [0.2, 0.6, 1.0]
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¢Ù† ÙÙŠ Ù†Ø·Ø§Ù‚ **[0, 1]**
- 0 = Ø£Ø³ÙˆØ¯ (Ø£ØºÙ…Ù‚ Ù†Ù‚Ø·Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©)
- 1 = Ø£Ø¨ÙŠØ¶ (Ø£ÙØªØ­ Ù†Ù‚Ø·Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©)

**Ù„Ù…Ø§Ø°Ø§ [0, 1] Ø£ÙØ¶Ù„ Ù…Ù† [0, 4095]ØŸ**
1. **ØªÙˆØ­ÙŠØ¯:** ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚
2. **Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¹Ø¯Ø¯ÙŠ:** Ø£Ø±Ù‚Ø§Ù… ØµØºÙŠØ±Ø© â†’ Ø­Ø³Ø§Ø¨Ø§Øª Ø£Ø¯Ù‚
3. **Ù…Ø¹ÙŠØ§Ø±:** ÙƒÙ„ Ù…ÙƒØªØ¨Ø§Øª deep learning ØªØªÙˆÙ‚Ø¹ [0, 1]
4. **ØªØ¬Ù†Ø¨ overflow:** Ø¹Ù…Ù„ÙŠØ§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ø¢Ù…Ù†Ø©

#### Ø§Ù„Ø³Ø·Ø± 9-11: ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø¹Ø±Ø¶ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

```python
if visualize:
    data = (data * 255).astype(np.uint8)
```

**Ù…ØªÙ‰ ÙŠÙ†ÙØ° Ù‡Ø°Ø§ØŸ**
- ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ `visualize=True`
- Ù„Ù„Ø¹Ø±Ø¶ Ø¨Ù€ matplotlib

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ØŸ**

**Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¶Ø±Ø¨ ÙÙŠ 255**
```python
data = data * 255
```
- ÙŠØ­ÙˆÙ„ Ù…Ù† [0, 1] Ø¥Ù„Ù‰ [0, 255]
- Ù…Ø«Ø§Ù„:
  ```python
  # Ù‚Ø¨Ù„:
  data = [[0.0, 0.4, 0.8],
          [0.2, 0.6, 1.0]]
  
  # Ø¨Ø¹Ø¯ Ø§Ù„Ø¶Ø±Ø¨:
  data = [[0.0, 102.0, 204.0],
          [51.0, 153.0, 255.0]]
  ```

**Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ uint8**
```python
.astype(np.uint8)
```
- ÙŠØ­ÙˆÙ„ Ù…Ù† float32 Ø¥Ù„Ù‰ uint8
- uint8: Ø£Ø¹Ø¯Ø§Ø¯ ØµØ­ÙŠØ­Ø© Ù…Ù† 0 Ø¥Ù„Ù‰ 255
- **Ù„Ù…Ø§Ø°Ø§ØŸ**
  - matplotlib ÙŠØªÙˆÙ‚Ø¹ uint8 Ù„Ù„ØµÙˆØ± Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ©
  - ÙŠÙˆÙØ± Ø°Ø§ÙƒØ±Ø© (1 byte Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 4 bytes)
  - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø· (Ù„ÙŠØ³ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©)

**Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„:**
```python
# Ù‚Ø¨Ù„:
data = [[0.0, 0.4, 1.0]]  # float32, range [0,1]

# Ø¨Ø¹Ø¯ Ø§Ù„Ø¶Ø±Ø¨:
data = [[0.0, 102.0, 255.0]]  # float32, range [0,255]

# Ø¨Ø¹Ø¯ astype:
data = [[0, 102, 255]]  # uint8, range [0,255]
```

#### Ø§Ù„Ø³Ø·Ø± 12: Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹

```python
return data
```
- ÙŠØ±Ø¬Ø¹ Ø§Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

**Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**

**Ø§Ù„Ø­Ø§Ù„Ø© 1: Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©**
```python
img = load_dicom(path, visualize=False)
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: float32 array, values [0, 1]
# Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ: preprocessing, model input
```

**Ø§Ù„Ø­Ø§Ù„Ø© 2: Ù„Ù„Ø¹Ø±Ø¶**
```python
img = load_dicom(path, visualize=True)
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: uint8 array, values [0, 255]
# Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ: plt.imshow()
```

**Ø£Ø³Ø¦Ù„Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ù‡Ù…Ø©:**

**Ø³1: Ù„Ù…Ø§Ø°Ø§ Ù†Ø·Ø¨Ù‘Ø¹ min-max ÙˆÙ„ÙŠØ³ z-scoreØŸ**
```python
# Min-Max (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…):
data = (data - min) / (max - min)  # Ù†Ø·Ø§Ù‚ [0, 1]

# Z-score (Ø¨Ø¯ÙŠÙ„):
data = (data - mean) / std  # Ù†Ø·Ø§Ù‚ [-âˆ, +âˆ]
```
**Ø§Ù„Ø¬ÙˆØ§Ø¨:**
- Min-Max Ø£ÙØ¶Ù„ Ù„Ù„ØµÙˆØ± Ù„Ø£Ù†:
  - Ù†Ø·Ø§Ù‚ Ù…Ø­Ø¯Ø¯ [0, 1]
  - Ø³Ù‡Ù„ Ø§Ù„ØªÙØ³ÙŠØ±
  - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ø±Ø¶
- Z-score Ø£ÙØ¶Ù„ Ù„Ù€ features ÙÙŠ ML

**Ø³2: Ù„Ù…Ø§Ø°Ø§ Ù†Ø·Ø¨Ù‘Ø¹ ÙƒÙ„ ØµÙˆØ±Ø© Ø¨Ù…ÙØ±Ø¯Ù‡Ø§ØŸ**
**Ø§Ù„Ø¬ÙˆØ§Ø¨:**
- ÙƒÙ„ ØµÙˆØ±Ø© Ù„Ù‡Ø§ Ø³Ø·ÙˆØ¹ Ù…Ø®ØªÙ„Ù
- ØªØ·Ø¨ÙŠØ¹ global (Ù„ÙƒÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©) ÙŠØ­ØªØ§Ø¬ Ø­Ø³Ø§Ø¨ statistics Ù…Ø³Ø¨Ù‚Ø§Ù‹
- ØªØ·Ø¨ÙŠØ¹ per-image Ø£Ø¨Ø³Ø· ÙˆØ£Ø³Ø±Ø¹

**Ø³3: Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹ØŸ**
```python
data = [[0, 0, 0],
        [0, 0, 0]]

# Ø¨Ø¹Ø¯ data - min:
data = [[0, 0, 0],
        [0, 0, 0]]  # max = 0

# Ø§Ù„Ù‚Ø³Ù…Ø©:
if np.max(data) != 0:  # False, Ù†ØªØ®Ø·Ù‰ Ø§Ù„Ù‚Ø³Ù…Ø©
    data = data / np.max(data)

# Ø§Ù„Ù†ØªÙŠØ¬Ø©: data ØªØ¨Ù‚Ù‰ [[0,0,0], [0,0,0]]
```

**Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©:**

**Ø®Ø·Ø£ âŒ 1: Ù†Ø³ÙŠØ§Ù† astype(float32)**
```python
# Ø®Ø·Ø£:
data = dicom.pixel_array  # uint16
data = data - np.min(data)  # Ù„Ø§ ÙŠØ²Ø§Ù„ uint16!
data = data / np.max(data)  # âŒ integer division!

# Ø§Ù„Ù†ØªÙŠØ¬Ø©: ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… = 0 Ø£Ùˆ 1 ÙÙ‚Ø·!
```

**Ø®Ø·Ø£ âŒ 2: Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† max = 0**
```python
# Ø®Ø·Ø£:
data = data - np.min(data)
data = data / np.max(data)  # âŒ Ù‚Ø¯ ÙŠÙ‚Ø³Ù… Ø¹Ù„Ù‰ ØµÙØ±!
```

**Ø®Ø·Ø£ âŒ 3: ØªØ·Ø¨ÙŠØ¹ Ø®Ø§Ø·Ø¦**
```python
# Ø®Ø·Ø£:
data = data / 255  # âŒ ÙŠÙØªØ±Ø¶ Ø£Ù† max = 255ØŒ Ù„ÙƒÙ† Ù‚Ø¯ ÙŠÙƒÙˆÙ† 4095!

# ØµØ­ÙŠØ­:
data = data / np.max(data)  # âœ… ÙŠØ³ØªØ®Ø¯Ù… max Ø§Ù„ÙØ¹Ù„ÙŠ
```

**ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ù…ÙƒÙ†Ø©:**

```python
def load_dicom_improved(path, visualize=False, clip_percentile=None):
    """
    Ù…Ø­Ø³Ù‘Ù† Ø¨Ø¥Ø¶Ø§ÙØ© percentile clipping
    """
    dicom = pydicom.dcmread(path)
    data = dicom.pixel_array.astype(np.float32)
    
    # Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ù‚Øµ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© (outliers)
    if clip_percentile:
        lower = np.percentile(data, clip_percentile)
        upper = np.percentile(data, 100 - clip_percentile)
        data = np.clip(data, lower, upper)
    
    # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
    data = data - np.min(data)
    if np.max(data) != 0:
        data = data / np.max(data)
    
    if visualize:
        data = (data * 255).astype(np.uint8)
    
    return data
```

**In English:**

This function is **the most important function in the notebook**! Let me explain it line by line:

#### Line 1: Function Definition
```python
def load_dicom(path, visualize=False):
```
**Parameters:**
- **`path`:** DICOM file path
- **`visualize=False`:** Optional boolean
  - `False` (default): for processing (values 0-1)
  - `True`: for display (values 0-255)

#### Line 2: Reading DICOM
```python
dicom = pydicom.dcmread(path)
```
- Opens .dcm file
- Reads metadata and pixel data

#### Line 3: Extracting Image
```python
data = dicom.pixel_array.astype(np.float32)
```
- Extracts only the image
- Converts to float32 for math operations

#### Lines 5-7: Normalization - Most Important Part!
```python
data = data - np.min(data)
if np.max(data) != 0:
    data = data / np.max(data)
```

**Why normalize?**
- DICOM images have different ranges
- Normalization standardizes to [0, 1]

**Step 1:** Subtract minimum â†’ makes min = 0
**Step 2:** Divide by maximum â†’ makes max = 1

**Why check `if np.max(data) != 0`?**
- Protection from division by zero!
- If image is completely black (all zeros)

**Why [0, 1] is better than [0, 4095]?**
1. **Standardization:** All images same range
2. **Numerical stability:** Small numbers â†’ more accurate calculations
3. **Standard:** All deep learning libraries expect [0, 1]
4. **Avoid overflow:** Safe math operations

#### Lines 9-11: Convert for Visualization
```python
if visualize:
    data = (data * 255).astype(np.uint8)
```
- Only when `visualize=True`
- Converts [0, 1] to [0, 255]
- Changes to uint8 for matplotlib

**Common Mistakes:**

**Mistake âŒ 1: Forgetting astype(float32)**
```python
data = dicom.pixel_array  # uint16
data = data / np.max(data)  # âŒ integer division!
```

**Mistake âŒ 2: Not checking max = 0**
```python
data = data / np.max(data)  # âŒ may divide by zero!
```

**Mistake âŒ 3: Wrong normalization**
```python
data = data / 255  # âŒ assumes max = 255, but could be 4095!
```

---

### âœ… Cell 28: Ø¯Ø§Ù„Ø© visualize_middle_slices - Ø¹Ø±Ø¶ ØµÙˆØ± Ø§Ù„Ù…Ø±ÙŠØ¶

```python
def visualize_middle_slices(patient_id, slice_i, mgmt_value, types=("FLAIR", "T1w", "T1wCE", "T2w")):
    
    plt.figure(figsize=(16, 5))
    patient_path = os.path.join(TRAIN_DATA_PATH, patient_id)
    
    for i, t in enumerate(types, 1):
        t_paths = sorted(
            glob.glob(os.path.join(patient_path, t, "*")), 
            key=lambda x: int(x[:-4].split("-")[-1]),
        )
        data = load_dicom(t_paths[int(len(t_paths) * slice_i)], visualize=True)
        plt.subplot(1, 4, i)
        plt.imshow(data, cmap="bone")
        plt.title(f"{t}, MGMT_value={mgmt_value}")
        plt.axis("off")
    
    plt.tight_layout()
    plt.show()
```

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

Ø¯Ø§Ù„Ø© Ù„Ø¹Ø±Ø¶ 4 Ø£Ù†ÙˆØ§Ø¹ MRI Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯ ÙÙŠ slice Ù…Ø­Ø¯Ø¯.

#### ØªØ­Ù„ÙŠÙ„ Parameters:

```python
def visualize_middle_slices(patient_id, slice_i, mgmt_value, types=(...)):
```

- **`patient_id`:** Ø±Ù‚Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ (string) Ù…Ø«Ù„ "00000"
- **`slice_i`:** Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù€ slice (float Ù…Ù† 0 Ø¥Ù„Ù‰ 1)
  - 0.0 = Ø£ÙˆÙ„ ØµÙˆØ±Ø©
  - 0.5 = ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
  - 1.0 = Ø¢Ø®Ø± ØµÙˆØ±Ø©
- **`mgmt_value`:** Ø§Ù„ØªØµÙ†ÙŠÙ (0 Ø£Ùˆ 1) - Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·
- **`types`:** tuple Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ MRI

#### Ø§Ù„Ø³Ø·Ø± 3: Ø¥Ù†Ø´Ø§Ø¡ Figure

```python
plt.figure(figsize=(16, 5))
```
- **`(16, 5)`:** Ø¹Ø±ÙŠØ¶ (16) ÙˆÙ‚ØµÙŠØ± (5)
- Ù„Ù…Ø§Ø°Ø§ØŸ Ù„Ø£Ù†Ù†Ø§ Ù†Ø¹Ø±Ø¶ 4 ØµÙˆØ± Ø¬Ù†Ø¨Ø§Ù‹ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨

#### Ø§Ù„Ø³Ø·Ø± 4: Ø¨Ù†Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø±ÙŠØ¶

```python
patient_path = os.path.join(TRAIN_DATA_PATH, patient_id)
```
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: `"/kaggle/input/.../train/00000"`

#### Ø§Ù„Ø³Ø·Ø± 6-9: Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

```python
for i, t in enumerate(types, 1):
```
- **`enumerate(types, 1)`:** ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø¹Ø¯ Ù…Ù† 1 (Ù„ÙŠØ³ 0)
  - i=1, t="FLAIR"
  - i=2, t="T1w"
  - i=3, t="T1wCE"
  - i=4, t="T2w"
- **Ù„Ù…Ø§Ø°Ø§ Ù†Ø¨Ø¯Ø£ Ù…Ù† 1ØŸ** Ù„Ø£Ù† `plt.subplot(1, 4, i)` ÙŠØªÙˆÙ‚Ø¹ 1-4 (Ù„ÙŠØ³ 0-3)

#### Ø§Ù„Ø³Ø·Ø± 7-10: Ø¬Ù„Ø¨ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ±

```python
t_paths = sorted(
    glob.glob(os.path.join(patient_path, t, "*")), 
    key=lambda x: int(x[:-4].split("-")[-1]),
)
```

**Ø¯Ø¹Ù†ÙŠ Ø£ÙÙƒÙƒÙ‡ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„:**

**Ø§Ù„Ø¬Ø²Ø¡ 1:**
```python
glob.glob(os.path.join(patient_path, t, "*"))
```
- ÙŠØ¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ÙˆØ¹
- Ù…Ø«Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©:
  ```python
  ['/path/FLAIR/Image-1.dcm',
   '/path/FLAIR/Image-100.dcm',
   '/path/FLAIR/Image-2.dcm',
   '/path/FLAIR/Image-10.dcm']
  ```

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** Ø§Ù„ØªØ±ØªÙŠØ¨ alphabetical ÙˆÙ„ÙŠØ³ Ø¹Ø¯Ø¯ÙŠ!
```
Image-1.dcm
Image-10.dcm   â† Ø®Ø·Ø£! ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨Ø¹Ø¯ Image-2
Image-100.dcm
Image-2.dcm
```

**Ø§Ù„Ø¬Ø²Ø¡ 2: Ø§Ù„Ø­Ù„ - Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹Ø¯Ø¯ÙŠ**
```python
key=lambda x: int(x[:-4].split("-")[-1])
```

**Ø¯Ø¹Ù†ÙŠ Ø£Ø´Ø±Ø­ lambda function:**

**Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ path:**
```python
x = '/kaggle/input/.../FLAIR/Image-123.dcm'
```

**Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**1. `x[:-4]`** - Ø¥Ø²Ø§Ù„Ø© Ø¢Ø®Ø± 4 Ø£Ø­Ø±Ù (.dcm)
```python
'/kaggle/input/.../FLAIR/Image-123.dcm'[:-4]
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: '/kaggle/input/.../FLAIR/Image-123'
```

**2. `.split("-")`** - ØªÙ‚Ø³ÙŠÙ… Ø¹Ù†Ø¯ "-"
```python
'/kaggle/input/.../FLAIR/Image-123'.split("-")
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: ['/kaggle/input/.../FLAIR/Image', '123']
```

**3. `[-1]`** - Ø£Ø®Ø° Ø¢Ø®Ø± Ø¹Ù†ØµØ±
```python
['/kaggle/input/.../FLAIR/Image', '123'][-1]
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: '123'
```

**4. `int(...)`** - ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
```python
int('123')
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: 123
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
- Ù…Ù† path â†’ Ø±Ù‚Ù… Ø§Ù„ØµÙˆØ±Ø©
- `sorted()` ØªØ±ØªØ¨ Ø­Ø³Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù…

**Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±ØªÙŠØ¨:**
```python
['/path/FLAIR/Image-1.dcm',
 '/path/FLAIR/Image-2.dcm',
 '/path/FLAIR/Image-10.dcm',
 '/path/FLAIR/Image-100.dcm']  â† ØªØ±ØªÙŠØ¨ ØµØ­ÙŠØ­! âœ…
```

#### Ø§Ù„Ø³Ø·Ø± 11: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù€ Slice

```python
data = load_dicom(t_paths[int(len(t_paths) * slice_i)], visualize=True)
```

**ØªØ­Ù„ÙŠÙ„:**

**`len(t_paths) * slice_i`:**
- Ø¥Ø°Ø§ ÙƒØ§Ù† `len(t_paths) = 400` Ùˆ `slice_i = 0.5`:
  - `400 * 0.5 = 200.0`
  - `int(200.0) = 200`
  - Ø§Ù„Ù†ØªÙŠØ¬Ø©: `t_paths[200]` â†’ ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ

**Ø£Ù…Ø«Ù„Ø©:**
```python
# slice_i = 0.0 â†’ Ø£ÙˆÙ„ ØµÙˆØ±Ø©
t_paths[int(400 * 0.0)] = t_paths[0]

# slice_i = 0.5 â†’ Ù…Ù†ØªØµÙ
t_paths[int(400 * 0.5)] = t_paths[200]

# slice_i = 0.75 â†’ 75%
t_paths[int(400 * 0.75)] = t_paths[300]

# slice_i = 1.0 â†’ âš ï¸ Ø®Ø·Ø£ Ù…Ø­ØªÙ…Ù„!
t_paths[int(400 * 1.0)] = t_paths[400]  # âŒ index out of range!
```

**âš ï¸ Bug Ù…Ø­ØªÙ…Ù„:**
- Ø¥Ø°Ø§ `slice_i = 1.0`ØŒ Ø³ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù€ `t_paths[400]`
- Ù„ÙƒÙ† Ø¢Ø®Ø± index Ù‡Ùˆ `t_paths[399]`!
- **Ø§Ù„Ø­Ù„:**
  ```python
  index = min(int(len(t_paths) * slice_i), len(t_paths) - 1)
  ```

**Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ load_dicom:**
```python
load_dicom(..., visualize=True)
```
- `visualize=True` â†’ Ù†Ø±ÙŠØ¯ uint8 [0,255] Ù„Ù„Ø¹Ø±Ø¶

#### Ø§Ù„Ø³Ø·Ø± 12-15: Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø©

```python
plt.subplot(1, 4, i)
```
- ÙŠÙ†Ø´Ø¦ subplot ÙÙŠ Ù…ÙˆÙ‚Ø¹ i
- **`(1, 4, i)`:** 
  - 1 ØµÙ
  - 4 Ø£Ø¹Ù…Ø¯Ø©
  - Ø§Ù„Ù…ÙˆÙ‚Ø¹ i (1, 2, 3, Ø£Ùˆ 4)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ i=1    â”‚ i=2    â”‚ i=3    â”‚ i=4    â”‚
â”‚ FLAIR  â”‚ T1w    â”‚ T1wCE  â”‚ T2w    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
plt.imshow(data, cmap="bone")
```
- **`cmap="bone"`:** Ø®Ø±ÙŠØ·Ø© Ø£Ù„ÙˆØ§Ù†
  - bone: Ø£Ø¨ÙŠØ¶-Ø±Ù…Ø§Ø¯ÙŠ-Ø£Ø³ÙˆØ¯ (Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©)
  - Ø¨Ø¯Ø§Ø¦Ù„: "gray", "hot", "viridis"

```python
plt.title(f"{t}, MGMT_value={mgmt_value}")
```
- ÙŠØ¶Ø¹ Ø¹Ù†ÙˆØ§Ù†: "FLAIR, MGMT_value=1"

```python
plt.axis("off")
```
- ÙŠØ®ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆØ± (Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨)
- Ø§Ù„ØµÙˆØ±Ø© ØªØ¨Ø¯Ùˆ Ø£Ù†Ø¸Ù

#### Ø§Ù„Ø³Ø·Ø± 17-18: Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡

```python
plt.tight_layout()
plt.show()
```
- `tight_layout()`: ÙŠÙ†Ø¸Ù… Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
- `show()`: ÙŠØ¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ù…

**Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø©:**
```python
visualize_middle_slices(
    patient_id="00000",
    slice_i=0.5,        # Ù…Ù†ØªØµÙ
    mgmt_value=1
)
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
- 4 ØµÙˆØ± Ø¬Ù†Ø¨Ø§Ù‹ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨
- ÙƒÙ„Ù‡Ø§ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶
- ÙƒÙ„Ù‡Ø§ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù€ slice (Ø§Ù„Ù…Ù†ØªØµÙ)
- Ù„ÙƒÙ† Ø¨Ø£Ù†ÙˆØ§Ø¹ MRI Ù…Ø®ØªÙ„ÙØ©

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ù‡Ù…Ø©ØŸ**
1. **Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:** Ù†Ø±Ù‰ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø£Ù†ÙˆØ§Ø¹ MRI
2. **Ø§Ù„ÙØ­Øµ:** Ù†ØªØ£ÙƒØ¯ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
3. **Ø§Ù„ÙÙ‡Ù…:** Ù†ÙÙ‡Ù… Ù…Ø§ ÙŠØ¸Ù‡Ø±Ù‡ ÙƒÙ„ Ù†ÙˆØ¹ MRI

**In English:**

Function to display 4 MRI types for one patient at specific slice.

#### Parameter Analysis:
- **`patient_id`:** Patient number (string) like "00000"
- **`slice_i`:** Slice position (float from 0 to 1)
  - 0.0 = first image
  - 0.5 = middle image
  - 1.0 = last image
- **`mgmt_value`:** Classification (0 or 1) - for display only

#### Lines 7-10: Getting Image Paths
```python
t_paths = sorted(
    glob.glob(...), 
    key=lambda x: int(x[:-4].split("-")[-1]),
)
```

**Breaking down lambda:**
```python
x = '/path/FLAIR/Image-123.dcm'
x[:-4]                    # Remove .dcm â†’ '/path/FLAIR/Image-123'
.split("-")               # Split at "-" â†’ [..., '123']
[-1]                      # Last element â†’ '123'
int(...)                  # Convert to number â†’ 123
```

**Why?**
- Alphabetical sorting: Image-1, Image-10, Image-2 âŒ
- Numerical sorting: Image-1, Image-2, Image-10 âœ…

#### Line 11: Selecting Slice
```python
data = load_dicom(t_paths[int(len(t_paths) * slice_i)], visualize=True)
```

**Example:**
- If `len(t_paths) = 400` and `slice_i = 0.5`:
  - `400 * 0.5 = 200.0`
  - `int(200.0) = 200`
  - Result: `t_paths[200]` â†’ middle image

**âš ï¸ Potential Bug:**
- If `slice_i = 1.0`, tries to access `t_paths[400]`
- But last valid index is `t_paths[399]`!

---
# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹ | Continuation Part 4

---

### âœ… Cell 30-32: Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø§Ù„Ø© Ø§Ù„Ø¹Ø±Ø¶ | Testing Visualization Function

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 30: Ø¹Ø±Ø¶ Ù…Ø±ÙŠØ¶ Ø¨Ù€ MGMT=1

```python
visualize_middle_slices(patient_id="01007", slice_i=0.5, mgmt_value=1)
```

**ØªØ­Ù„ÙŠÙ„:**
- **`patient_id="01007"`:** Ù…Ø±ÙŠØ¶ Ø±Ù‚Ù… 01007
- **`slice_i=0.5`:** Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ ØªÙ…Ø§Ù…Ø§Ù‹
- **`mgmt_value=1`:** Ù‡Ø°Ø§ Ø§Ù„Ù…Ø±ÙŠØ¶ Ù„Ø¯ÙŠÙ‡ MGMT methylated (Ø¥ÙŠØ¬Ø§Ø¨ÙŠ)

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø¨Ø­Ø« Ø¹Ù†Ù‡ ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©ØŸ**
1. **Ù‡Ù„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ø¶Ø­Ø©ØŸ**
2. **Ù‡Ù„ Ø§Ù„Ø¯Ù…Ø§Øº Ù…Ø±Ø¦ÙŠ Ø¨ÙˆØ¶ÙˆØ­ØŸ**
3. **Ù‡Ù„ Ù‡Ù†Ø§Ùƒ ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©ØŸ**

**Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†ÙˆØ§Ø¹:**

**FLAIR:**
- ÙŠÙØ¸Ù‡Ø± Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ø¨ÙˆØ¶ÙˆØ­
- Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ (white matter lesions) ØªØ¸Ù‡Ø± Ø³Ø§Ø·Ø¹Ø©
- Ù…ÙÙŠØ¯ Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ÙˆØ°Ù…Ø© (edema) Ø­ÙˆÙ„ Ø§Ù„ÙˆØ±Ù…

**T1w:**
- ØªØ¨Ø§ÙŠÙ† ØªØ´Ø±ÙŠØ­ÙŠ Ø¬ÙŠØ¯
- ÙŠÙØ¸Ù‡Ø± Ø¨Ù†ÙŠØ© Ø§Ù„Ø¯Ù…Ø§Øº
- Ø§Ù„Ø£ÙˆØ±Ø§Ù… ØªØ¸Ù‡Ø± Ø¯Ø§ÙƒÙ†Ø© Ø¹Ø§Ø¯Ø©

**T1wCE (with Contrast):**
- **Ø§Ù„Ø£Ù‡Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£ÙˆØ±Ø§Ù…!**
- Ø¨Ø¹Ø¯ Ø­Ù‚Ù† Ù…Ø§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Gadolinium)
- Ø§Ù„Ø£ÙˆØ±Ø§Ù… ØªÙ…ØªØµ Ø§Ù„Ù…Ø§Ø¯Ø© â†’ ØªØ¸Ù‡Ø± Ø³Ø§Ø·Ø¹Ø©
- ÙŠÙØ¸Ù‡Ø± Ø­Ø¯ÙˆØ¯ Ø§Ù„ÙˆØ±Ù… Ø¨ÙˆØ¶ÙˆØ­

**T2w:**
- Ø­Ø³Ø§Ø³ Ù„Ù„Ù…Ø§Ø¡
- Ø§Ù„Ø£ÙˆØ±Ø§Ù… ÙˆØ§Ù„ÙˆØ°Ù…Ø© ØªØ¸Ù‡Ø± Ø³Ø§Ø·Ø¹Ø©
- ÙŠÙØ¸Ù‡Ø± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©

#### Cell 31-32: Ø¹Ø±Ø¶ Ù…Ø±ÙŠØ¶ÙŠÙ† Ø¨Ù€ MGMT=0

```python
visualize_middle_slices(patient_id="01010", slice_i=0.5, mgmt_value=0)
visualize_middle_slices(patient_id="01009", slice_i=0.5, mgmt_value=0)
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø¹Ø±Ø¶ Ù…Ø±Ø¶Ù‰ Ø¨Ù€ MGMT=0 Ùˆ MGMT=1ØŸ**

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„ÙŠÙ‡:**
"Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø±Ø¤ÙŠØ© ÙØ±Ù‚ Ø¨ØµØ±ÙŠ Ø¨ÙŠÙ† MGMT=0 Ùˆ MGMT=1ØŸ"

**Ø§Ù„Ø¬ÙˆØ§Ø¨:**
- **Ù„Ù„Ø£Ø³ÙØŒ Ø¹Ø§Ø¯Ø©Ù‹ Ù„Ø§!**
- Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† MGMT methylated Ùˆ non-methylated **Ø¬Ø²ÙŠØ¦ÙŠ** (Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬ÙŠÙ†Ø§Øª)
- Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø±Ø¤ÙŠØªÙ‡ Ø¨Ø§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ø¬Ø±Ø¯Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±
- **Ù„Ù‡Ø°Ø§ Ù†Ø­ØªØ§Ø¬ machine learning!**

**Ø¥Ø°Ù† Ù„Ù…Ø§Ø°Ø§ Ù†Ø¹Ø±Ø¶Ù‡Ù…ØŸ**
1. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** Ù‡Ù„ Ø§Ù„ØµÙˆØ± Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ
2. **Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ø§Ù…:** ÙƒÙŠÙ ØªØ¨Ø¯Ùˆ Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø¯Ù…Ø§ØºØŸ
3. **Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø·:** Ù‚Ø¯ ØªÙˆØ¬Ø¯ Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹

**In English:**

#### Cells 30-32: Testing Visualization

**What we're looking for:**
- Are images clear?
- Is brain visible clearly?
- Is there difference between the 4 types?

**Expected differences between types:**

**FLAIR:**
- Shows fluids clearly
- White matter lesions appear bright
- Useful for seeing edema around tumor

**T1w:**
- Good anatomical contrast
- Shows brain structure
- Tumors usually appear dark

**T1wCE (with Contrast):**
- **Most important for tumor detection!**
- After Gadolinium injection
- Tumors absorb contrast â†’ appear bright
- Shows tumor boundaries clearly

**T2w:**
- Sensitive to water
- Tumors and edema appear bright
- Shows fine details

**Why show both MGMT=0 and MGMT=1?**
- Question: "Can we see visual difference between MGMT=0 and MGMT=1?"
- Answer: **Usually no!**
- The difference is **molecular** (at gene level)
- Cannot be seen with naked eye
- **This is why we need machine learning!**

---

### âœ… Cell 33-36: Ø¥Ù†Ø´Ø§Ø¡ Animation | Creating Animation

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 33: Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ

```markdown
Very very nice! we see just the middle image but what if we need to see all images of each type not just middle one!

let's create animation
```

**Ø§Ù„ÙÙƒØ±Ø©:**
- Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© (Ø§Ù„Ù…Ù†ØªØµÙ)ØŒ Ù†Ø±ÙŠØ¯ Ø±Ø¤ÙŠØ© **ÙƒÙ„ Ø§Ù„ØµÙˆØ±** ÙƒÙÙŠØ¯ÙŠÙˆ
- Ù…Ø«Ù„ "ØªÙ‚Ù„ÙŠØ¨ Ø§Ù„ØµÙØ­Ø§Øª" Ø¹Ø¨Ø± Ø§Ù„Ø¯Ù…Ø§Øº Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø¥Ù„Ù‰ Ø£Ø³ÙÙ„

#### Cell 34: Ø¯Ø§Ù„Ø© create_animation

```python
rc('animation', html='jshtml')

def create_animation(images):
    fig = plt.figure(figsize=(6, 6))
    plt.axis('off')
    image = plt.imshow(images[0], cmap="gray")

    def animate_func(i):
        image.set_array(images[i])
        return [image]

    ani= animation.FuncAnimation(fig, animate_func, frames = len(images), interval = 1000//24)
    plt.close(fig)
    return ani
```

**ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ø¬Ø¯Ø§Ù‹:**

#### Ø§Ù„Ø³Ø·Ø± 1: Ø¥Ø¹Ø¯Ø§Ø¯ matplotlib

```python
rc('animation', html='jshtml')
```

**Ù…Ø§ Ù‡Ùˆ rcØŸ**
- rc = Runtime Configuration
- ÙŠØ¶Ø¨Ø· Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª matplotlib

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ `html='jshtml'`ØŸ**
- ÙŠØ®Ø¨Ø± matplotlib ÙƒÙŠÙ ÙŠØ¹Ø±Ø¶ Ø§Ù„Ù€ animation ÙÙŠ Jupyter
- **Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª:**
  - `'html5'`: ÙŠØ­ÙØ¸ ÙƒÙÙŠØ¯ÙŠÙˆ HTML5
  - `'jshtml'`: ÙŠØ³ØªØ®Ø¯Ù… JavaScript (ØªÙØ§Ø¹Ù„ÙŠØŒ ÙŠÙ…ÙƒÙ† Ø¥ÙŠÙ‚Ø§ÙÙ‡)
  - `'none'`: Ù„Ø§ ÙŠØ¹Ø±Ø¶

**Ù„Ù…Ø§Ø°Ø§ jshtmlØŸ**
- ØªÙØ§Ø¹Ù„ÙŠ (ÙŠÙ…ÙƒÙ† Ø¥ÙŠÙ‚Ø§Ù/ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ)
- Ø£Ø®Ù Ù…Ù† HTML5
- ÙŠØ¹Ù…Ù„ ÙÙŠ Jupyter Notebook

#### Ø§Ù„Ø³Ø·Ø± 3-5: Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¯Ø§Ù„Ø©

```python
def create_animation(images):
    fig = plt.figure(figsize=(6, 6))
    plt.axis('off')
```

**Parameters:**
- **`images`:** list Ù…Ù† Ø§Ù„ØµÙˆØ± (numpy arrays)
  - Ù…Ø«Ø§Ù„: `[image_1, image_2, ..., image_400]`

**Ø¥Ù†Ø´Ø§Ø¡ Figure:**
```python
fig = plt.figure(figsize=(6, 6))
```
- ÙŠÙ†Ø´Ø¦ figure ÙØ§Ø±ØºØ© 6Ã—6 Ø¥Ù†Ø´

**Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆØ±:**
```python
plt.axis('off')
```
- ÙŠØ®ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨

#### Ø§Ù„Ø³Ø·Ø± 6: Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰

```python
image = plt.imshow(images[0], cmap="gray")
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŸ**
- Ù„Ù†Ø¹Ø±Ù‘Ù object Ø³Ù†Ø­Ø¯Ø«Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
- `image` Ù‡Ù†Ø§ Ù„ÙŠØ³ Ù…ØµÙÙˆÙØ©ØŒ Ø¨Ù„ **AxesImage object**
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ reference Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©

**Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†:**
```python
# Ù‡Ø°Ø§:
image = plt.imshow(images[0], cmap="gray")

# ÙˆÙ‡Ø°Ø§:
plt.imshow(images[0], cmap="gray")
```
**Ø§Ù„Ø¬ÙˆØ§Ø¨:**
- Ø§Ù„Ø£ÙˆÙ„: ÙŠØ­ÙØ¸ reference ÙÙŠ Ù…ØªØºÙŠØ± `image`
- Ø§Ù„Ø«Ø§Ù†ÙŠ: ÙŠØ¹Ø±Ø¶ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ reference
- Ù†Ø­ØªØ§Ø¬ reference Ù„Ù„ØªØ­Ø¯ÙŠØ« Ù„Ø§Ø­Ù‚Ø§Ù‹!

#### Ø§Ù„Ø³Ø·Ø± 8-10: Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ø±ÙŠÙƒ

```python
def animate_func(i):
    image.set_array(images[i])
    return [image]
```

**Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ø¯Ø§Ø®Ù„ÙŠØ© (nested function)!**

**Parameters:**
- **`i`:** Ø±Ù‚Ù… Ø§Ù„Ø¥Ø·Ø§Ø± (frame number)
  - Ø³ÙŠÙØ³ØªØ¯Ø¹Ù‰ Ø¨Ù€: 0, 1, 2, ..., len(images)-1

**Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ØŸ**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
image.set_array(images[i])
```
- **`image`:** Ø§Ù„Ù€ AxesImage object Ù…Ù† Ø§Ù„Ø³Ø·Ø± 6
- **`.set_array()`:** ÙŠØ³ØªØ¨Ø¯Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©
- **`images[i]`:** Ø§Ù„ØµÙˆØ±Ø© Ø±Ù‚Ù… i

**Ù…Ø«Ø§Ù„:**
```python
# Ø§Ù„Ø¥Ø·Ø§Ø± 0:
image.set_array(images[0])  # ÙŠØ¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰

# Ø§Ù„Ø¥Ø·Ø§Ø± 1:
image.set_array(images[1])  # ÙŠØ³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ©

# Ø§Ù„Ø¥Ø·Ø§Ø± 2:
image.set_array(images[2])  # ÙŠØ³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø«Ø§Ù„Ø«Ø©

# ... ÙˆÙ‡ÙƒØ°Ø§
```

**Ø§Ù„Ø³Ø·Ø± 2:**
```python
return [image]
```
- ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù€ artists Ø§Ù„Ù…Ø­Ø¯Ø«Ø©
- matplotlib ÙŠØ­ØªØ§Ø¬ Ù‡Ø°Ø§ Ù„Ù„Ø±Ø³Ù…
- **Ù„Ù…Ø§Ø°Ø§ Ù‚Ø§Ø¦Ù…Ø©ØŸ** Ù„Ø£Ù† Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© objects Ù…Ø­Ø¯Ø«Ø©

#### Ø§Ù„Ø³Ø·Ø± 12: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ Animation

```python
ani = animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000//24)
```

**Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ù‡Ùˆ Ø§Ù„Ù‚Ù„Ø¨!**

**ØªØ­Ù„ÙŠÙ„ Parameters:**

**1. `fig`:**
- Ø§Ù„Ù€ figure Ø§Ù„ØªÙŠ Ø³Ù†Ø±Ø³Ù… Ø¹Ù„ÙŠÙ‡Ø§

**2. `animate_func`:**
- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙŠ ØªÙØ³ØªØ¯Ø¹Ù‰ Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±
- Ø³ØªÙØ³ØªØ¯Ø¹Ù‰ Ù…Ø±Ø§Ø±Ø§Ù‹ Ù…Ø¹ i Ù…Ø®ØªÙ„Ù

**3. `frames=len(images)`:**
- Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
- Ø¥Ø°Ø§ ÙƒØ§Ù† `len(images) = 400`ØŒ Ø³ÙŠÙÙ†Ø´Ø¦ 400 Ø¥Ø·Ø§Ø±
- Ø³ÙŠØ³ØªØ¯Ø¹ÙŠ `animate_func(0)`, `animate_func(1)`, ..., `animate_func(399)`

**4. `interval=1000//24`:**
- Ø§Ù„ÙØªØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª **Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©**
- `1000//24 â‰ˆ 41.67 ms`
- **Ù„Ù…Ø§Ø°Ø§ 1000//24ØŸ**

**Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª (FPS):**
```python
# Ù†Ø±ÙŠØ¯ 24 Ø¥Ø·Ø§Ø± ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ© (24 FPS)
# Ø§Ù„Ø«Ø§Ù†ÙŠØ© = 1000 Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
# Ø§Ù„ÙˆÙ‚Øª Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø± = 1000 / 24

1000 // 24 = 41  # Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±

# Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª = 1000 / 41 â‰ˆ 24.4 FPS
```

**Ù„Ù…Ø§Ø°Ø§ 24 FPSØŸ**
- 24 FPS = Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ø³ÙŠÙ†Ù…Ø§
- Ø³Ù„Ø³ Ù„Ù„Ø¹ÙŠÙ† Ø§Ù„Ø¨Ø´Ø±ÙŠØ©
- Ù„ÙŠØ³ Ø³Ø±ÙŠØ¹Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ (ÙŠØµØ¹Ø¨ Ø§Ù„Ø±Ø¤ÙŠØ©)
- Ù„ÙŠØ³ Ø¨Ø·ÙŠØ¦Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ (ÙŠØ¨Ø¯Ùˆ Ù…ØªÙ‚Ø·Ø¹Ø§Ù‹)

**Ø¨Ø¯Ø§Ø¦Ù„:**
```python
interval = 1000//12  # 12 FPS - Ø¨Ø·ÙŠØ¡ØŒ Ø¬ÙŠØ¯ Ù„Ù„ÙØ­Øµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
interval = 1000//24  # 24 FPS - Ù…ØªÙˆØ³Ø· âœ…
interval = 1000//30  # 30 FPS - Ø³Ø±ÙŠØ¹
interval = 1000//60  # 60 FPS - Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹
```

#### Ø§Ù„Ø³Ø·Ø± 13-14: Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡

```python
plt.close(fig)
return ani
```

**`plt.close(fig)`:**
- ÙŠØºÙ„Ù‚ Ø§Ù„Ù€ figure
- **Ù„Ù…Ø§Ø°Ø§ØŸ** Ù„Ù…Ù†Ø¹ Ø¹Ø±Ø¶Ù‡Ø§ Ù…Ø±ØªÙŠÙ†
- Animation Ø³ÙŠÙØ¹Ø±Ø¶ Ø¨Ù†ÙØ³Ù‡

**`return ani`:**
- ÙŠØ±Ø¬Ø¹ ÙƒØ§Ø¦Ù† Animation
- Jupyter Ø³ÙŠØ¹Ø±Ø¶Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹

#### Cell 35: Ø¯Ø§Ù„Ø© get_modality_slices

```python
def get_modality_slices(modality_path):
    t_paths = sorted(
        glob.glob(os.path.join(modality_path, "*")), 
        key=lambda x: int(x[:-4].split("-")[-1]),
    )
    images = []
    for filename in t_paths:
        image = load_dicom(filename, visualize=True)
        if image.max() == 0:
            continue
        images.append(image)
        
    return images
```

**ØªØ­Ù„ÙŠÙ„:**

**Ø§Ù„Ù‡Ø¯Ù:**
- Ø¬Ù„Ø¨ ÙƒÙ„ ØµÙˆØ± Ù†ÙˆØ¹ Ù…Ø¹ÙŠÙ† (Ù…Ø«Ù„Ø§Ù‹ FLAIR) Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯

**Ø§Ù„Ø³Ø·Ø± 2-5: Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª**
- Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ (sorted Ø¨Ù€ lambda)

**Ø§Ù„Ø³Ø·Ø± 6-11: Ø§Ù„Ø­Ù„Ù‚Ø©**

```python
images = []
for filename in t_paths:
    image = load_dicom(filename, visualize=True)
```
- ÙŠÙ‚Ø±Ø£ ÙƒÙ„ ØµÙˆØ±Ø©
- `visualize=True` â†’ uint8 Ù„Ù„Ø¹Ø±Ø¶

**Ø§Ù„Ø³Ø·Ø± 9-10: ØªØ®Ø·ÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡**

```python
if image.max() == 0:
    continue
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø· Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ØŸ**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:**
- Ø¨Ø¹Ø¶ Ø§Ù„ØµÙˆØ± ÙÙŠ MRI **Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹**
- ØªØ­Ø¯Ø« ÙÙŠ:
  - Ø¨Ø¯Ø§ÙŠØ©/Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø³Ø­ (Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¯Ù…Ø§Øº)
  - Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØµÙˆÙŠØ±
  - Ù…Ù†Ø§Ø·Ù‚ Ø¨Ø¯ÙˆÙ† Ø¥Ø´Ø§Ø±Ø©

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¨Ø¯ÙˆÙ† Ø§Ù„ÙÙ„ØªØ±ØŸ**
```python
# Ø¨Ø¯ÙˆÙ† Ø§Ù„ÙÙ„ØªØ±:
images = [black, black, brain, brain, brain, ..., black, black]
# Animation: Ø´Ø§Ø´Ø§Øª Ø³ÙˆØ¯Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©

# Ù…Ø¹ Ø§Ù„ÙÙ„ØªØ±:
images = [brain, brain, brain, ...]
# Animation: ÙÙ‚Ø· Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ÙÙŠØ¯Ø© âœ…
```

**Ø§Ù„ØªØ­Ù‚Ù‚:**
```python
if image.max() == 0:
```
- Ø¥Ø°Ø§ Ø£ÙƒØ¨Ø± Ù‚ÙŠÙ…Ø© = 0 â†’ ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… = 0 â†’ ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡
- `continue` â†’ ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©

**Ø§Ù„Ø³Ø·Ø± 11:**
```python
images.append(image)
```
- ÙŠØ¶ÙŠÙ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø¬ÙŠØ¯Ø© Ù„Ù„Ù‚Ø§Ø¦Ù…Ø©

**Ø§Ù„Ø³Ø·Ø± 13:**
```python
return images
```
- ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØµÙˆØ±
- **Ø§Ù„Ù†ÙˆØ¹:** `List[np.ndarray]`
- **Ø§Ù„Ø´ÙƒÙ„:** ÙƒÙ„ Ø¹Ù†ØµØ± (256, 256) Ø£Ùˆ (512, 512)

#### Cell 36: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù€ Animation

```python
images = get_modality_slices(modality_path=os.path.join(TRAIN_DATA_PATH, "01007/FLAIR"))
create_animation(images)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«ØŸ**

**Ø§Ù„Ø³Ø·Ø± 1:**
- ÙŠØ¬Ù„Ø¨ ÙƒÙ„ ØµÙˆØ± FLAIR Ù„Ù„Ù…Ø±ÙŠØ¶ 01007
- Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡
- Ù…Ø«Ø§Ù„: 400 ØµÙˆØ±Ø© Ø£ØµÙ„ÙŠØ© â†’ 120 ØµÙˆØ±Ø© Ø¬ÙŠØ¯Ø©

**Ø§Ù„Ø³Ø·Ø± 2:**
- ÙŠÙ†Ø´Ø¦ animation Ù…Ù† Ø§Ù„Ù€ 120 ØµÙˆØ±Ø©
- ÙŠØ¹Ø±Ø¶Ù‡Ø§ ÙƒÙÙŠØ¯ÙŠÙˆ
- ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„/Ø¥ÙŠÙ‚Ø§Ù/ØªÙ‚Ø¯ÙŠÙ…/ØªØ£Ø®ÙŠØ±

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø±Ø§Ù‡ØŸ**
- "Ø±Ø­Ù„Ø©" Ø¹Ø¨Ø± Ø§Ù„Ø¯Ù…Ø§Øº
- Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø£Ø³ Ø¥Ù„Ù‰ Ø£Ø³ÙÙ„Ù‡ (Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³)
- Ø§Ù„ÙˆØ±Ù… ÙŠØ¸Ù‡Ø± ÙˆÙŠØ®ØªÙÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù€ slices

**ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ù€ Animation:**
1. **Ø§Ù„ÙÙ‡Ù… Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:** Ø§Ù„Ø¯Ù…Ø§Øº 3DØŒ Ù†Ø±Ù‰ ÙƒÙŠÙ ÙŠØªØºÙŠØ±
2. **Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ°:** ØµÙˆØ± ÙØ§Ø³Ø¯Ø© ØªØ¸Ù‡Ø± ÙˆØ§Ø¶Ø­Ø©
3. **ÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** ÙƒÙŠÙ ÙŠØ¨Ø¯Ùˆ Ø§Ù„ÙˆØ±Ù… ÙÙŠ slices Ù…Ø®ØªÙ„ÙØ©

**In English:**

#### Cell 34: create_animation Function

**Line-by-line analysis:**

```python
rc('animation', html='jshtml')
```
- Sets matplotlib to display animations as interactive JavaScript

```python
def create_animation(images):
    fig = plt.figure(figsize=(6, 6))
    plt.axis('off')
    image = plt.imshow(images[0], cmap="gray")
```
- Creates figure
- Displays first image
- Saves reference in `image` variable

```python
def animate_func(i):
    image.set_array(images[i])
    return [image]
```
- **Nested function** called for each frame
- Updates displayed image to `images[i]`
- Returns list of updated artists

```python
ani = animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000//24)
```
**Parameters:**
- `fig`: Figure to draw on
- `animate_func`: Function called per frame
- `frames=len(images)`: Number of frames
- `interval=1000//24`: Time between frames in milliseconds

**Why 1000//24?**
```python
# We want 24 frames per second (24 FPS)
# 1 second = 1000 milliseconds
# Time per frame = 1000 / 24

1000 // 24 = 41  # milliseconds per frame
```

**Why 24 FPS?**
- 24 FPS = cinema standard
- Smooth to human eye
- Not too fast (hard to see)
- Not too slow (looks choppy)

#### Cell 35: get_modality_slices Function

```python
if image.max() == 0:
    continue
```

**Why is this condition very important?**

**The Problem:**
- Some MRI images are **completely black**
- Occurs at:
  - Beginning/end of scan (outside brain)
  - Imaging errors
  - Regions without signal

**Without filter:**
```python
images = [black, black, brain, brain, brain, ..., black, black]
# Animation: black screens at beginning and end
```

**With filter:**
```python
images = [brain, brain, brain, ...]
# Animation: only useful images âœ…
```

#### Cell 36: Testing Animation

```python
images = get_modality_slices(modality_path=os.path.join(TRAIN_DATA_PATH, "01007/FLAIR"))
create_animation(images)
```

**What we see:**
- "Journey" through the brain
- From top of head to bottom (or reverse)
- Tumor appears and disappears across slices

**Benefits of Animation:**
1. **3D Understanding:** Brain is 3D, see how it changes
2. **Anomaly Detection:** Corrupted images show clearly
3. **Data Understanding:** How tumor looks in different slices

---

### âœ… Cell 37-40: ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒØ«Ø§ÙØ© | Intensity Distribution Analysis

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 37: Ø¯Ø§Ù„Ø© show_intensity_hist

```python
def show_intensity_hist(images):
    """Display pixel intensity histogram for all slices combined."""
    images = np.array(images)
    plt.figure(figsize=(6, 4))
    plt.hist(images.ravel(), bins=50, color='gray')
    plt.title("Pixel Intensity Distribution")
    plt.xlabel("Intensity")
    plt.ylabel("Frequency")
    plt.show()
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

#### Ø§Ù„Ø³Ø·Ø± 1-2: Ø§Ù„ØªÙˆØ«ÙŠÙ‚

```python
def show_intensity_hist(images):
    """Display pixel intensity histogram for all slices combined."""
```
- Docstring ÙŠØ´Ø±Ø­ Ø§Ù„Ø¯Ø§Ù„Ø©
- "for all slices combined" â†’ Ù†Ø­Ù„Ù„ ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ù…Ø¹Ø§Ù‹

#### Ø§Ù„Ø³Ø·Ø± 3: ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy array

```python
images = np.array(images)
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ø§Ù„ØªØ­ÙˆÙŠÙ„ØŸ**

**Ù‚Ø¨Ù„:**
```python
images = [array1, array2, array3, ...]  # Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† arrays
# Ø´ÙƒÙ„ ÙƒÙ„ array: (256, 256)
# Ø§Ù„Ù†ÙˆØ¹: list
```

**Ø¨Ø¹Ø¯:**
```python
images = np.array([array1, array2, array3, ...])
# Ø§Ù„Ø´ÙƒÙ„: (120, 256, 256)  # 120 ØµÙˆØ±Ø©ØŒ ÙƒÙ„ ÙˆØ§Ø­Ø¯Ø© 256Ã—256
# Ø§Ù„Ù†ÙˆØ¹: numpy array 3D
```

**Ø§Ù„ÙØ§Ø¦Ø¯Ø©:**
- Ø¹Ù…Ù„ÙŠØ§Øª numpy Ø£Ø³Ø±Ø¹ Ø¹Ù„Ù‰ arrays
- ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… `.ravel()` Ø¹Ù„Ù‰ array ÙƒØ§Ù…Ù„

#### Ø§Ù„Ø³Ø·Ø± 5: Ø±Ø³Ù… Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…

```python
plt.hist(images.ravel(), bins=50, color='gray')
```

**ØªØ­Ù„ÙŠÙ„ `.ravel()`:**

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ ravel()ØŸ**
- ÙŠØ­ÙˆÙ„ Ù…ØµÙÙˆÙØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© Ø£Ø­Ø§Ø¯ÙŠØ© (1D)
- **"ÙŠÙØ±Ø¯" Ø§Ù„Ù…ØµÙÙˆÙØ©**

**Ù…Ø«Ø§Ù„:**
```python
# Ù‚Ø¨Ù„ ravel:
images.shape = (120, 256, 256)
# 120 ØµÙˆØ±Ø© Ã— 256 ØµÙ Ã— 256 Ø¹Ù…ÙˆØ¯
# Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 7,864,320 Ø¨ÙƒØ³Ù„

# Ø¨Ø¹Ø¯ ravel:
images.ravel().shape = (7864320,)
# Ù…ØµÙÙˆÙØ© ÙˆØ§Ø­Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ù…Ù† ÙƒÙ„ Ù‚ÙŠÙ… Ø§Ù„Ø¨ÙƒØ³Ù„
```

**ØªØµÙˆØ±:**
```python
# ØµÙˆØ±Ø© 2Ã—2:
image = [[10, 20],
         [30, 40]]

# Ø¨Ø¹Ø¯ ravel:
image.ravel() = [10, 20, 30, 40]
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬ ravelØŸ**
- `plt.hist()` ÙŠØªÙˆÙ‚Ø¹ Ù…ØµÙÙˆÙØ© 1D
- Ù†Ø±ÙŠØ¯ histogram Ù„ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª Ù…Ù† ÙƒÙ„ Ø§Ù„ØµÙˆØ±

**bins=50:**
- Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…
- ÙŠÙ‚Ø³Ù… Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 255] Ø¥Ù„Ù‰ 50 Ø¬Ø²Ø¡
- ÙƒÙ„ Ø¬Ø²Ø¡: 255/50 = 5.1 Ù‚ÙŠÙ…Ø©

#### Ø§Ù„Ø³Ø·Ø± 6-8: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª

```python
plt.title("Pixel Intensity Distribution")
plt.xlabel("Intensity")
plt.ylabel("Frequency")
```
- Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø³Ù…
- Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£ÙÙ‚ÙŠ: Ù‚ÙŠÙ…Ø© Ø§Ù„ÙƒØ«Ø§ÙØ© (0-255)
- Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ: Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª

#### Cell 38: Ø±Ø³Ù… Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù… Ù„Ù„Ù…Ø±ÙŠØ¶ 01007

```python
show_intensity_hist(images)
```

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†ØªÙˆÙ‚Ø¹ Ø±Ø¤ÙŠØªÙ‡ØŸ**

**Ø´ÙƒÙ„ Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù„Ù€ MRI:**
```
Frequency (Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª)
    â”‚
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ                  â† Ø°Ø±ÙˆØ© Ø¹Ù†Ø¯ Ù‚ÙŠÙ… Ù…Ù†Ø®ÙØ¶Ø©
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
    â”‚   â–ˆâ–ˆâ–ˆâ–“
    â”‚   â–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘             â† Ø°ÙŠÙ„ Ø·ÙˆÙŠÙ„
    â”‚   â–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘â–‘
    â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Intensity
        0   20  40  60  80  100 ... 200 255
```

**Ø§Ù„ØªÙØ³ÙŠØ±:**

**1. Ø§Ù„Ø°Ø±ÙˆØ© Ø¹Ù†Ø¯ 0-20 (Ø£Ø³ÙˆØ¯):**
- **Ø§Ù„Ø³Ø¨Ø¨:** Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø© = Ø®Ù„ÙÙŠØ© (background)
- MRI ÙŠØµÙˆØ± Ø§Ù„Ø¯Ù…Ø§Øº ÙÙ‚Ø·
- Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØµÙˆØ±Ø© ÙØ§Ø±Øº/Ø£Ø³ÙˆØ¯
- **Ù†Ø³Ø¨Ø©:** 70-80% Ù…Ù† Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª

**2. Ø°Ø±ÙˆØ© ØµØºÙŠØ±Ø© Ø¹Ù†Ø¯ 100-150 (Ø±Ù…Ø§Ø¯ÙŠ):**
- **Ø§Ù„Ø³Ø¨Ø¨:** Ø£Ù†Ø³Ø¬Ø© Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
- Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ© (gray matter)
- Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ (white matter)
- **Ù†Ø³Ø¨Ø©:** 15-25% Ù…Ù† Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª

**3. Ù‚ÙŠÙ… Ù†Ø§Ø¯Ø±Ø© Ø¹Ù†Ø¯ 150-255 (ÙØ§ØªØ­):**
- **Ø§Ù„Ø³Ø¨Ø¨:** 
  - Ø§Ù„ÙˆØ±Ù… (ÙÙŠ T1wCE)
  - Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ (ÙÙŠ FLAIR)
  - Ù…Ù†Ø§Ø·Ù‚ Ø³Ø§Ø·Ø¹Ø©
- **Ù†Ø³Ø¨Ø©:** < 5% Ù…Ù† Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ù…Ù‡Ù…ØŸ**

**Ù…Ø´ÙƒÙ„Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†:**
```python
# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª:
Background: 75%
Normal brain: 20%
Tumor: 5%  â† Ù‡Ø°Ø§ Ù…Ø§ Ù†Ø±ÙŠØ¯ Ø§ÙƒØªØ´Ø§ÙÙ‡!
```

**Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª:**
1. **Imbalanced data:** Ù…Ø¹Ø¸Ù… Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª Ø®Ù„ÙÙŠØ©
2. **Low signal:** Ø§Ù„ÙˆØ±Ù… Ù†Ø³Ø¨Ø© Ù‚Ù„ÙŠÙ„Ø©
3. **Noise:** Ù‚Ø¯ ÙŠØ®ØªÙ„Ø· Ø§Ù„ÙˆØ±Ù… Ù…Ø¹ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡

**Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©:**
1. **Cropping:** Ù‚Øµ Ø§Ù„Ø®Ù„ÙÙŠØ©ØŒ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù…Ø§Øº
2. **Masking:** Ø§Ø³ØªØ®Ø¯Ø§Ù… mask Ù„Ø¹Ø²Ù„ Ø§Ù„Ø¯Ù…Ø§Øº
3. **Normalization:** ØªØ·Ø¨ÙŠØ¹ per-image
4. **Augmentation:** Ø²ÙŠØ§Ø¯Ø© ØªÙ†ÙˆØ¹ Ø§Ù„Ø£ÙˆØ±Ø§Ù…

#### Cell 39-40: Ù…Ø±ÙŠØ¶ Ø¢Ø®Ø±

```python
images = get_modality_slices(os.path.join(TRAIN_DATA_PATH, "01010/FLAIR"))
create_animation(images)
show_intensity_hist(images)
```

**Ù„Ù…Ø§Ø°Ø§ Ù†ÙƒØ±Ø± Ù„Ù…Ø±ÙŠØ¶ Ø¢Ø®Ø±ØŸ**

**Ø§Ù„Ø£Ù‡Ø¯Ø§Ù:**
1. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚:** 
   - Ù‡Ù„ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù„Ù‡Ù… Ù†ÙØ³ Ø§Ù„ØªÙˆØ²ÙŠØ¹ØŸ
   - Ø£Ù… Ù‡Ù†Ø§Ùƒ Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©ØŸ

2. **Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø´Ø°ÙˆØ°:**
   - Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù… Ù…Ø±ÙŠØ¶ Ù…Ø®ØªÙ„Ù Ø¬Ø¯Ø§Ù‹ â†’ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø³Ø¯Ø©

3. **ÙÙ‡Ù… Ø§Ù„ØªØ¨Ø§ÙŠÙ†:**
   - ÙƒÙ… Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø¶Ù‰ØŸ
   - Ù‡Ù„ Ù†Ø­ØªØ§Ø¬ normalization Ù…Ø®ØªÙ„Ù Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶ØŸ

**Ù…Ù‚Ø§Ø±Ù†Ø© Ù…ØªÙˆÙ‚Ø¹Ø©:**
```python
# Ø§Ù„Ù…Ø±ÙŠØ¶ 01007:
Peak at: 0-20 (background)
Secondary peak: 80-120 (brain)
Max: ~200

# Ø§Ù„Ù…Ø±ÙŠØ¶ 01010:
Peak at: 0-20 (background)  â† Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡
Secondary peak: 90-130 (brain)  â† Ù‚Ø¯ ÙŠØ®ØªÙ„Ù Ù‚Ù„ÙŠÙ„Ø§Ù‹
Max: ~180  â† Ù‚Ø¯ ÙŠØ®ØªÙ„Ù
```

**Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:**

**Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©:**
âœ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ³Ù‚Ø©
âœ… ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ preprocessing Ù„ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰

**Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø®ØªÙ„ÙØ© Ø¬Ø¯Ø§Ù‹:**
âš ï¸ Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬:
- Per-patient normalization
- ÙØ­Øµ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ©

**In English:**

#### Cell 37: show_intensity_hist Function

```python
images = np.array(images)
```
**Why this conversion?**

**Before:**
```python
images = [array1, array2, array3, ...]  # list of arrays
# Each array shape: (256, 256)
```

**After:**
```python
images = np.array([...])
# Shape: (120, 256, 256)  # 120 images, each 256Ã—256
# Type: 3D numpy array
```

```python
plt.hist(images.ravel(), bins=50, color='gray')
```

**What does ravel() do?**
- Converts multi-dimensional array to 1D
- **"Flattens" the array**

**Example:**
```python
# Before ravel:
images.shape = (120, 256, 256)
# Total: 7,864,320 pixels

# After ravel:
images.ravel().shape = (7864320,)
# One long array of all pixel values
```

**Why need ravel?**
- `plt.hist()` expects 1D array
- We want histogram of all pixels from all images

#### Expected Histogram Shape

```
Frequency
    â”‚
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ                  â† Peak at low values
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
    â”‚   â–ˆâ–ˆâ–ˆâ–“
    â”‚   â–ˆâ–ˆâ–ˆâ–“â–‘â–‘â–‘             â† Long tail
    â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Intensity
        0   20  40  60  80  100 ... 200 255
```

**Interpretation:**

**1. Peak at 0-20 (black):**
- **Reason:** Most of image = background
- MRI images only brain
- Rest is empty/black
- **Percentage:** 70-80% of pixels

**2. Small peak at 100-150 (gray):**
- **Reason:** Normal brain tissue
- Gray matter
- White matter
- **Percentage:** 15-25% of pixels

**3. Rare values at 150-255 (bright):**
- **Reason:**
  - Tumor (in T1wCE)
  - Fluids (in FLAIR)
  - Bright regions
- **Percentage:** < 5% of pixels

**Why is this distribution important?**

**Imbalance problem:**
```python
Background: 75%
Normal brain: 20%
Tumor: 5%  â† This is what we want to detect!
```

**Challenges:**
1. **Imbalanced data:** Most pixels are background
2. **Low signal:** Tumor is small percentage
3. **Noise:** Tumor may mix with noise

**Possible solutions:**
1. **Cropping:** Cut background, focus on brain
2. **Masking:** Use mask to isolate brain
3. **Normalization:** Per-image normalization
4. **Augmentation:** Increase tumor variety

---

# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³ | Continuation Part 5

---

### âœ… Cell 41-42: ØªØ­Ù„ÙŠÙ„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù… ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ | Histogram Problem Analysis

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 41: Ø´Ø±Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªÙˆØ²ÙŠØ¹

```markdown
The histogram shows that most pixel intensities are clustered near 0, with a tiny portion spread between 20â€“150.
That means:

* Most of the image area is background (black / empty) â†’ typical for MRI brain scans, since only the brain occupies a small central region, and everything else (air, padding, etc.) is black.
* The actual brain tissue occupies a much smaller portion of the pixel intensity range.
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**

```python
# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ ØµÙˆØ±Ø© MRI Ù†Ù…ÙˆØ°Ø¬ÙŠØ©:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Background (0-20):     75-80%          â”‚  â† Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø©!
â”‚ Brain tissue (20-150):  15-20%         â”‚  â† Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
â”‚ Bright areas (150+):    < 5%           â”‚  â† Ø§Ù„ÙˆØ±Ù…/Ø³ÙˆØ§Ø¦Ù„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ø´ÙƒÙ„Ø©ØŸ**

**1. Ù‡Ø¯Ø± Ø§Ù„Ù…Ø³Ø§Ø­Ø© (Waste of Space):**
```python
# ØµÙˆØ±Ø© 512Ã—512:
Total pixels: 512 Ã— 512 = 262,144 pixels

# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:
Background: 262,144 Ã— 0.75 = 196,608 pixels  â† Ø¹Ø¯ÙŠÙ…Ø© Ø§Ù„ÙØ§Ø¦Ø¯Ø©!
Brain: 262,144 Ã— 0.20 = 52,429 pixels        â† Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…ÙÙŠØ¯Ø©
Bright: 262,144 Ã— 0.05 = 13,107 pixels       â† Ø§Ù„Ø£Ù‡Ù…
```

**2. Ù‡Ø¯Ø± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ (Waste of Resources):**
```python
# Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
- ØªØ®Ø²ÙŠÙ† 196,608 Ø¨ÙƒØ³Ù„ Ø£Ø³ÙˆØ¯ = Ù‡Ø¯Ø±!
- Ù…Ø¹Ø§Ù„Ø¬Ø© 196,608 Ø¨ÙƒØ³Ù„ Ø¨Ù„Ø§ ÙØ§Ø¦Ø¯Ø© = Ù‡Ø¯Ø± ÙˆÙ‚Øª!
- ØªØ¯Ø±ÙŠØ¨ model Ø¹Ù„Ù‰ Ø®Ù„ÙÙŠØ© = ØªØ¹Ù„Ù… Ø£Ø´ÙŠØ§Ø¡ ØºÙŠØ± Ù…ÙÙŠØ¯Ø©!
```

**3. ØªØ£Ø«ÙŠØ± Ø³Ù„Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:**
```python
# Model ÙŠØ±Ù‰:
Input = [0,0,0,0,0,0,0,0,...,0,0,0,brain,brain,0,0,0,...]
                          â†‘
                    Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù… Ø¶Ø§Ø¦Ø¹ ÙÙŠ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡!

# Model ÙŠØªØ¹Ù„Ù…:
"Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡" â†’ Ù„ÙŠØ³ Ù…ÙÙŠØ¯Ø§Ù‹!
Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†:
"Ø´ÙƒÙ„ Ø§Ù„ÙˆØ±Ù… ÙˆÙ…ÙˆÙ‚Ø¹Ù‡" â†’ Ù…ÙÙŠØ¯!
```

**4. ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„:**
```python
# Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„:
[0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 255]
 â†‘                                          â†‘
 Background                              Bright

# Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙØ¹Ù„ÙŠØ§Ù‹:
[0 â”€â”€â”€ 20 â”€â”€â”€â”€â”€â”€ 150 â”€ 255]
      â†‘          â†‘
      Brain   Tumor

# Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:
# - Ø§Ù„Ø¯Ù…Ø§Øº ÙŠØ³ØªØ®Ø¯Ù… Ù†Ø·Ø§Ù‚ Ø¶ÙŠÙ‚ (20-150)
# - Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ (150-255) ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…
# - Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ø¶ØºÙˆØ·Ø© ÙÙŠ Ù†Ø·Ø§Ù‚ Ø¶ÙŠÙ‚!
```

#### Cell 42: Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø­Ù„ÙˆÙ„

```markdown
**What we can do:**

1. Mask or crop the region of interest (ROI):
    Remove background using bounding boxes or segmentation masks but the segmentation masks not available to us so we will crop to the smallest box that contains nonzero pixels.

2. Normalize each image individually:
Instead of global normalization, normalize based on per-image statistics.
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø­Ù„ÙˆÙ„:**

**Ø§Ù„Ø­Ù„ 1: Cropping (Ø§Ù„Ù‚Øµ) â­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ÙƒÙˆØ¯**

**Ø§Ù„ÙÙƒØ±Ø©:**
```python
# Ù‚Ø¨Ù„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         (512Ã—512)               â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â”‚  Brain  â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚  â† Ø§Ù„Ø¯Ù…Ø§Øº ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ²
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â”‚ (200Ã—200)â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚  â† Ø®Ù„ÙÙŠØ© ØºÙŠØ± Ù…ÙÙŠØ¯Ø©
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Ø¨Ø¹Ø¯ Cropping:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Brain  â”‚  (200Ã—200) ÙÙ‚Ø·
â”‚         â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Ø§Ù„Ù†ØªÙŠØ¬Ø©:
- Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: 512Â² â†’ 200Â² (ØªÙ‚Ù„ÙŠÙ„ Ø¨Ù†Ø³Ø¨Ø© 84%!)
- Ø§Ù„Ø°Ø§ÙƒØ±Ø©: Ø£Ù‚Ù„ Ø¨ÙƒØ«ÙŠØ±
- Ø§Ù„ØªØ¯Ø±ÙŠØ¨: Ø£Ø³Ø±Ø¹
- Ø§Ù„ØªÙØ§ØµÙŠÙ„: Ø£ÙˆØ¶Ø­
```

**Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©:**
```python
def crop_to_brain(image):
    # 1. Ø¬Ø¯ ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ØºÙŠØ± Ø§Ù„ØµÙØ±ÙŠØ©
    rows_with_data = np.where(image > 0)[0]  # ØµÙÙˆÙ ÙÙŠÙ‡Ø§ Ø¯Ù…Ø§Øº
    cols_with_data = np.where(image > 0)[1]  # Ø£Ø¹Ù…Ø¯Ø© ÙÙŠÙ‡Ø§ Ø¯Ù…Ø§Øº
    
    # 2. Ø¬Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯
    min_row = rows_with_data.min()  # Ø£Ø¹Ù„Ù‰ ØµÙ ÙÙŠÙ‡ Ø¯Ù…Ø§Øº
    max_row = rows_with_data.max()  # Ø£Ø³ÙÙ„ ØµÙ ÙÙŠÙ‡ Ø¯Ù…Ø§Øº
    min_col = cols_with_data.min()  # Ø£ÙŠØ³Ø± Ø¹Ù…ÙˆØ¯ ÙÙŠÙ‡ Ø¯Ù…Ø§Øº
    max_col = cols_with_data.max()  # Ø£ÙŠÙ…Ù† Ø¹Ù…ÙˆØ¯ ÙÙŠÙ‡ Ø¯Ù…Ø§Øº
    
    # 3. Ù‚Øµ Ø§Ù„ØµÙˆØ±Ø©
    cropped = image[min_row:max_row, min_col:max_col]
    
    return cropped
```

**Ù…Ø«Ø§Ù„ Ø±Ù‚Ù…ÙŠ:**
```python
# ØµÙˆØ±Ø© Ø£ØµÙ„ÙŠØ©:
image = np.array([
    [0, 0, 0, 0, 0],
    [0, 5, 10, 8, 0],
    [0, 12, 20, 15, 0],
    [0, 7, 13, 9, 0],
    [0, 0, 0, 0, 0]
])

# Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ØºÙŠØ± Ø§Ù„ØµÙØ±ÙŠØ©:
rows = [1, 1, 1, 2, 2, 2, 3, 3, 3]  # Ø§Ù„ØµÙÙˆÙ 1, 2, 3
cols = [1, 2, 3, 1, 2, 3, 1, 2, 3]  # Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© 1, 2, 3

# Ø§Ù„Ø­Ø¯ÙˆØ¯:
min_row = 1, max_row = 3
min_col = 1, max_col = 3

# Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ©:
cropped = [
    [5, 10, 8],
    [12, 20, 15],
    [7, 13, 9]
]
# Ù…Ù† 5Ã—5 Ø¥Ù„Ù‰ 3Ã—3 âœ…
```

**Ù…Ù…ÙŠØ²Ø§Øª Cropping:**
âœ… Ø¨Ø³ÙŠØ· Ø¬Ø¯Ø§Ù‹
âœ… Ù„Ø§ ÙŠØ­ØªØ§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (Ù„Ø§ segmentation masks)
âœ… ÙŠØ¹Ù…Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„ÙƒÙ„ ØµÙˆØ±Ø©
âœ… ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø­Ø¬Ù… Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±
âœ… ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©

**Ø¹ÙŠÙˆØ¨ Cropping:**
âš ï¸ Ù‚Ø¯ ÙŠÙ‚Øµ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ø­ÙˆØ§Ù
âš ï¸ Ù„Ø§ ÙŠØ²ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø¯Ø§Ø®Ù„ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¯Ù…Ø§Øº
âš ï¸ Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ®ØªÙ„Ù Ù…Ù† ØµÙˆØ±Ø© Ù„Ø£Ø®Ø±Ù‰

**Ø§Ù„Ø­Ù„ 2: Per-Image Normalization**

**Ø§Ù„ÙÙƒØ±Ø©:**
Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ·Ø¨ÙŠØ¹ global (ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ù…Ø¹Ø§Ù‹)ØŒ Ù†Ø·Ø¨Ù‘Ø¹ ÙƒÙ„ ØµÙˆØ±Ø© Ø¨Ù…ÙØ±Ø¯Ù‡Ø§.

**Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Global (Ø§Ù„Ù…Ø´ÙƒÙ„Ø©):**
```python
# Ø­Ø³Ø§Ø¨ statistics Ù„ÙƒÙ„ Ø§Ù„ØµÙˆØ±:
all_images = [img1, img2, img3, ...]
global_mean = mean(all_images)
global_std = std(all_images)

# ØªØ·Ø¨ÙŠØ¹:
for img in all_images:
    img_normalized = (img - global_mean) / global_std

# Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¯Ø§ÙƒÙ†Ø© Ø¬Ø¯Ø§Ù‹ â†’ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ global_mean
# - ØµÙˆØ± Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø³Ø·ÙˆØ¹ â†’ ØªØ·Ø¨ÙŠØ¹ ØºÙŠØ± Ø¹Ø§Ø¯Ù„
```

**Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Per-Image (Ø§Ù„Ø­Ù„):**
```python
for img in all_images:
    # ÙƒÙ„ ØµÙˆØ±Ø© Ù„Ù‡Ø§ statistics Ø®Ø§ØµØ©:
    img_mean = mean(img)
    img_std = std(img)
    img_normalized = (img - img_mean) / img_std

# Ø§Ù„ÙØ§Ø¦Ø¯Ø©:
# - ÙƒÙ„ ØµÙˆØ±Ø© Ù…Ø¹Ø§Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„
# - Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¯Ø§ÙƒÙ†Ø© Ù„Ø§ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„ÙØ§ØªØ­Ø©
# - ØªØ·Ø¨ÙŠØ¹ Ø¹Ø§Ø¯Ù„ Ù„Ù„Ø¬Ù…ÙŠØ¹
```

**Ù…Ø«Ø§Ù„ ØªÙˆØ¶ÙŠØ­ÙŠ:**

**Ø§Ù„ØµÙˆØ±Ø© 1 (Ø¯Ø§ÙƒÙ†Ø©):**
```python
img1 = [10, 20, 30, 40]  # mean=25, std=12.91
# Ø¨Ø¹Ø¯ per-image normalization:
img1_norm = [-1.16, -0.39, 0.39, 1.16]  # Ù…ØªÙˆØ³Ø·=0, std=1 âœ…
```

**Ø§Ù„ØµÙˆØ±Ø© 2 (ÙØ§ØªØ­Ø©):**
```python
img2 = [100, 110, 120, 130]  # mean=115, std=12.91
# Ø¨Ø¹Ø¯ per-image normalization:
img2_norm = [-1.16, -0.39, 0.39, 1.16]  # Ù…ØªÙˆØ³Ø·=0, std=1 âœ…
```

**Ù„Ø§Ø­Ø¸:** Ù†ÙØ³ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹! Ø±ØºÙ… Ø£Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ø®ØªÙ„ÙØ© Ø¬Ø¯Ø§Ù‹.

**Ù…Ù…ÙŠØ²Ø§Øª Per-Image Normalization:**
âœ… ÙŠÙˆØ­Ø¯ Ø§Ù„Ø³Ø·ÙˆØ¹ Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±
âœ… Ù„Ø§ ÙŠØªØ£Ø«Ø± Ø¨Ø§Ù„Ù€ outliers
âœ… ÙƒÙ„ ØµÙˆØ±Ø© Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„ØªÙˆØ²ÙŠØ¹
âœ… ÙŠØ­Ø³Ù† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

**Ø¹ÙŠÙˆØ¨:**
âš ï¸ Ù‚Ø¯ ÙŠÙÙ‚Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø·ÙˆØ¹ Ø§Ù„Ø£ØµÙ„ÙŠØ©
âš ï¸ ØµÙˆØ± Ù…Ø®ØªÙ„ÙØ© Ù‚Ø¯ ØªØ¨Ø¯Ùˆ Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
âš ï¸ ÙŠØ­ØªØ§Ø¬ Ø­Ø³Ø§Ø¨ statistics Ù„ÙƒÙ„ ØµÙˆØ±Ø© (Ø£Ø¨Ø·Ø£ Ù‚Ù„ÙŠÙ„Ø§Ù‹)

**Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ: Ø¯Ù…Ø¬ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†! ğŸ¯**

```python
def preprocess_image(image):
    # 1. Crop: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
    cropped = crop_to_brain(image)
    
    # 2. Resize: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø¬Ù…
    resized = cv2.resize(cropped, (256, 256))
    
    # 3. Per-image normalization: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø³Ø·ÙˆØ¹
    mean = resized.mean()
    std = resized.std()
    if std > 0:
        normalized = (resized - mean) / std
    
    return normalized
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
âœ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© â†’ ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù…Ø§Øº
âœ… Ø­Ø¬Ù… Ù…ÙˆØ­Ø¯ â†’ Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
âœ… Ø³Ø·ÙˆØ¹ Ù…ÙˆØ­Ø¯ â†’ ØªØ¯Ø±ÙŠØ¨ Ø£ÙØ¶Ù„

**In English:**

#### Cell 41: Histogram Problem Explanation

**The fundamental problem:**
```python
# Pixel distribution in typical MRI:
Background (0-20):     75-80%  â† Most of image!
Brain tissue (20-150): 15-20%  â† Important region
Bright areas (150+):   < 5%    â† Tumor/fluids
```

**Why is this a problem?**

**1. Waste of Space:**
- 75% of pixels are useless background
- Only 20% contain useful brain data

**2. Waste of Resources:**
- Storing 75% black pixels = waste!
- Processing 75% useless pixels = waste of time!
- Training model on background = learning useless things!

**3. Negative Training Impact:**
```python
# Model sees:
Input = [0,0,0,0,0,0,...,brain,brain,0,0,0,...]
                  â†‘
          Important part lost in noise!

# Model learns:
"Most of image is black" â†’ Not useful!
Instead of:
"Tumor shape and location" â†’ Useful!
```

#### Cell 42: Solution Proposals

**Solution 1: Cropping â­ Used in code**

**The idea:**
```python
# Before:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         (512Ã—512)               â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â”‚  Brain  â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â”‚  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# After Cropping:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Brain  â”‚  (200Ã—200) only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Result:
- Data size: 512Â² â†’ 200Â² (84% reduction!)
- Memory: Much less
- Training: Faster
- Details: Clearer
```

**Algorithm:**
```python
def crop_to_brain(image):
    # 1. Find all non-zero pixels
    rows_with_data = np.where(image > 0)[0]
    cols_with_data = np.where(image > 0)[1]
    
    # 2. Find boundaries
    min_row = rows_with_data.min()
    max_row = rows_with_data.max()
    min_col = cols_with_data.min()
    max_col = cols_with_data.max()
    
    # 3. Crop image
    cropped = image[min_row:max_row, min_col:max_col]
    
    return cropped
```

**Cropping Advantages:**
âœ… Very simple
âœ… Doesn't need additional data (no segmentation masks)
âœ… Works automatically for each image
âœ… Significantly reduces size
âœ… Preserves all useful information

**Cropping Disadvantages:**
âš ï¸ May crop parts of brain if close to edges
âš ï¸ Doesn't remove noise inside brain region
âš ï¸ Output size varies from image to image

**Solution 2: Per-Image Normalization**

**The idea:**
Instead of global normalization (all images together), normalize each image separately.

**Per-Image Normalization:**
```python
for img in all_images:
    img_mean = mean(img)
    img_std = std(img)
    img_normalized = (img - img_mean) / img_std
```

**Advantages:**
âœ… Standardizes brightness across images
âœ… Not affected by outliers
âœ… Each image has same distribution
âœ… Improves model performance

**The ideal solution: Combine both! ğŸ¯**
```python
def preprocess_image(image):
    cropped = crop_to_brain(image)
    resized = cv2.resize(cropped, (256, 256))
    normalized = (resized - mean) / std
    return normalized
```

---

### âœ… Cell 43-48: ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙØ±Ø¯ÙŠØ© | Single Image Analysis

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 43-44: Ø¹Ù†ÙˆØ§Ù† Ù‚Ø³Ù… Ø¬Ø¯ÙŠØ¯

```markdown
#### 2- Single image
```

**Ø§Ù„Ù‡Ø¯Ù:**
- Ø§Ù„Ø¢Ù† Ù†Ù†ØªÙ‚Ù„ Ù…Ù† ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø§Ù„ØµÙˆØ± (volumes) Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„

#### Cell 44: Ø¯Ø§Ù„Ø© visualize_image

```python
def visualize_image(path, cmap='gray'):
    image = load_dicom(path, visualize=True)
    plt.figure(figsize=(6, 6))
    plt.imshow(image, cmap=cmap)
    plt.axis("off")
    plt.title("DICOM Image")
    plt.show()
```

**ØªØ­Ù„ÙŠÙ„:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
def visualize_image(path, cmap='gray'):
```
- **Parameters:**
  - `path`: Ù…Ø³Ø§Ø± Ù…Ù„Ù DICOM
  - `cmap='gray'`: Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† (Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹ Ø±Ù…Ø§Ø¯ÙŠ)

**Ø§Ù„Ø³Ø·Ø± 2:**
```python
image = load_dicom(path, visualize=True)
```
- ÙŠÙ‚Ø±Ø£ Ø§Ù„ØµÙˆØ±Ø©
- `visualize=True` â†’ uint8 [0, 255]

**Ø§Ù„Ø³Ø·Ø± 3-6: Ø§Ù„Ø¹Ø±Ø¶**
```python
plt.figure(figsize=(6, 6))
plt.imshow(image, cmap=cmap)
plt.axis("off")
plt.title("DICOM Image")
```
- figure Ù…Ø±Ø¨Ø¹Ø© 6Ã—6
- Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø©
- Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø§ÙˆØ±
- Ø¹Ù†ÙˆØ§Ù† Ø¨Ø³ÙŠØ·

**Ù„Ù…Ø§Ø°Ø§ Ø¯Ø§Ù„Ø© Ù…Ù†ÙØµÙ„Ø© Ù„Ù„Ø¹Ø±Ø¶ØŸ**
- **Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù…Ø±Ø§Ø±Ø§Ù‹
- **Ø§Ù„Ø¨Ø³Ø§Ø·Ø©:** 6 Ø£Ø³Ø·Ø± â†’ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯
- **Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ù‡Ù„:** Ù†Ø¹Ø¯Ù„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ ÙŠØ·Ø¨Ù‚ ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù†

#### Cell 45: Ø¯Ø§Ù„Ø© get_image_info

```python
def get_image_info(path):
    dicom = pydicom.dcmread(path)
    data = dicom.pixel_array
    size_bytes = os.path.getsize(path)
    print(f"Image shape: {data.shape}")
    print(f"File size: {size_bytes / 1024:.2f} KB")
    print(f"Pixel range: [{data.min()} - {data.max()}]")
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ø³Ø·Ø± 2-3:**
```python
dicom = pydicom.dcmread(path)
data = dicom.pixel_array
```
- Ù‚Ø±Ø§Ø¡Ø© DICOM
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ pixel array (Ø¨Ø¯ÙˆÙ† normalization)

**Ø§Ù„Ø³Ø·Ø± 4:**
```python
size_bytes = os.path.getsize(path)
```

**Ù…Ø§ Ù‡Ùˆ `os.path.getsize()`ØŸ**
- ÙŠØ¹Ø·ÙŠ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù **Ø¨Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª (bytes)**
- ÙŠÙ‚Ø±Ø£ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¨Ø§Ø´Ø±Ø©
- Ù„Ø§ ÙŠÙØªØ­ Ø§Ù„Ù…Ù„Ù (Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹)

**Ù…Ø«Ø§Ù„:**
```python
path = "/path/to/Image-100.dcm"
size = os.path.getsize(path)
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: 524288 bytes (512 KB)
```

**Ø§Ù„Ø³Ø·Ø± 5:**
```python
print(f"Image shape: {data.shape}")
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
Image shape: (512, 512)
```

**Ø§Ù„ØªÙØ³ÙŠØ±:**
- **512:** Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ (height)
- **512:** Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (width)
- **Ù„Ù…Ø§Ø°Ø§ Ø¨Ø¯ÙˆÙ† channel dimensionØŸ**
  - MRI = grayscale (Ø±Ù…Ø§Ø¯ÙŠ)
  - RGB ÙƒØ§Ù† Ø³ÙŠÙƒÙˆÙ† (512, 512, 3)

**Ø§Ù„Ø³Ø·Ø± 6:**
```python
print(f"File size: {size_bytes / 1024:.2f} KB")
```

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:**

**`size_bytes / 1024`:**
- ÙŠØ­ÙˆÙ„ Ù…Ù† bytes Ø¥Ù„Ù‰ kilobytes
- 1 KB = 1024 bytes

**`:.2f`:**
- Format specifier
- `.2f` = Ø¹Ø¯Ø¯ Ø¹Ø´Ø±ÙŠ Ø¨Ø®Ø§Ù†ØªÙŠÙ†

**Ù…Ø«Ø§Ù„:**
```python
size_bytes = 524288
size_kb = 524288 / 1024  # 512.0
# Ø§Ù„Ù…Ø®Ø±Ø¬: "File size: 512.00 KB"
```

**Ø§Ù„Ø³Ø·Ø± 7:**
```python
print(f"Pixel range: [{data.min()} - {data.max()}]")
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
Pixel range: [0 - 4095]
```

**Ù„Ù…Ø§Ø°Ø§ 4095ØŸ**
- MRI Ø¹Ø§Ø¯Ø© 12-bit: 2^12 = 4096 Ù‚ÙŠÙ…Ø© (0-4095)
- Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© 16-bit: 2^16 = 65536 Ù‚ÙŠÙ…Ø© (0-65535)

**âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:**
Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ³ØªØ®Ø¯Ù… `dicom.pixel_array` Ù…Ø¨Ø§Ø´Ø±Ø© (Ø¨Ø¯ÙˆÙ† normalization)!
```python
# Ù‡Ù†Ø§:
data = dicom.pixel_array  # Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£ØµÙ„ÙŠØ© [0-4095]

# ÙÙŠ load_dicom:
data = normalized  # Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ [0-1] Ø£Ùˆ [0-255]
```

#### Cell 46-48: Ø§Ø®ØªØ¨Ø§Ø± ØµÙˆØ±Ø© Ù…Ø­Ø¯Ø¯Ø©

```python
# Cell 46:
image_path = "/kaggle/input/.../train/00000/FLAIR/Image-100.dcm"

# Cell 47:
visualize_image(image_path)

# Cell 48:
get_image_info(image_path)
```

**Ù„Ù…Ø§Ø°Ø§ Image-100 Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŸ**
- **Image-100** Ø¹Ø§Ø¯Ø© ÙÙŠ Ù…Ù†ØªØµÙ Ø§Ù„Ù†Ø·Ø§Ù‚
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ 200 ØµÙˆØ±Ø©ØŒ ÙÙ€ Image-100 ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
- Ø§Ù„Ù…Ù†ØªØµÙ Ø¹Ø§Ø¯Ø© ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ **Ø£ÙƒØ«Ø± ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù…Ø§Øº**

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**

**Ù…Ù† `visualize_image()`:**
- ØµÙˆØ±Ø© MRI ØªØ¸Ù‡Ø± slice Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
- Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯ (grayscale)
- Ø´ÙƒÙ„ Ø§Ù„Ø¯Ù…Ø§Øº ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ²
- Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡ Ø­ÙˆÙ„ Ø§Ù„Ø¯Ù…Ø§Øº

**Ù…Ù† `get_image_info()`:**
```
Image shape: (512, 512)
File size: 532.50 KB
Pixel range: [0 - 4095]
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**
- **Shape 512Ã—512:** Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø© (Ù…Ø¹ÙŠØ§Ø± ÙÙŠ MRI)
- **File size ~532 KB:** 
  - Ø­Ø³Ø§Ø¨ Ù†Ø¸Ø±ÙŠ: 512 Ã— 512 Ã— 2 bytes = 524,288 bytes â‰ˆ 512 KB
  - Ø§Ù„Ø²ÙŠØ§Ø¯Ø©: metadata ÙÙŠ DICOM
- **Range [0-4095]:** 12-bit imaging

#### Cell 49-51: ØµÙˆØ±Ø© Ø£Ø®Ø±Ù‰ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©

```python
# Cell 49:
image_path = "/kaggle/input/.../train/00000/FLAIR/Image-116.dcm"

# Cell 50:
visualize_image(image_path)

# Cell 51:
get_image_info(image_path)
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø¬Ø±Ø¨ Image-116ØŸ**
- Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Image-100
- Ù„Ø±Ø¤ÙŠØ© ÙƒÙŠÙ ÙŠØªØºÙŠØ± Ø§Ù„Ø¯Ù…Ø§Øº Ø¹Ø¨Ø± Ø§Ù„Ù€ slices

**Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**

**Image-100 (Ø£Ø³ÙÙ„ Ù‚Ù„ÙŠÙ„Ø§Ù‹):**
- Ù‚Ø¯ ÙŠØ¸Ù‡Ø±:
  - Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¯Ù…Ø§Øº
  - Ø§Ù„Ù…Ø®ÙŠØ® (cerebellum)
  - Ø¬Ø°Ø¹ Ø§Ù„Ø¯Ù…Ø§Øº (brainstem)

**Image-116 (Ø£Ø¹Ù„Ù‰ Ù‚Ù„ÙŠÙ„Ø§Ù‹):**
- Ù‚Ø¯ ÙŠØ¸Ù‡Ø±:
  - Ù‚Ø´Ø±Ø© Ø§Ù„Ø¯Ù…Ø§Øº (cortex)
  - Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡/Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ©
  - Ø§Ù„Ø¨Ø·ÙŠÙ†Ø§Øª (ventricles)

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†ØªØ¹Ù„Ù…Ù‡ØŸ**

**1. Ø§Ù„Ø¨Ù†ÙŠØ© Ø«Ù„Ø§Ø«ÙŠØ© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:**
- Ø§Ù„Ø¯Ù…Ø§Øº 3D object
- ÙƒÙ„ slice = "Ø´Ø±ÙŠØ­Ø©" Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
- Slices Ù…Ø®ØªÙ„ÙØ© â†’ ØªØ´Ø±ÙŠØ­ Ù…Ø®ØªÙ„Ù

**2. Ø§Ù„ØªØ¨Ø§ÙŠÙ†:**
- ÙƒÙ„ slice Ù„Ù‡ Ø®ØµØ§Ø¦Øµ Ù…Ø®ØªÙ„ÙØ©
- Ø¨Ø¹Ø¶ Ø§Ù„Ù€ slices Ø£ÙƒØ«Ø± ÙØ§Ø¦Ø¯Ø© Ù…Ù† ØºÙŠØ±Ù‡Ø§
- Ø§Ù„Ù…Ù†ØªØµÙ Ø¹Ø§Ø¯Ø© Ø§Ù„Ø£ÙØ¶Ù„

**3. Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
- Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„ (512Ã—512)
- Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚ (0-4095)
- Ù†ÙØ³ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
- **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:** Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ³Ù‚Ø© âœ…

**In English:**

#### Cell 45: get_image_info Function

```python
def get_image_info(path):
    dicom = pydicom.dcmread(path)
    data = dicom.pixel_array
    size_bytes = os.path.getsize(path)
    print(f"Image shape: {data.shape}")
    print(f"File size: {size_bytes / 1024:.2f} KB")
    print(f"Pixel range: [{data.min()} - {data.max()}]")
```

**What is `os.path.getsize()`?**
- Returns file size **in bytes**
- Reads from file system directly
- Doesn't open file (very fast)

**Expected output:**
```
Image shape: (512, 512)
File size: 532.50 KB
Pixel range: [0 - 4095]
```

**Analysis:**
- **Shape 512Ã—512:** Good resolution (standard in MRI)
- **File size ~532 KB:**
  - Theoretical: 512 Ã— 512 Ã— 2 bytes = 524,288 bytes â‰ˆ 512 KB
  - Extra: metadata in DICOM
- **Range [0-4095]:** 12-bit imaging

#### Cells 49-51: Another Image for Comparison

**Why try Image-116?**
- To compare with Image-100
- To see how brain changes across slices

**Expected difference:**

**Image-100 (lower):**
- May show:
  - Base of brain
  - Cerebellum
  - Brainstem

**Image-116 (higher):**
- May show:
  - Cortex
  - White/gray matter
  - Ventricles

**What do we learn?**

**1. 3D Structure:**
- Brain is 3D object
- Each slice = "cut" through brain
- Different slices â†’ different anatomy

**2. Variation:**
- Each slice has different characteristics
- Some slices more useful than others
- Middle usually best

**3. Data Consistency:**
- Same shape (512Ã—512)
- Same range (0-4095)
- Same file size approximately
- **Conclusion:** Data is consistent âœ…

# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¯Ø³ | Continuation Part 6

---

### âœ… Cell 52-63: ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± | Comprehensive Analysis of All Images

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 52: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯

```markdown
#### 3. All images
```

**Ø§Ù„Ù‡Ø¯Ù:**
- Ø§Ù„Ø¢Ù† Ù†Ù†ØªÙ‚Ù„ Ù…Ù† ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ **Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±** ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
- Ù†Ø±ÙŠØ¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©

#### Cell 53: Ø¯Ø§Ù„Ø© shapes_per_modality

```python
def shapes_per_modality(modality_path):
    shapes = []
    dicom_files = glob.glob(os.path.join(modality_path, "*.dcm"))
    for file in dicom_files:
        dcm = pydicom.dcmread(file)
        shape = dcm.pixel_array.shape
        shapes.append(shape)
    return shapes
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ø³Ø·Ø± 1-2:**
```python
def shapes_per_modality(modality_path):
    shapes = []
```
- **Parameter:** Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ÙˆØ¹ (Ù…Ø«Ù„Ø§Ù‹ FLAIR Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯)
- **`shapes`:** Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© Ø³ØªØ­ÙØ¸ Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØµÙˆØ±

**Ø§Ù„Ø³Ø·Ø± 3:**
```python
dicom_files = glob.glob(os.path.join(modality_path, "*.dcm"))
```

**ØªØ­Ù„ÙŠÙ„ `"*.dcm"`:**
- **`*`:** wildcard (Ø£ÙŠ Ø´ÙŠØ¡)
- **`.dcm`:** Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
- **Ø§Ù„Ù†ØªÙŠØ¬Ø©:** ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ†ØªÙ‡ÙŠ Ø¨Ù€ .dcm

**Ù…Ø«Ø§Ù„:**
```python
modality_path = "/path/to/00000/FLAIR"
# Ø§Ù„Ù†ØªÙŠØ¬Ø©:
dicom_files = [
    "/path/to/00000/FLAIR/Image-1.dcm",
    "/path/to/00000/FLAIR/Image-2.dcm",
    ...
    "/path/to/00000/FLAIR/Image-400.dcm"
]
```

**âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©:** Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© **ØºÙŠØ± Ù…Ø±ØªØ¨Ø©**!

**Ø§Ù„Ø³Ø·Ø± 4-7: Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©**

```python
for file in dicom_files:
    dcm = pydicom.dcmread(file)
    shape = dcm.pixel_array.shape
    shapes.append(shape)
```

**Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**1. Ù‚Ø±Ø§Ø¡Ø© DICOM:**
```python
dcm = pydicom.dcmread(file)
```
- ÙŠÙ‚Ø±Ø£ Ù…Ù„Ù ÙˆØ§Ø­Ø¯

**2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´ÙƒÙ„:**
```python
shape = dcm.pixel_array.shape
```
- **`.shape`:** ÙŠØ¹Ø·ÙŠ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØµÙÙˆÙØ©
- **Ø§Ù„Ù†ÙˆØ¹:** tuple
- **Ù…Ø«Ø§Ù„:** `(512, 512)`

**Ù„Ù…Ø§Ø°Ø§ `.shape` ÙˆÙ„ÙŠØ³ `.size`ØŸ**
```python
# .shape: Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØµÙÙˆÙØ©
image.shape  # (512, 512) âœ…

# .size: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙƒÙ„ÙŠ
image.size  # 262144 (512Ã—512)
```

**3. Ø§Ù„Ø­ÙØ¸:**
```python
shapes.append(shape)
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
```python
shapes = [
    (512, 512),  # Image-1
    (512, 512),  # Image-2
    (512, 512),  # Image-3
    ...
    (512, 512)   # Image-400
]
```

**Ø§Ù„Ø³Ø·Ø± 8:**
```python
return shapes
```

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªØ¬ÙŠØ¨ Ø¹Ù†Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø©:**
"Ù…Ø§ Ù‡ÙŠ Ø£Ø´ÙƒØ§Ù„ ÙƒÙ„ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ù„Ø¯ØŸ Ù‡Ù„ ÙƒÙ„Ù‡Ø§ Ù…ØªØ³Ø§ÙˆÙŠØ©ØŸ"

#### Cell 54-56: Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ 3 Ù…Ø±Ø¶Ù‰ Ù…Ø®ØªÙ„ÙÙŠÙ†

**Cell 54: Ø§Ù„Ù…Ø±ÙŠØ¶ 00000 - FLAIR**
```python
modality_path = "/kaggle/input/.../train/00000/FLAIR"
shapes = shapes_per_modality(modality_path)
pd.Series(shapes).value_counts()
```

**ØªØ­Ù„ÙŠÙ„ `pd.Series(shapes).value_counts()`:**

**Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**1. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Series:**
```python
shapes = [(512, 512), (512, 512), (512, 512), ...]
pd.Series(shapes)
# Ø§Ù„Ù†ØªÙŠØ¬Ø©:
# 0    (512, 512)
# 1    (512, 512)
# 2    (512, 512)
# ...
# dtype: object
```

**2. Ø¹Ø¯ Ø§Ù„Ù‚ÙŠÙ…:**
```python
.value_counts()
# Ø§Ù„Ù†ØªÙŠØ¬Ø©:
# (512, 512)    400
# dtype: int64
```

**Ø§Ù„ØªÙØ³ÙŠØ±:**
- ÙƒÙ„ Ø§Ù„Ù€ 400 ØµÙˆØ±Ø© Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„ (512, 512)
- **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:** Ù…ØªØ³Ù‚Ø© âœ…

**Cell 55: Ø§Ù„Ù…Ø±ÙŠØ¶ 00011 - FLAIR**
```python
modality_path = "/kaggle/input/.../train/00011/FLAIR"
shapes = shapes_per_modality(modality_path)
pd.Series(shapes).value_counts()
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
(512, 512)    385
```
- Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„ØŒ Ù„ÙƒÙ† Ø¹Ø¯Ø¯ Ù…Ø®ØªÙ„Ù Ù…Ù† Ø§Ù„ØµÙˆØ±
- Ø§Ù„Ù…Ø±ÙŠØ¶ 00011 Ù„Ø¯ÙŠÙ‡ 385 ØµÙˆØ±Ø© (Ù„ÙŠØ³ 400)

**Cell 56: Ø§Ù„Ù…Ø±ÙŠØ¶ 00111 - T1w**
```python
modality_path = "/kaggle/input/.../train/00111/T1w"
shapes = shapes_per_modality(modality_path)
pd.Series(shapes).value_counts()
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø®ØªØ¨Ø± T1wØŸ**
- Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† Ø¬Ø±Ø¨Ù†Ø§ FLAIR ÙÙ‚Ø·
- Ù†Ø±ÙŠØ¯ Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† T1w Ø£ÙŠØ¶Ø§Ù‹ Ù…ØªØ³Ù‚

**Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ù† Cell 54-56:**
âœ… ÙƒÙ„ ØµÙˆØ± Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„
âœ… Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ù…ØªØ³Ù‚Ø© (512Ã—512 Ø´Ø§Ø¦Ø¹)
âœ… Ø§Ù„Ø¹Ø¯Ø¯ ÙŠØ®ØªÙ„Ù Ù…Ù† Ù…Ø±ÙŠØ¶ Ù„Ø¢Ø®Ø±

#### Cell 57: ØªØ¹Ù„ÙŠÙ‚ ØªÙˆØ¶ÙŠØ­ÙŠ

```markdown
It seems that each type of scan (e.g. T1w) per patient has the same shape.
```

**Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:**
- **Per patient, per modality:** Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„
- **Ù„ÙƒÙ† Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø¶Ù‰:** Ù‚Ø¯ ÙŠØ®ØªÙ„Ù!

**Ù…Ø«Ø§Ù„:**
```python
# Ø§Ù„Ù…Ø±ÙŠØ¶ 00000:
FLAIR: (512, 512) Ã— 400 images âœ…
T1w:   (512, 512) Ã— 400 images âœ…
T1wCE: (512, 512) Ã— 400 images âœ…
T2w:   (512, 512) Ã— 400 images âœ…

# Ø§Ù„Ù…Ø±ÙŠØ¶ 00011:
FLAIR: (512, 512) Ã— 385 images âœ…
T1w:   (512, 512) Ã— 385 images âœ…
T1wCE: (512, 512) Ã— 385 images âœ…
T2w:   (512, 512) Ã— 385 images âœ…

# Ø§Ù„Ù…Ø±ÙŠØ¶ 00020 (Ù…Ø«Ø§Ù„):
FLAIR: (256, 192) Ã— 420 images â† Ø´ÙƒÙ„ Ù…Ø®ØªÙ„Ù! âš ï¸
```

#### Cell 58: Ø¯Ø§Ù„Ø© get_images_info - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„

```python
def get_images_info(train_path):
    records = []

    for patient_id in sorted(os.listdir(train_path)):
        patient_path = os.path.join(train_path, patient_id)

        for modality in ["FLAIR", "T1w", "T1wCE", "T2w"]:
            modality_path = os.path.join(patient_path, modality)

            dicom_files = glob.glob(os.path.join(modality_path, "*.dcm"))

            # we enough with just one image from each modality because the rest has the same shape
            dcm = pydicom.dcmread(dicom_files[0])
            shape = dcm.pixel_array.shape

            records.append({
                'patient_id': patient_id,
                'modality': modality,
                'shape': shape,
                'num_slices': len(dicom_files)
            })
    
    return pd.DataFrame(records)
```

**ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ø¬Ø¯Ø§Ù‹:**

**Ø§Ù„Ù‡Ø¯Ù:**
- Ø¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† **ÙƒÙ„ Ù…Ø±ÙŠØ¶** Ùˆ **ÙƒÙ„ Ù†ÙˆØ¹**
- Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ø´Ø§Ù…Ù„

**Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©:**
```python
# Ø­Ù„Ù‚Ø© Ù…Ø²Ø¯ÙˆØ¬Ø© (nested loop):
for patient in all_patients:           # 582 Ù…Ø±ÙŠØ¶
    for modality in 4_modalities:      # 4 Ø£Ù†ÙˆØ§Ø¹
        # Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        
# Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 582 Ã— 4 = 2,328 Ø³Ø¬Ù„
```

**Ø§Ù„Ø³Ø·Ø± 1-2:**
```python
def get_images_info(train_path):
    records = []
```
- **Parameter:** Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
- **`records`:** Ù‚Ø§Ø¦Ù…Ø© Ø³ØªØ­ÙØ¸ ÙƒÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª

**Ø§Ù„Ø³Ø·Ø± 4:**
```python
for patient_id in sorted(os.listdir(train_path)):
```

**Ù„Ù…Ø§Ø°Ø§ `sorted()`ØŸ**
- **Ø¨Ø¯ÙˆÙ† sorted:**
  ```python
  ['00200', '00003', '00100', ...]  # ØªØ±ØªÙŠØ¨ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
  ```
- **Ù…Ø¹ sorted:**
  ```python
  ['00000', '00002', '00003', ...]  # ØªØ±ØªÙŠØ¨ Ø£Ø¨Ø¬Ø¯ÙŠ/Ø¹Ø¯Ø¯ÙŠ âœ…
  ```

**Ø§Ù„ÙØ§Ø¦Ø¯Ø©:**
- Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø± (reproducible)
- Ø³Ù‡Ù„ Ø§Ù„ØªØªØ¨Ø¹ ÙˆØ§Ù„Ù€ debugging

**Ø§Ù„Ø³Ø·Ø± 5:**
```python
patient_path = os.path.join(train_path, patient_id)
```
- Ù…Ø«Ø§Ù„: `"/kaggle/input/.../train/00000"`

**Ø§Ù„Ø³Ø·Ø± 7-8:**
```python
for modality in ["FLAIR", "T1w", "T1wCE", "T2w"]:
    modality_path = os.path.join(patient_path, modality)
```

**Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©:**
- ØªÙƒØ±Ø± 4 Ù…Ø±Ø§Øª Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶
- Ù…Ø«Ø§Ù„: `"/kaggle/input/.../train/00000/FLAIR"`

**Ø§Ù„Ø³Ø·Ø± 10:**
```python
dicom_files = glob.glob(os.path.join(modality_path, "*.dcm"))
```
- ÙŠØ¬Ù„Ø¨ ÙƒÙ„ Ù…Ù„ÙØ§Øª DICOM ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯

**Ø§Ù„Ø³Ø·Ø± 12-13: â­ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø°ÙƒÙŠ**

```python
# we enough with just one image from each modality because the rest has the same shape
dcm = pydicom.dcmread(dicom_files[0])
shape = dcm.pixel_array.shape
```

**Ù„Ù…Ø§Ø°Ø§ `dicom_files[0]` ÙÙ‚Ø·ØŸ**

**Ø§Ù„ØªÙÙƒÙŠØ±:**
- Ù…Ù† Cell 57ØŒ Ù†Ø¹Ø±Ù Ø£Ù† **ÙƒÙ„ ØµÙˆØ± Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„**
- Ø¥Ø°Ù†ØŒ Ù†Ù‚Ø±Ø£ **ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·** (Ø§Ù„Ø£ÙˆÙ„Ù‰) ÙˆÙ†ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ù…Ø«Ù„Ù‡Ø§

**Ø§Ù„ÙØ§Ø¦Ø¯Ø©:**
```python
# Ø¨Ø¯ÙˆÙ† Ø§Ù„ØªØ­Ø³ÙŠÙ†:
for file in dicom_files:  # 400 Ù…Ù„Ù
    shape = read_shape(file)  # Ù†Ù‚Ø±Ø£ 400 Ù…Ø±Ø©!

# Ù…Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†:
shape = read_shape(dicom_files[0])  # Ù†Ù‚Ø±Ø£ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·! âœ…

# ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª:
# 582 patients Ã— 4 modalities Ã— 1 read = 2,328 reads
# Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†:
# 582 patients Ã— 4 modalities Ã— ~130 reads = ~302,640 reads
# ØªÙˆÙÙŠØ±: 99%!
```

**âš ï¸ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ Ø§Ù„Ø®Ø·ÙŠØ±:**
```python
# Ø§Ù„ÙƒÙˆØ¯ ÙŠÙØªØ±Ø¶:
"ÙƒÙ„ ØµÙˆØ± Ù†ÙØ³ Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„"

# Ù„ÙƒÙ† Ù…Ø§Ø°Ø§ Ù„Ùˆ:
dicom_files = [
    "Image-1.dcm",   # (512, 512)
    "Image-2.dcm",   # (512, 512)
    ...
    "Image-399.dcm", # (512, 512)
    "Image-400.dcm"  # (256, 256) â† Ù…Ø®ØªÙ„Ù!
]

# Ø³Ù†Ù‚Ø±Ø£ ÙÙ‚Ø· Image-1.dcm â†’ (512, 512)
# ÙˆÙ†ÙÙˆØª Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù ÙÙŠ Image-400.dcm! âŒ
```

**Ù‡Ù„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ Ø¢Ù…Ù†ØŸ**
- **ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: Ù†Ø¹Ù…** âœ…
- **Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…: Ù„Ø§!** âš ï¸
- **Ø§Ù„Ø£ÙØ¶Ù„:** ÙØ­Øµ Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø£Ùˆ ÙƒÙ„ Ø§Ù„ØµÙˆØ±

**Ø§Ù„Ø³Ø·Ø± 15-20: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø³Ø¬Ù„**

```python
records.append({
    'patient_id': patient_id,
    'modality': modality,
    'shape': shape,
    'num_slices': len(dicom_files)
})
```

**Ø¨Ù†ÙŠØ© Ø§Ù„Ø³Ø¬Ù„ (record):**
```python
{
    'patient_id': '00000',
    'modality': 'FLAIR',
    'shape': (512, 512),
    'num_slices': 400
}
```

**`len(dicom_files)`:**
- Ø¹Ø¯Ø¯ Ù…Ù„ÙØ§Øª DICOM ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯
- = Ø¹Ø¯Ø¯ Ø§Ù„Ù€ slices Ù„Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹

**Ø§Ù„Ø³Ø·Ø± 22:**
```python
return pd.DataFrame(records)
```

**Ø´ÙƒÙ„ Ø§Ù„Ù€ DataFrame Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:**
```
   patient_id modality      shape  num_slices
0       00000    FLAIR  (512, 512)         400
1       00000      T1w  (512, 512)         400
2       00000    T1wCE  (512, 512)         400
3       00000      T2w  (512, 512)         400
4       00002    FLAIR  (512, 512)         385
5       00002      T1w  (512, 512)         385
...
2327    00999      T2w  (256, 256)         420
```

**Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ:**
- 582 Ù…Ø±ÙŠØ¶ Ã— 4 Ø£Ù†ÙˆØ§Ø¹ = 2,328 ØµÙ

#### Cell 59: ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø©

```python
imges_info = get_images_info(TRAIN_DATA_PATH)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«ØŸ**
- ÙŠÙ…Ø± Ø¹Ù„Ù‰ 582 Ù…Ø±ÙŠØ¶
- Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶ØŒ ÙŠÙ…Ø± Ø¹Ù„Ù‰ 4 Ø£Ù†ÙˆØ§Ø¹
- ÙŠÙ‚Ø±Ø£ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† ÙƒÙ„ Ù†ÙˆØ¹
- ÙŠØ­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
- Ù‚Ø±Ø§Ø¡Ø© 2,328 Ù…Ù„Ù DICOM
- ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ 1-2 Ø¯Ù‚ÙŠÙ‚Ø©

**âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ø³Ù… Ø§Ù„Ù…ØªØºÙŠØ± `imges_info` Ø¨Ù‡ Ø®Ø·Ø£ Ø¥Ù…Ù„Ø§Ø¦ÙŠ!
- Ø§Ù„ØµØ­ÙŠØ­: `images_info`
- Ù„ÙƒÙ† Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„

#### Cell 60: Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©

```python
imges_info.sample(10)
```

**Ù…Ø§Ø°Ø§ ÙŠÙØ¹Ù„ `.sample(10)`ØŸ**
- ÙŠØ®ØªØ§Ø± 10 ØµÙÙˆÙ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
- **Ù„Ù…Ø§Ø°Ø§ Ø¹Ø´ÙˆØ§Ø¦ÙŠØŸ** Ù„Ø±Ø¤ÙŠØ© ØªÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

**Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬:**
```
     patient_id modality      shape  num_slices
1234      00250    T1wCE  (512, 512)         392
567       00115      T2w  (256, 192)         420
...
```

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø¨Ø­Ø« Ø¹Ù†Ù‡ØŸ**
1. **ØªÙ†ÙˆØ¹ Ø§Ù„Ø£Ø´ÙƒØ§Ù„:** Ù‡Ù„ ÙƒÙ„Ù‡Ø§ (512, 512)ØŸ
2. **ØªÙ†ÙˆØ¹ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ slices:** Ù‡Ù„ ÙƒÙ„Ù‡Ø§ 400ØŸ
3. **Ø´Ø°ÙˆØ°:** Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ ØºØ±ÙŠØ¨ØŸ

#### Cell 61: Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø´ÙŠÙˆØ¹Ø§Ù‹

```python
imges_info['shape'].value_counts()[:10]
```

**ØªØ­Ù„ÙŠÙ„:**

**`['shape']`:**
- ÙŠØ®ØªØ§Ø± Ø¹Ù…ÙˆØ¯ shape

**`.value_counts()`:**
- ÙŠØ¹Ø¯ ÙƒÙ… Ù…Ø±Ø© Ø¸Ù‡Ø± ÙƒÙ„ Ø´ÙƒÙ„
- ÙŠØ±ØªØ¨ Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„

**`[:10]`:**
- ÙŠØ£Ø®Ø° Ø£ÙˆÙ„ 10 ÙÙ‚Ø· (Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹)

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```
(512, 512)    2100  â† Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹ Ø¨ÙƒØ«ÙŠØ±!
(256, 192)     150
(256, 256)      50
(480, 480)      20
...
```

**Ø§Ù„ØªÙØ³ÙŠØ±:**
- **90% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** (512, 512)
- **10% Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©:** Ø£Ø´ÙƒØ§Ù„ Ù…ØªÙ†ÙˆØ¹Ø©
- **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:** Ù…Ø¹Ø¸Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ³Ù‚Ø©ØŒ Ù„ÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø§Ø®ØªÙ„Ø§ÙØ§Øª

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ**
- Ù†Ø­ØªØ§Ø¬ **ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø¬Ù…** Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- Ø³Ù†Ø³ØªØ®Ø¯Ù… `cv2.resize()` Ù„Ø¬Ø¹Ù„ ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù…

#### Cell 62: Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø£Ø´ÙƒØ§Ù„

```python
shape_counts = imges_info['shape'].value_counts()[:15].reset_index()
shape_counts.columns = ['shape', 'count']
plt.figure(figsize=(8, 5))
sns.barplot(y='shape', x='count', data=shape_counts, palette="viridis")
plt.title("Most Common Image Shapes")
plt.xlabel("Number of Images")
plt.ylabel("Shape (H, W)")
plt.show()
```

**ØªØ­Ù„ÙŠÙ„ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
shape_counts = imges_info['shape'].value_counts()[:15].reset_index()
```

**Ù…Ø§ ÙŠØ­Ø¯Ø«:**

**1. `.value_counts()[:15]`:**
```python
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: Series
(512, 512)    2100
(256, 192)     150
...
```

**2. `.reset_index():`**
```python
# ÙŠØ­ÙˆÙ„ Series Ø¥Ù„Ù‰ DataFrame:
        shape  count
0  (512, 512)   2100
1  (256, 192)    150
...
```

**Ù„Ù…Ø§Ø°Ø§ reset_indexØŸ**
- `value_counts()` ÙŠØ¬Ø¹Ù„ Ø§Ù„Ø´ÙƒÙ„ index
- Ù†Ø±ÙŠØ¯Ù‡ Ø¹Ù…ÙˆØ¯ Ø¹Ø§Ø¯ÙŠ Ù„Ù„Ø±Ø³Ù…

**Ø§Ù„Ø³Ø·Ø± 2:**
```python
shape_counts.columns = ['shape', 'count']
```
- Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
- Ù…Ù† `['index', 'shape']` Ø¥Ù„Ù‰ `['shape', 'count']`

**Ø§Ù„Ø³Ø·Ø± 3-4:**
```python
plt.figure(figsize=(8, 5))
sns.barplot(y='shape', x='count', data=shape_counts, palette="viridis")
```

**`sns.barplot(y='shape', x='count')`:**
- **horizontal bar chart!**
- `y='shape'`: Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ
- `x='count'`: Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£ÙÙ‚ÙŠ

**Ù„Ù…Ø§Ø°Ø§ horizontal ÙˆÙ„ÙŠØ³ verticalØŸ**
```python
# Vertical (y='count', x='shape'):
     â”‚
2100 â”‚  â–ˆ
     â”‚  â–ˆ
     â”‚  â–ˆ
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       (512,512)

# Horizontal (y='shape', x='count'):
(512,512)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
(256,192)  â–ˆâ–ˆ
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              2100

# Ø§Ù„ÙØ§Ø¦Ø¯Ø©: Ø£Ø³Ù‡Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø´ÙƒØ§Ù„ (512, 512) Ø£ÙÙ‚ÙŠØ§Ù‹
```

**`palette="viridis"`:**
- Ø®Ø±ÙŠØ·Ø© Ø£Ù„ÙˆØ§Ù†
- viridis: Ø£Ø²Ø±Ù‚ â†’ Ø£Ø®Ø¶Ø± â†’ Ø£ØµÙØ±
- Ø¬Ù…ÙŠÙ„Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©

**Ø§Ù„Ø³Ø·Ø± 5-7:**
```python
plt.title("Most Common Image Shapes")
plt.xlabel("Number of Images")
plt.ylabel("Shape (H, W)")
```
- Ø¹Ù†ÙˆØ§Ù† ÙˆØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø§ÙˆØ±

**Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙƒØ´ÙÙ‡ Ø§Ù„Ø±Ø³Ù…ØŸ**
- (512, 512) **ÙŠÙ‡ÙŠÙ…Ù†** Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ù†Ø§Ø¯Ø±Ø© Ø¬Ø¯Ø§Ù‹
- **Ø§Ù„Ù‚Ø±Ø§Ø±:** Ù†ÙˆØ­Ø¯ ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø­Ø¬Ù… ÙˆØ§Ø­Ø¯ (Ù…Ø«Ù„Ø§Ù‹ 256Ã—256)

#### Cell 63: ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬

```markdown
Most images have a resolution of **512Ã—512**, followed by smaller sizes like **256Ã—192** and **256Ã—256**.

May it's suggested to resize images before training a model to ensure consistency across all inputs.
```

**Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:**
âœ… ÙŠØ¬Ø¨ ØªÙˆØ­ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ± Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
âœ… Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø´Ø§Ø¦Ø¹: 224Ã—224 Ø£Ùˆ 256Ã—256

**In English:**

#### Cell 58: get_images_info Function - Comprehensive Analysis

**The goal:**
- Collect information about **every patient** and **every modality**
- Create comprehensive DataFrame

**Structure:**
```python
# Nested loop:
for patient in all_patients:           # 582 patients
    for modality in 4_modalities:      # 4 types
        # Collect information
        
# Total: 582 Ã— 4 = 2,328 records
```

**Line 12-13: â­ Smart Part**
```python
# we enough with just one image from each modality because the rest has the same shape
dcm = pydicom.dcmread(dicom_files[0])
shape = dcm.pixel_array.shape
```

**Why only `dicom_files[0]`?**

**Reasoning:**
- From Cell 57, we know **all images of same patient and same type have same shape**
- So, read **only one image** (first) and assume rest are same

**Benefit:**
```python
# Without optimization:
for file in dicom_files:  # 400 files
    shape = read_shape(file)  # Read 400 times!

# With optimization:
shape = read_shape(dicom_files[0])  # Read only once! âœ…

# Time saving:
# 582 patients Ã— 4 modalities Ã— 1 read = 2,328 reads
# Instead of:
# 582 patients Ã— 4 modalities Ã— ~130 reads = ~302,640 reads
# Saving: 99%!
```

**âš ï¸ Dangerous Assumption:**
```python
# Code assumes:
"All images of same patient and same type have same shape"

# But what if:
# One image is different? We'll miss it!
```

#### Cell 61: Most Common Shapes

```python
imges_info['shape'].value_counts()[:10]
```

**Expected output:**
```
(512, 512)    2100  â† Most common by far!
(256, 192)     150
(256, 256)      50
```

**Interpretation:**
- **90% of data:** (512, 512)
- **10% remaining:** Various shapes
- **Conclusion:** Most data is consistent, but there are variations

**Why is this important?**
- We need **size standardization** before training
- Will use `cv2.resize()` to make all images same size

#### Cell 62: Shape Distribution Plot

**Why horizontal bar chart?**
- Easier to read shapes like (512, 512) horizontally
- Clearer visualization

**What does the plot reveal?**
- (512, 512) **dominates** the data
- Other shapes are very rare
- **Decision:** Standardize all images to one size (e.g., 256Ã—256)

# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù…Ù† (ØªÙƒÙ…Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©) | Continuation Part 8 (Processing Continued)

---

### âœ… Cell 66-73: Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø§Ù„Ø© Cropping | Testing Cropping Function

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 66: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±

```markdown
**Cropping Test**
```

#### Cell 67: ØªØ·Ø¨ÙŠÙ‚ Cropping Ø¹Ù„Ù‰ ØµÙˆØ±Ø©

```python
image = load_dicom('/kaggle/input/.../train/00000/FLAIR/Image-152.dcm', visualize=True)
image = crop_image(image)
plt.figure(figsize=(6, 6))
plt.imshow(image, cmap='gray')
plt.axis("off")
plt.title("DICOM Image")
plt.show()
```

**ØªØ­Ù„ÙŠÙ„ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**Ø§Ù„Ø³Ø·Ø± 1: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©**
```python
image = load_dicom('/kaggle/.../Image-152.dcm', visualize=True)
```
- ÙŠÙ‚Ø±Ø£ DICOM
- `visualize=True` â†’ uint8 [0, 255]
- Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ: (512, 512) Ø¹Ø§Ø¯Ø©

**Ø§Ù„Ø³Ø·Ø± 2: ØªØ·Ø¨ÙŠÙ‚ Cropping**
```python
image = crop_image(image)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ØŸ**
```python
# Ù‚Ø¨Ù„:
image.shape = (512, 512)
# Ù…Ø¹Ø¸Ù…Ù‡Ø§ Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡:
# [0, 0, 0, 0, ..., brain pixels ..., 0, 0, 0]

# Ø¯Ø§Ø®Ù„ crop_image:
# 1. ÙŠØ¬Ø¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¯Ù…Ø§Øº
# 2. ÙŠÙ‚Øµ Ø§Ù„Ø®Ù„ÙÙŠØ©
# 3. ÙŠØ¶ÙŠÙ margin ØµØºÙŠØ±

# Ø¨Ø¹Ø¯:
image.shape = (280, 260)  # Ù…Ø«Ø§Ù„
# ÙÙ‚Ø· Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¯Ù…Ø§Øº + margin
```

**Ø§Ù„Ø³Ø·Ø± 3-7: Ø§Ù„Ø¹Ø±Ø¶**
- Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ©
- Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†ØªÙˆÙ‚Ø¹ Ø±Ø¤ÙŠØªÙ‡ØŸ**

**Ù‚Ø¨Ù„ Cropping:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                   â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â”‚  Brain  â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â”‚   Data  â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        512 Ã— 512 pixels
```

**Ø¨Ø¹Ø¯ Cropping:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚  â† margin
â”‚   â–‘Brainâ–‘   â”‚  â† brain region
â”‚   â–‘Data â–‘   â”‚
â”‚   â–‘â–‘â–‘â–‘â–‘â–‘â–‘   â”‚  â† margin
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ~280 Ã— 260
```

**Ø§Ù„ÙØ±Ù‚:**
- **Ù‚Ø¨Ù„:** Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡
- **Ø¨Ø¹Ø¯:** Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¯Ù…Ø§Øº
- **Ø§Ù„Ø­Ø¬Ù…:** ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† 512Ã—512 Ø¥Ù„Ù‰ ~280Ã—260 (Ø­ÙˆØ§Ù„ÙŠ 70% Ø£ØµØºØ±)

#### Cell 68: ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨

```markdown
WOW! Great!
```
- ÙŠØ¤ÙƒØ¯ Ø£Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ù…ØªØ§Ø²Ø©!

#### Cell 69: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯

```python
image.shape
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```python
(280, 260)
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**
- **280:** Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ© (Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ)
- **260:** Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù‚ØµÙˆØµØ© (Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©)
- **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ø´ÙƒÙ„ **ØºÙŠØ± Ù…Ø±Ø¨Ø¹**! (280 â‰  260)

**Ù„Ù…Ø§Ø°Ø§ ØºÙŠØ± Ù…Ø±Ø¨Ø¹ØŸ**
- Ø§Ù„Ø¯Ù…Ø§Øº Ù„ÙŠØ³ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø´ÙƒÙ„!
- Ø¹Ø§Ø¯Ø© Ø£Ø·ÙˆÙ„ Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù…Ù† Ø¹Ø±Ø¶Ù‡
- Ù‡Ø°Ø§ Ø·Ø¨ÙŠØ¹ÙŠ ØªÙ…Ø§Ù…Ø§Ù‹

---

### âœ… Cell 70-73: Ø¯Ø§Ù„Ø© Resize ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡Ø§ | Resize Function and Testing

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 70: Ø¯Ø§Ù„Ø© resize_image

```python
def resize_image(img, size):
    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ø³Ø·Ø± 1: Ø§Ù„ØªØ¹Ø±ÙŠÙ**
```python
def resize_image(img, size):
```

**Parameters:**
- **`img`:** numpy array Ù„Ù„ØµÙˆØ±Ø©
- **`size`:** tuple Ù„Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (width, height)
  - **âš ï¸ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹:** OpenCV ÙŠØ³ØªØ®Ø¯Ù… (width, height) ÙˆÙ„ÙŠØ³ (height, width)!

**Ø§Ù„Ø³Ø·Ø± 2: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚**
```python
return cv2.resize(img, size, interpolation=cv2.INTER_AREA)
```

**ØªØ­Ù„ÙŠÙ„ Parameters:**

**1. `img`:** Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø©

**2. `size`:** Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
```python
size = (256, 256)  # (width, height)
# NOT (height, width)!
```

**âš ï¸ Ø®Ø·Ø£ Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹:**
```python
# Ø®Ø·Ø£ âŒ:
img.shape = (280, 260)  # (height, width)
size = (280, 260)       # Ù†Ø³Ø®Ù†Ø§ Ù…Ù† shape
cv2.resize(img, size)   # Ø®Ø·Ø£! Ø³ØªÙƒÙˆÙ† (260, 280)

# ØµØ­ÙŠØ­ âœ…:
img.shape = (280, 260)  # (height, width)
size = (260, 280)       # Ø¹ÙƒØ³Ù†Ø§Ù‡Ø§!
# Ø£Ùˆ Ø§Ù„Ø£ÙØ¶Ù„:
size = (256, 256)       # Ø­Ø¬Ù… Ø«Ø§Ø¨Øª
```

**3. `interpolation=cv2.INTER_AREA`:**

**Ù…Ø§ Ù‡Ùˆ InterpolationØŸ**
Ø¹Ù†Ø¯ ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©ØŒ Ù†Ø­ØªØ§Ø¬ "Ø§Ø®ØªØ±Ø§Ø¹" Ù‚ÙŠÙ… Ø¨ÙƒØ³Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.

**Ù…Ø«Ø§Ù„:**
```python
# ØªØµØºÙŠØ± Ù…Ù† 4Ã—4 Ø¥Ù„Ù‰ 2Ã—2:
Original (4Ã—4):
[10, 20, 30, 40]
[50, 60, 70, 80]
[90, 100, 110, 120]
[130, 140, 150, 160]

Resized (2Ã—2):
[?, ?]  â† Ù…Ø§ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ
[?, ?]
```

**Ø£Ù†ÙˆØ§Ø¹ Interpolation ÙÙŠ OpenCV:**

**1. `cv2.INTER_NEAREST`:**
- **Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** Ø£Ù‚Ø±Ø¨ Ø¬Ø§Ø±
- **Ø§Ù„Ø³Ø±Ø¹Ø©:** Ø£Ø³Ø±Ø¹
- **Ø§Ù„Ø¬ÙˆØ¯Ø©:** Ø£Ø³ÙˆØ£ (Ø­ÙˆØ§Ù Ù…Ø³Ù†Ù†Ø©)
- **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** Ø¹Ù†Ø¯Ù…Ø§ Ø§Ù„Ø³Ø±Ø¹Ø© Ù…Ù‡Ù…Ø©

```python
# Ù…Ø«Ø§Ù„:
Original: [10, 20, 30, 40]
Resized (2):  [10, 30]  â† Ø£Ø®Ø° Ø£Ù‚Ø±Ø¨ Ù‚ÙŠÙ…Ø©
```

**2. `cv2.INTER_LINEAR` (Bilinear):**
- **Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** Ù…ØªÙˆØ³Ø· Ø®Ø·ÙŠ
- **Ø§Ù„Ø³Ø±Ø¹Ø©:** Ù…ØªÙˆØ³Ø·Ø©
- **Ø§Ù„Ø¬ÙˆØ¯Ø©:** Ø¬ÙŠØ¯Ø©
- **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¹Ø§Ø¯Ø©

```python
# Ù…Ø«Ø§Ù„:
Original: [10, 20, 30, 40]
Resized (2):  [15, 35]  â† Ù…ØªÙˆØ³Ø·
```

**3. `cv2.INTER_CUBIC` (Bicubic):**
- **Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** ØªÙ‚Ø±ÙŠØ¨ ØªÙƒØ¹ÙŠØ¨ÙŠ
- **Ø§Ù„Ø³Ø±Ø¹Ø©:** Ø¨Ø·ÙŠØ¡
- **Ø§Ù„Ø¬ÙˆØ¯Ø©:** Ù…Ù…ØªØ§Ø²Ø©
- **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** Ø¹Ù†Ø¯ Ø§Ù„ØªÙƒØ¨ÙŠØ±

**4. `cv2.INTER_AREA` â­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**
- **Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** Ø¥Ø¹Ø§Ø¯Ø© Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø­Ø© (area resampling)
- **Ø§Ù„Ø³Ø±Ø¹Ø©:** Ù…ØªÙˆØ³Ø·Ø©
- **Ø§Ù„Ø¬ÙˆØ¯Ø©:** **Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„ØªØµØºÙŠØ±!**
- **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:** Ø§Ù„ØªØµØºÙŠØ± (downsampling)

**Ù„Ù…Ø§Ø°Ø§ INTER_AREA Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„ØªØµØºÙŠØ±ØŸ**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ Ø·Ø±Ù‚ Ø£Ø®Ø±Ù‰:**
```python
# ØªØµØºÙŠØ± Ù…Ù† 512Ã—512 Ø¥Ù„Ù‰ 256Ã—256
# ÙƒÙ„ Ø¨ÙƒØ³Ù„ ÙÙŠ Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ…Ø«Ù„ 4 Ø¨ÙƒØ³Ù„Ø§Øª Ù…Ù† Ø§Ù„Ø£ØµÙ„ÙŠ (2Ã—2)

# INTER_LINEAR:
new_pixel = average_of_2_pixels  â† ÙŠØ£Ø®Ø° 2 ÙÙ‚Ø·! ÙŠÙÙ‚Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

# INTER_AREA:
new_pixel = average_of_4_pixels  â† ÙŠØ£Ø®Ø° ÙƒÙ„ Ø§Ù„Ù€ 4! âœ…
```

**Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„ØªÙ‚Ù†ÙŠ:**
- INTER_AREA ÙŠØ£Ø®Ø° **Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©**
- ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£ÙƒØ¨Ø± Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
- ÙŠÙ‚Ù„Ù„ Ø§Ù„Ù€ aliasing (Ø§Ù„ØªØ´ÙˆÙŠØ´)

**Ù…Ø«Ø§Ù„ Ø±Ù‚Ù…ÙŠ:**
```python
# Original 4Ã—4 â†’ Resize to 2Ã—2

Original:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10  20 â”‚ 30  40 â”‚
â”‚ 50  60 â”‚ 70  80 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 90 100 â”‚110 120 â”‚
â”‚130 140 â”‚150 160 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# INTER_AREA:
# Top-left pixel = average(10, 20, 50, 60) = 35
# Top-right pixel = average(30, 40, 70, 80) = 55
# Bottom-left pixel = average(90, 100, 130, 140) = 115
# Bottom-right pixel = average(110, 120, 150, 160) = 135

Resized:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  35 â”‚  55 â”‚
â”‚ 115 â”‚ 135 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

**Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:**

| Method | Speed | Quality | Best for |
|--------|-------|---------|----------|
| INTER_NEAREST | âš¡âš¡âš¡ | â­ | Speed |
| INTER_LINEAR | âš¡âš¡ | â­â­â­ | General |
| INTER_CUBIC | âš¡ | â­â­â­â­ | Upsampling |
| **INTER_AREA** | âš¡âš¡ | **â­â­â­â­â­** | **Downsampling** |

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø­Ù† Ù†Ø³ØªØ®Ø¯Ù… INTER_AREAØŸ**
âœ… Ù†Ø­Ù† Ù†ØµØºÙ‘Ø± Ø§Ù„ØµÙˆØ± (Ù…Ù† 512Ã—512 Ø¥Ù„Ù‰ 256Ã—256)
âœ… INTER_AREA Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„ØªØµØºÙŠØ±
âœ… ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„
âœ… ÙŠÙ‚Ù„Ù„ Ø§Ù„ØªØ´ÙˆÙŠØ´

#### Cell 71: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±

```markdown
**Resize Test**
```

#### Cell 72: ØªØ·Ø¨ÙŠÙ‚ Resize

```python
image = resize_image(image, (256, 256))
image.shape
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
image = resize_image(image, (256, 256))
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«ØŸ**
```python
# Ù‚Ø¨Ù„:
image.shape = (280, 260)  # Ù…Ù† Cropping

# Ø¯Ø§Ø®Ù„ resize_image:
cv2.resize(image, (256, 256), interpolation=cv2.INTER_AREA)
# ÙŠØ£Ø®Ø° ÙƒÙ„ 280Ã—260 Ø¨ÙƒØ³Ù„
# ÙŠØ­ÙˆÙ„Ù‡Ù… Ø¥Ù„Ù‰ 256Ã—256 Ø¨ÙƒØ³Ù„
# Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… area resampling

# Ø¨Ø¹Ø¯:
image.shape = (256, 256)  # Ù…Ø±Ø¨Ø¹ ØªÙ…Ø§Ù…Ø§Ù‹! âœ…
```

**Ø§Ù„Ø³Ø·Ø± 2:**
```python
image.shape
```

**Ø§Ù„Ù…Ø®Ø±Ø¬:**
```python
(256, 256)
```

**Ù„Ù…Ø§Ø°Ø§ 256Ã—256ØŸ**

**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:**
1. **Ù‚ÙˆØ© 2:** 256 = 2^8
   - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
   - Ø³Ø±ÙŠØ¹ ÙÙŠ Ø§Ù„Ø­ÙˆØ³Ø¨Ø©

2. **ØªÙˆØ§Ø²Ù†:**
   - Ù„ÙŠØ³ ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ÙŠÙÙ‚Ø¯ ØªÙØ§ØµÙŠÙ„)
   - Ù„ÙŠØ³ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ (ÙŠØ³ØªÙ‡Ù„Ùƒ Ø°Ø§ÙƒØ±Ø©)

3. **Ù…Ø¹ÙŠØ§Ø±:**
   - Ø´Ø§Ø¦Ø¹ ÙÙŠ medical imaging
   - ÙŠØ¹Ù…Ù„ Ø¬ÙŠØ¯Ø§Ù‹ Ù…Ø¹ CNNs

4. **Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„:**
   ```python
   # Ø®ÙŠØ§Ø±Ø§Øª Ø£Ø®Ø±Ù‰ Ø´Ø§Ø¦Ø¹Ø©:
   (224, 224)  â† Ù…Ø¹ÙŠØ§Ø± ImageNet
   (128, 128)  â† Ø£ØµØºØ±ØŒ Ø£Ø³Ø±Ø¹
   (512, 512)  â† Ø£ÙƒØ¨Ø±ØŒ Ø£Ø¨Ø·Ø£
   ```

#### Cell 73: ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù†Ø¬Ø§Ø­

```markdown
Nice!
```

**Ù…Ù„Ø®Øµ Pipeline Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†:**

```python
# 1. Load DICOM
image = load_dicom(path, visualize=True)
# Shape: (512, 512), Values: [0, 255]

# 2. Crop
image = crop_image(image)
# Shape: (280, 260), Values: [0, 255]
# Removed: ~70% of pixels (background)

# 3. Resize
image = resize_image(image, (256, 256))
# Shape: (256, 256), Values: [0, 255]
# Standardized: all images now same size âœ…

# Ready for next step: Normalization
```

**In English:**

#### Cell 70: resize_image Function

```python
def resize_image(img, size):
    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)
```

**Deep Analysis:**

**âš ï¸ Very Important:** OpenCV uses (width, height) NOT (height, width)!

**Common mistake:**
```python
# Wrong âŒ:
img.shape = (280, 260)  # (height, width)
size = (280, 260)       # copied from shape
cv2.resize(img, size)   # Wrong! Will be (260, 280)

# Correct âœ…:
size = (256, 256)       # fixed size
```

**Why `interpolation=cv2.INTER_AREA`?**

**Interpolation types:**

**1. `cv2.INTER_NEAREST`:**
- Fastest
- Worst quality (jagged edges)

**2. `cv2.INTER_LINEAR`:**
- Medium speed
- Good quality

**3. `cv2.INTER_CUBIC`:**
- Slow
- Excellent quality
- Best for upsampling

**4. `cv2.INTER_AREA` â­ Used:**
- Medium speed
- **Best quality for downsampling!**
- Best for our case

**Why INTER_AREA best for downsampling?**

**The problem with other methods:**
```python
# Downsampling from 512Ã—512 to 256Ã—256
# Each new pixel should represent 4 original pixels (2Ã—2)

# INTER_LINEAR:
new_pixel = average_of_2_pixels  â† Only 2! Loses info

# INTER_AREA:
new_pixel = average_of_4_pixels  â† All 4! âœ…
```

**Why we use INTER_AREA?**
âœ… We're downsampling (512Ã—512 â†’ 256Ã—256)
âœ… INTER_AREA best for downsampling
âœ… Preserves maximum details
âœ… Reduces aliasing

**Why 256Ã—256?**

**Reasons:**
1. **Power of 2:** 256 = 2^8 (good for digital processing)
2. **Balance:** Not too small, not too large
3. **Standard:** Common in medical imaging
4. **Works well with CNNs**

**Summary of Pipeline so far:**
```python
# 1. Load: (512, 512), [0, 255]
# 2. Crop: (280, 260), [0, 255], removed ~70% background
# 3. Resize: (256, 256), [0, 255], standardized âœ…
```

---

### âœ… Cell 74-82: Ø¯Ø§Ù„Ø© Normalization ÙˆØ§Ø®ØªØ¨Ø§Ø±Ù‡Ø§ | Normalization Function and Testing

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 74: Ø¯Ø§Ù„Ø© normalize_volume

```python
def normalize_volume(volume):
    """
    Normalize MRI volume per patient (Z-score normalization).
    """
    mean = np.mean(volume)
    std = np.std(volume)
    if std > 0:
        volume = (volume - mean) / std
    return volume
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ù‡Ø¯Ù:**
- ØªØ·Ø¨ÙŠØ¹ **Volume ÙƒØ§Ù…Ù„** (ÙƒÙ„ ØµÙˆØ± Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯)
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Z-score normalization

**Ù…Ø§ Ù‡Ùˆ Z-score NormalizationØŸ**

**Ø§Ù„ÙÙƒØ±Ø©:**
```python
normalized_value = (value - mean) / std
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
- Mean = 0
- Standard deviation = 1
- Ø§Ù„Ù‚ÙŠÙ… Ù…ÙˆØ²Ø¹Ø© Ø­ÙˆÙ„ 0

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø·Ø¨Ù‘Ø¹ Volume ÙƒØ§Ù…Ù„ ÙˆÙ„ÙŠØ³ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©ØŸ**

**Ø§Ù„ÙØ±Ù‚:**

**Per-Image Normalization:**
```python
# ÙƒÙ„ ØµÙˆØ±Ø© Ø¨Ù…ÙØ±Ø¯Ù‡Ø§:
for image in patient_images:
    mean = image.mean()
    std = image.std()
    normalized = (image - mean) / std

# Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:
# Slice 1 (Ø¯Ø§ÙƒÙ†Ø©): mean=50  â†’ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: mean=0
# Slice 2 (ÙØ§ØªØ­Ø©): mean=150 â†’ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: mean=0
# ÙÙ‚Ø¯Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø§Ù„Ø³Ø·ÙˆØ¹ Ø§Ù„Ù†Ø³Ø¨ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ù€ slices!
```

**Per-Volume Normalization (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…):**
```python
# ÙƒÙ„ ØµÙˆØ± Ø§Ù„Ù…Ø±ÙŠØ¶ Ù…Ø¹Ø§Ù‹:
all_slices = stack(patient_images)
mean = all_slices.mean()
std = all_slices.std()

for image in patient_images:
    normalized = (image - mean) / std

# Ø§Ù„ÙØ§Ø¦Ø¯Ø©:
# Slice 1 (Ø¯Ø§ÙƒÙ†Ø©): Ù‚ÙŠÙ… Ø³Ø§Ù„Ø¨Ø©  â† ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ÙƒÙˆÙ†Ù‡Ø§ Ø¯Ø§ÙƒÙ†Ø©
# Slice 2 (ÙØ§ØªØ­Ø©): Ù‚ÙŠÙ… Ù…ÙˆØ¬Ø¨Ø© â† ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ÙƒÙˆÙ†Ù‡Ø§ ÙØ§ØªØ­Ø©
# Ø­Ø§ÙØ¸Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©! âœ…
```

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø³Ø·Ø± Ø¨Ø³Ø·Ø±:**

**Ø§Ù„Ø³Ø·Ø± 5-6:**
```python
mean = np.mean(volume)
std = np.std(volume)
```

**`volume`:**
- numpy array Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
- Ø§Ù„Ø´ÙƒÙ„: (slices, height, width)
- Ù…Ø«Ø§Ù„: (128, 256, 256)

**`np.mean(volume)`:**
- ÙŠØ­Ø³Ø¨ Ù…ØªÙˆØ³Ø· **ÙƒÙ„** Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ Volume
- Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª: 128 Ã— 256 Ã— 256 = 8,388,608
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ø±Ù‚Ù… ÙˆØ§Ø­Ø¯ (scalar)

**Ù…Ø«Ø§Ù„:**
```python
volume.shape = (128, 256, 256)
# Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª: 8,388,608

# Ù‚ÙŠÙ… Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª:
# [0.1, 0.2, 0.15, 0.3, ...]  â† 8 Ù…Ù„ÙŠÙˆÙ† Ù‚ÙŠÙ…Ø©

mean = np.mean(volume)
# mean = 0.5  â† Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ…

std = np.std(volume)
# std = 0.2  â† Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
```

**Ø§Ù„Ø³Ø·Ø± 7-8:**
```python
if std > 0:
    volume = (volume - mean) / std
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„Ø´Ø±Ø· `if std > 0`ØŸ**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:**
```python
# Ø¥Ø°Ø§ ÙƒØ§Ù† Volume Ø£Ø³ÙˆØ¯ ØªÙ…Ø§Ù…Ø§Ù‹:
volume = np.zeros((128, 256, 256))

mean = 0
std = 0  â† ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…ØªØ³Ø§ÙˆÙŠØ©!

# Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø´Ø±Ø·:
volume = (volume - 0) / 0  â† Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±! âŒ
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: nan (Not a Number) Ø£Ùˆ inf
```

**Ù…Ø¹ Ø§Ù„Ø´Ø±Ø·:**
```python
if std > 0:  # False
    # Ù„Ø§ Ù†Ù†ÙØ° Ø§Ù„Ù‚Ø³Ù…Ø©
    
return volume  # Ù†Ø±Ø¬Ø¹ Volume ÙƒÙ…Ø§ Ù‡Ùˆ (ÙƒÙ„Ù‡ Ø£ØµÙØ§Ø±)
```

**Ø§Ù„ØªØ·Ø¨ÙŠØ¹:**
```python
volume = (volume - mean) / std
```

**Ù…Ø«Ø§Ù„ Ø±Ù‚Ù…ÙŠ:**
```python
# Ù‚Ø¨Ù„:
volume = [0.1, 0.3, 0.5, 0.7, 0.9]
mean = 0.5
std = 0.3

# Ø§Ù„ØªØ·Ø¨ÙŠØ¹:
volume[0] = (0.1 - 0.5) / 0.3 = -0.4 / 0.3 = -1.33
volume[1] = (0.3 - 0.5) / 0.3 = -0.2 / 0.3 = -0.67
volume[2] = (0.5 - 0.5) / 0.3 = 0.0 / 0.3 = 0.00   â† Ø§Ù„Ù…ØªÙˆØ³Ø·
volume[3] = (0.7 - 0.5) / 0.3 = 0.2 / 0.3 = 0.67
volume[4] = (0.9 - 0.5) / 0.3 = 0.4 / 0.3 = 1.33

# Ø¨Ø¹Ø¯:
volume = [-1.33, -0.67, 0.00, 0.67, 1.33]
mean = 0.0  âœ…
std = 1.0   âœ…
```

**Ø®ØµØ§Ø¦Øµ Z-score:**
- **68%** Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† [-1, 1]
- **95%** Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† [-2, 2]
- **99.7%** Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† [-3, 3]

**Ø§Ù„Ø³Ø·Ø± 9:**
```python
return volume
```

**Ù„Ù…Ø§Ø°Ø§ Normalization Ù…Ù‡Ù…ØŸ**

**1. ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†Ø·Ø§Ù‚:**
```python
# Ù‚Ø¨Ù„:
Patient 1: mean=100, std=50  â†’ values [0, 200]
Patient 2: mean=150, std=30  â†’ values [90, 210]

# Ø¨Ø¹Ø¯:
Patient 1: mean=0, std=1  â†’ values [-2, 2]
Patient 2: mean=0, std=1  â†’ values [-2, 2]

# ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚! âœ…
```

**2. ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨:**
- Gradients Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹
- Convergence Ø£Ø³Ø±Ø¹
- Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„

**3. Ù…Ø¹ÙŠØ§Ø± ÙÙŠ Deep Learning:**
- Ù…Ø¹Ø¸Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ ØªØªÙˆÙ‚Ø¹ normalized inputs
- ÙŠØ³Ù‡Ù„ Transfer learning

#### Cell 75: Ø¯Ø§Ù„Ø© get_modality_volume

```python
def get_modality_volume(modality_path, visualize, size=(256, 256)):
    """
    Get all slices for a modality and return as a 3D numpy array.
    Each slice is preprocessed (cropped, resized, normalized).
    """
    dicom_files = sorted(
        glob.glob(os.path.join(modality_path, "*.dcm")),
        key=lambda x: int(x[:-4].split("-")[-1]),
    )
    
    volume_slices = []
    for file in dicom_files:
        image = load_dicom(file, visualize)
        image = crop_image(image)
        image = resize_image(image, size)
        volume_slices.append(image)
    
    volume = np.stack(volume_slices, axis=-1)
    volume = normalize_volume(volume)
    
    return volume
```

**ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„:**

Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¬Ù…Ø¹ **ÙƒÙ„ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©** ÙÙŠ Ù…ÙƒØ§Ù† ÙˆØ§Ø­Ø¯!

**Parameters:**
- **`modality_path`:** Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ÙˆØ¹ (FLAIR, T1w, etc.)
- **`visualize`:** bool - Ù„Ù„Ø¹Ø±Ø¶ Ø£Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŸ
- **`size=(256, 256)`:** Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø¹Ø¯ resize

**Ø§Ù„Ø³Ø·Ø± 6-9: ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù„ÙØ§Øª**
```python
dicom_files = sorted(
    glob.glob(os.path.join(modality_path, "*.dcm")),
    key=lambda x: int(x[:-4].split("-")[-1]),
)
```
- Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ (sorted numerically)

**Ø§Ù„Ø³Ø·Ø± 11-16: Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© - Pipeline ÙƒØ§Ù…Ù„!**

```python
volume_slices = []
for file in dicom_files:
    image = load_dicom(file, visualize)
    image = crop_image(image)
    image = resize_image(image, size)
    volume_slices.append(image)
```

**ØªØªØ¨Ø¹ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©:**
```python
# Ø§Ù„ØµÙˆØ±Ø© 1:
file = ".../Image-100.dcm"

# Step 1: Load
image = load_dicom(file, visualize)
# Shape: (512, 512)
# Values: [0, 255] if visualize=True, [0, 1] if False

# Step 2: Crop
image = crop_image(image)
# Shape: (280, 260)  â† Ø£ØµØºØ±
# Values: Ù†ÙØ³Ù‡Ø§

# Step 3: Resize
image = resize_image(image, (256, 256))
# Shape: (256, 256)  â† Ù…ÙˆØ­Ø¯
# Values: Ù†ÙØ³Ù‡Ø§

# Step 4: Append
volume_slices.append(image)
# volume_slices = [image1]
```

**Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø©:**
```python
volume_slices = [
    image1,  # (256, 256)
    image2,  # (256, 256)
    image3,  # (256, 256)
    ...
    image400  # (256, 256)
]
# Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† 400 ØµÙˆØ±Ø©
```

**Ø§Ù„Ø³Ø·Ø± 18: Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ 3D array**

```python
volume = np.stack(volume_slices, axis=-1)
```

**Ù…Ø§ Ù‡Ùˆ `np.stack()`ØŸ**

**Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:**
```python
volume_slices = [
    array1,  # shape (256, 256)
    array2,  # shape (256, 256)
    ...
]
```

**`axis=-1` ÙŠØ¹Ù†ÙŠ:**
- stack Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£Ø®ÙŠØ±
- `-1` = Ø§Ù„Ù…Ø­ÙˆØ± Ø§Ù„Ø£Ø®ÙŠØ± ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
```python
volume.shape = (256, 256, 400)
#               â†‘    â†‘    â†‘
#              H    W   Slices
```

**ØªØµÙˆØ±:**
```python
# Ù‚Ø¨Ù„ stack:
[image1, image2, image3, ...]
  256    256    256
  Ã—      Ã—      Ã—
  256    256    256

# Ø¨Ø¹Ø¯ stack:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  image1    â”‚ â† slice 0
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  image2    â”‚ â† slice 1
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  image3    â”‚ â† slice 2
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    ...     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  256 Ã— 256 Ã— 400
```

**Ø¨Ø¯Ø§Ø¦Ù„ Ù„Ù€ axis:**
```python
# axis=-1 (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…):
volume = np.stack(slices, axis=-1)
# Shape: (256, 256, 400)  â† Slices ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©

# axis=0:
volume = np.stack(slices, axis=0)
# Shape: (400, 256, 256)  â† Slices ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©

# axis=1:
volume = np.stack(slices, axis=1)
# Shape: (256, 400, 256)  â† Slices ÙÙŠ Ø§Ù„ÙˆØ³Ø· (ØºØ±ÙŠØ¨!)
```

**Ù„Ù…Ø§Ø°Ø§ axis=-1ØŸ**
- ÙŠØªØ¨Ø¹ Ø§ØªÙØ§Ù‚ÙŠØ© (H, W, Slices)
- Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ø±Ø¶: `volume[:, :, i]` â†’ slice Ø±Ù‚Ù… i

**Ø§Ù„Ø³Ø·Ø± 19: Normalization**

```python
volume = normalize_volume(volume)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«ØŸ**
```python
# Ù‚Ø¨Ù„:
volume.shape = (256, 256, 400)
# Values: [0, 255] Ø£Ùˆ [0, 1] Ø­Ø³Ø¨ visualize

# Ø¯Ø§Ø®Ù„ normalize_volume:
mean = volume.mean()  # Ù…Ø«Ù„Ø§Ù‹ 127.5
std = volume.std()    # Ù…Ø«Ù„Ø§Ù‹ 50.2
volume = (volume - 127.5) / 50.2

# Ø¨Ø¹Ø¯:
volume.shape = (256, 256, 400)  â† Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„
# Values: ~ [-3, 3], mean=0, std=1 âœ…
```

**Ø§Ù„Ø³Ø·Ø± 21:**
```python
return volume
```

**Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:**
- **Type:** numpy array 3D
- **Shape:** (256, 256, num_slices)
- **Values:** Z-score normalized (mean=0, std=1)
- **Ready:** Ù„Ù„Ø¹Ø±Ø¶ØŒ Ù„Ù„ØªØ­Ù„ÙŠÙ„ØŒ Ø£Ùˆ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬!

**In English:**

#### Cell 74: normalize_volume Function

**Why normalize entire volume instead of per-image?**

**Per-Image Normalization:**
```python
# Each image separately loses relative brightness info
Slice 1 (dark): mean=50  â†’ after: mean=0
Slice 2 (bright): mean=150 â†’ after: mean=0
# Lost relative brightness! âŒ
```

**Per-Volume Normalization (Used):**
```python
# All patient images together preserve relative brightness
Slice 1 (dark): negative values  â† stays dark
Slice 2 (bright): positive values â† stays bright
# Preserved relative brightness! âœ…
```

**Why check `if std > 0`?**
```python
# If volume is completely black:
std = 0
# Without check:
volume / 0  â† Division by zero! âŒ
```

**Z-score properties:**
- **68%** of values between [-1, 1]
- **95%** of values between [-2, 2]
- **99.7%** of values between [-3, 3]

#### Cell 75: get_modality_volume Function

This function combines **all processing steps** in one place!

**Processing Pipeline:**
```python
# For each slice:
1. load_dicom()     â†’ (512, 512), [0-4095]
2. crop_image()     â†’ (280, 260), remove background
3. resize_image()   â†’ (256, 256), standardize
4. append to list

# After loop:
5. np.stack()       â†’ (256, 256, 400), 3D array
6. normalize_volume() â†’ mean=0, std=1

# Output: Ready for model!
```

**What is `np.stack()`?**
```python
# Input: list of 2D arrays
[array1, array2, array3, ...]  # Each (256, 256)

# Output: 3D array
np.stack(arrays, axis=-1)
# Shape: (256, 256, 400)  â† Stacked on last axis
```

**Why axis=-1?**
- Follows convention (H, W, Slices)
- Easy to access: `volume[:, :, i]` â†’ slice i


# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØªØ§Ø³Ø¹ (Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ§Ù„ØªØµÙÙŠØ©) | Continuation Part 9 (Testing and Filtering)

---

### âœ… Cell 76-82: Ø§Ø®ØªØ¨Ø§Ø± get_modality_volume | Testing get_modality_volume

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 76: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ù„Ø©

```python
modality_path = "/kaggle/input/.../train/00000/FLAIR"
volume = get_modality_volume(modality_path=modality_path, visualize=True)
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«ØŸ**

**Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**

**1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±:**
```python
modality_path = ".../train/00000/FLAIR"
# Ø§Ù„Ù…Ø±ÙŠØ¶ 00000ØŒ Ù†ÙˆØ¹ FLAIR
```

**2. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø©:**
```python
volume = get_modality_volume(modality_path, visualize=True)
```

**Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø§Ù„Ø©:**
```python
# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª
dicom_files = sorted(glob.glob(".../FLAIR/*.dcm"), ...)
# Ø§Ù„Ù†ØªÙŠØ¬Ø©: 400 Ù…Ù„Ù Ù…Ø±ØªØ¨

# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„Ù
for file in dicom_files:  # 400 ØªÙƒØ±Ø§Ø±
    image = load_dicom(file, visualize=True)  # (512, 512) â†’ [0, 255]
    image = crop_image(image)                  # â†’ (280, 260)
    image = resize_image(image, (256, 256))    # â†’ (256, 256)
    volume_slices.append(image)

# Ø§Ù„Ø®Ø·ÙˆØ© 3: Stack
volume = np.stack(volume_slices, axis=-1)
# Shape: (256, 256, 400)

# Ø§Ù„Ø®Ø·ÙˆØ© 4: Normalize
volume = normalize_volume(volume)
# Values: mean=0, std=1
```

**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```python
# 400 ØµÙˆØ±Ø© Ã— 0.05 Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ ØµÙˆØ±Ø©
# â‰ˆ 20 Ø«Ø§Ù†ÙŠØ©
```

#### Cell 77: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´ÙƒÙ„

```python
print(volume.shape)
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```python
(256, 256, 400)
```

**Ø§Ù„ØªÙØ³ÙŠØ±:**
- **256:** Ø§Ø±ØªÙØ§Ø¹ ÙƒÙ„ slice
- **256:** Ø¹Ø±Ø¶ ÙƒÙ„ slice
- **400:** Ø¹Ø¯Ø¯ Ø§Ù„Ù€ slices

**Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:**
```python
# float32 (4 bytes per value):
size = 256 Ã— 256 Ã— 400 Ã— 4 bytes
     = 104,857,600 bytes
     = 100 MB

# Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯ØŒ Ù†ÙˆØ¹ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·!

# Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø­Ø¯ØŒ 4 Ø£Ù†ÙˆØ§Ø¹:
100 MB Ã— 4 = 400 MB

# Ù„ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰ (582):
400 MB Ã— 582 = 232,800 MB â‰ˆ 233 GB!

# Ù„Ù‡Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… on-the-fly processing! âœ…
```

#### Cell 78-81: Ø¯Ø§Ù„Ø© ÙˆØ¹Ø±Ø¶ slices Ù…Ø®ØªÙ„ÙØ©

**Cell 78: Ø¯Ø§Ù„Ø© Ø§Ù„Ø¹Ø±Ø¶**
```python
def visualize_modality_volume(volume, slice_idx):
    plt.figure(figsize=(6,6))
    plt.imshow(volume[:, :, slice_idx], cmap='gray')
    plt.title(f"Slice {slice_idx}")
    plt.axis('off')
```

**ØªØ­Ù„ÙŠÙ„:**

**Ø§Ù„Ø³Ø·Ø± 1:**
```python
def visualize_modality_volume(volume, slice_idx):
```
- **`volume`:** 3D array (256, 256, slices)
- **`slice_idx`:** Ø±Ù‚Ù… Ø§Ù„Ù€ slice Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¹Ø±Ø¶Ù‡

**Ø§Ù„Ø³Ø·Ø± 3:**
```python
plt.imshow(volume[:, :, slice_idx], cmap='gray')
```

**ØªØ­Ù„ÙŠÙ„ `volume[:, :, slice_idx]`:**

**Ø§Ù„ÙÙ‡Ù…:**
```python
volume.shape = (256, 256, 400)
#               H    W   Slices

# Indexing:
volume[:, :, slice_idx]
#      â†‘  â†‘  â†‘
#      |  |  â””â”€ slice Ù…Ø­Ø¯Ø¯
#      |  â””â”€â”€â”€ ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (width)
#      â””â”€â”€â”€â”€â”€ ÙƒÙ„ Ø§Ù„ØµÙÙˆÙ (height)

# Ø§Ù„Ù†ØªÙŠØ¬Ø©: 2D array
# Shape: (256, 256)
```

**Ø£Ù…Ø«Ù„Ø©:**
```python
# Slice Ø§Ù„Ø£ÙˆÙ„:
volume[:, :, 0]  â†’ image (256, 256)

# Slice ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ:
volume[:, :, 200]  â†’ image (256, 256)

# Slice Ø§Ù„Ø£Ø®ÙŠØ±:
volume[:, :, 399]  â†’ image (256, 256)
```

**ØªØµÙˆØ±:**
```python
Volume = stack of images
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Slice 0â”‚ â† volume[:, :, 0]
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Slice 1â”‚ â† volume[:, :, 1]
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ...  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚Slice399â”‚ â† volume[:, :, 399]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Cell 79-81: Ø¹Ø±Ø¶ 3 slices Ù…Ø®ØªÙ„ÙØ©**

```python
# Cell 79:
visualize_modality_volume(volume, 20)

# Cell 80:
visualize_modality_volume(volume, 200)

# Cell 81:
visualize_modality_volume(volume, 350)
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŸ**

**Slice 20:**
- ÙÙŠ **Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©** (20 Ù…Ù† 400)
- **5%** Ù…Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚
- Ø¹Ø§Ø¯Ø©: Ù‚Ù…Ø© Ø§Ù„Ø±Ø£Ø³
- Ù‚Ø¯ ÙŠØ¸Ù‡Ø±: Ù‚Ø´Ø±Ø© Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø¹Ù„ÙˆÙŠØ©

**Slice 200:**
- ÙÙŠ **Ø§Ù„Ù…Ù†ØªØµÙ** ØªÙ…Ø§Ù…Ø§Ù‹ (200 Ù…Ù† 400)
- **50%** Ù…Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚
- Ø¹Ø§Ø¯Ø©: Ø£ÙƒØ«Ø± Ø¬Ø²Ø¡ Ù…ÙÙŠØ¯!
- ÙŠØ¸Ù‡Ø±: Ù…Ø¹Ø¸Ù… Ø¨Ù†ÙŠØ© Ø§Ù„Ø¯Ù…Ø§ØºØŒ Ø§Ù„Ø£ÙˆØ±Ø§Ù… ØºØ§Ù„Ø¨Ø§Ù‹ Ù‡Ù†Ø§

**Slice 350:**
- Ù‚Ø±Ø¨ **Ø§Ù„Ù†Ù‡Ø§ÙŠØ©** (350 Ù…Ù† 400)
- **87.5%** Ù…Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚
- Ø¹Ø§Ø¯Ø©: Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¯Ù…Ø§Øº
- Ù‚Ø¯ ÙŠØ¸Ù‡Ø±: Ø§Ù„Ù…Ø®ÙŠØ®ØŒ Ø¬Ø°Ø¹ Ø§Ù„Ø¯Ù…Ø§Øº

**ØªØµÙˆØ± 3D:**
```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† Top of head
     â”‚ Slice 20â”‚   (cortex, minimal brain)
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚   ...   â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚Slice 200â”‚   â† Middle (most informative)
     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚      (full brain structure)
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚   ...   â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚Slice 350â”‚   â† Bottom
     â”‚ â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘ â”‚      (cerebellum, brainstem)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Neck
```

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø¨Ø­Ø« Ø¹Ù†Ù‡ØŸ**

**ÙÙŠ Slice 20:**
âœ“ Ù‡Ù„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø¶Ø­Ø©ØŸ
âœ“ Ù‡Ù„ Ø§Ù„Ù‚Øµ (cropping) Ø¬ÙŠØ¯ØŸ
âœ— Ù„ÙŠØ³ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

**ÙÙŠ Slice 200:**
âœ“ Ù‡Ù„ Ù†Ø±Ù‰ Ø§Ù„Ø¯Ù…Ø§Øº ÙƒØ§Ù…Ù„Ø§Ù‹ØŸ
âœ“ Ù‡Ù„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¬ÙŠØ¯ØŸ
âœ“ Ù‡Ù„ Ø§Ù„ÙˆØ±Ù… Ù…Ø±Ø¦ÙŠ (Ø¥Ù† ÙˆØ¬Ø¯)ØŸ
âœ“ **Ø§Ù„Ø£Ù‡Ù… Ù„Ù„ØªØ´Ø®ÙŠØµ!**

**ÙÙŠ Slice 350:**
âœ“ Ù‡Ù„ Ù…Ø§ Ø²Ù„Ù†Ø§ Ù†Ø±Ù‰ Ø¯Ù…Ø§ØºØŸ
âœ“ Ø£Ù… Ø¨Ø¯Ø£Ù†Ø§ Ù†Ø±Ù‰ Ø§Ù„Ø±Ù‚Ø¨Ø©/Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¯Ù…Ø§ØºØŸ
âœ“ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù€ volume

#### Cell 82: Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©

```markdown
**YES! there are many outliers! We can notice that the outer images either black or have some white pixels! but the center ones have a real data**

Let's handle it.
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©:**
```python
# ØªÙˆØ²ÙŠØ¹ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù€ slices:
Slice 0-50:     â–‘â–‘â–‘â–‘â–‘  â† Ù…Ø¹Ø¸Ù…Ù‡Ø§ Ø³ÙˆØ¯Ø§Ø¡ Ø£Ùˆ Ù‚Ù„ÙŠÙ„Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
Slice 50-350:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙÙŠØ¯Ø© (Ø§Ù„Ø¯Ù…Ø§Øº)
Slice 350-400:  â–‘â–‘â–‘â–‘â–‘  â† Ù…Ø¹Ø¸Ù…Ù‡Ø§ Ø³ÙˆØ¯Ø§Ø¡ Ø£Ùˆ Ù‚Ù„ÙŠÙ„Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

# Ù…Ù† Ø£ØµÙ„ 400 slice:
# - ÙÙ‚Ø· ~300 Ù…ÙÙŠØ¯Ø©
# - ~100 outliers (Ø®Ø§Ø±Ø¬ÙŠØ©)
```

**Ù„Ù…Ø§Ø°Ø§ ØªÙˆØ¬Ø¯ outliersØŸ**

**1. Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø³Ø­ (Top slices):**
```python
# Slices 0-50:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â–‘â–‘â–‘â–‘â–‘  â”‚ â† Ù‚Ù…Ø© Ø§Ù„Ø±Ø£Ø³
â”‚  â–‘â–‘â–ˆâ–‘â–‘  â”‚ â† Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
â”‚  â–‘â–‘â–‘â–‘â–‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# Ù…Ø¹Ø¸Ù…Ù‡Ø§ Ù‡ÙˆØ§Ø¡/Ø¬Ù…Ø¬Ù…Ø©
```

**2. Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø³Ø­ (Bottom slices):**
```python
# Slices 350-400:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â–‘â–‘â–‘â–‘â–‘  â”‚ â† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ù…Ø¬Ù…Ø©
â”‚  â–‘â–ˆâ–ˆâ–ˆâ–‘  â”‚ â† Ø±Ù‚Ø¨Ø©/brainstem ØµØºÙŠØ±
â”‚  â–‘â–‘â–‘â–‘â–‘  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# Ø®Ø§Ø±Ø¬ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
```

**3. ØµÙˆØ± Ø³ÙˆØ¯Ø§Ø¡ (Artifacts):**
```python
# Ø¨Ø¹Ø¶ Ø§Ù„Ù€ slices:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  00000  â”‚
â”‚  00000  â”‚ â† Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØµÙˆÙŠØ±
â”‚  00000  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹
```

**ØªØ£Ø«ÙŠØ± Outliers:**

**Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:**
```python
# Ù…Ø¹ outliers:
Input = [black, black, brain, brain, brain, ..., black, black]
#        â†‘                                              â†‘
#     Ø¶ÙˆØ¶Ø§Ø¡                                         Ø¶ÙˆØ¶Ø§Ø¡

# Model ÙŠØªØ¹Ù„Ù…:
"Ø¨Ø¹Ø¶ Ø§Ù„ØµÙˆØ± Ø³ÙˆØ¯Ø§Ø¡" â†’ Ù„ÙŠØ³ Ù…ÙÙŠØ¯!
```

**Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:**
```python
# 400 slices:
Total size = 256 Ã— 256 Ã— 400 = 100 MB

# Ø¥Ø°Ø§ Ø£Ø¨Ù‚ÙŠÙ†Ø§ ÙÙ‚Ø· Ø§Ù„Ù…ÙÙŠØ¯ (~300):
Useful size = 256 Ã— 256 Ã— 300 = 75 MB
# ØªÙˆÙÙŠØ±: 25%!
```

**Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡:**
```python
# Processing time:
400 slices Ã— 0.05s = 20s

# Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©:
300 slices Ã— 0.05s = 15s
# ØªÙˆÙÙŠØ±: 25%!
```

**Ø§Ù„Ø­Ù„:** Ø¯Ø§Ù„Ø© ØªØµÙÙŠØ©! (Cell 83)

---

### âœ… Cell 83-91: Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© | Advanced Filtering Function

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 83: Ø¯Ø§Ù„Ø© get_filtered_modality_volume

```python
def get_filtered_modality_volume(modality_path, visualize, size, target_slices=128):
    dicom_files = sorted(
        glob.glob(os.path.join(modality_path, "*.dcm")),
        key=lambda x: int(x[:-4].split("-")[-1]),
    )
    processed = []
    for file in dicom_files:
        image = load_dicom(file, visualize)
        image = crop_image(image)
        if np.mean(image) < 0.01:  # ignore dark slices
            continue
        image = resize_image(image, size)
        processed.append(image)
    
    # Select middle slices
    if len(processed) > target_slices:
        start = (len(processed) - target_slices) // 2
        processed = processed[start:start + target_slices]
    
    volume = np.stack(processed, axis=-1)
    volume = normalize_volume(volume)
    
    return volume
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹:**

Ù‡Ø°Ù‡ Ù†Ø³Ø®Ø© **Ù…Ø­Ø³Ù‘Ù†Ø©** Ù…Ù† `get_modality_volume` Ù…Ø¹ **ØªØµÙÙŠØ© Ø°ÙƒÙŠØ©**!

**Parameter Ø§Ù„Ø¬Ø¯ÙŠØ¯:**
```python
target_slices=128
```
- Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ù€ slices Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
- Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: 128 (Ù‚ÙˆØ© 2)

**Ù„Ù…Ø§Ø°Ø§ 128ØŸ**
```python
# Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª:
64   â† Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ù‚Ø¯ Ù†ÙÙ‚Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
128  â† ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯ âœ…
256  â† ÙƒØ«ÙŠØ±ØŒ Ù‚Ø¯ ÙŠØ´Ù…Ù„ outliers
400  â† Ø§Ù„ÙƒÙ„ØŒ Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ±Ø©
```

**Ø§Ù„Ø³Ø·Ø± 6-13: Ø§Ù„Ø­Ù„Ù‚Ø© Ù…Ø¹ Ø§Ù„ØªØµÙÙŠØ©**

```python
processed = []
for file in dicom_files:
    image = load_dicom(file, visualize)
    image = crop_image(image)
    if np.mean(image) < 0.01:  # ignore dark slices
        continue
    image = resize_image(image, size)
    processed.append(image)
```

**Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© - Ø§Ù„Ø³Ø·Ø± 10-11:**

```python
if np.mean(image) < 0.01:  # ignore dark slices
    continue
```

**ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø±Ø·:**

**`np.mean(image)`:**
- Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
- Ø§Ù„Ù†Ø·Ø§Ù‚: [0, 1] (Ø¨Ø¹Ø¯ load_dicom normalization)

**`< 0.01`:**
- threshold Ù„Ù„ØµÙˆØ± Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©
- 0.01 = 1% Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„

**Ø£Ù…Ø«Ù„Ø©:**

**Ù…Ø«Ø§Ù„ 1: ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹**
```python
image = np.zeros((256, 256))
# ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª = 0

mean = np.mean(image)
# mean = 0.0

if 0.0 < 0.01:  # True
    continue  # ØªØ®Ø·ÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© âœ…
```

**Ù…Ø«Ø§Ù„ 2: ØµÙˆØ±Ø© Ø¯Ø§ÙƒÙ†Ø© Ø¬Ø¯Ø§Ù‹ (Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº)**
```python
# Ù…Ø¹Ø¸Ù… Ø§Ù„ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡ØŒ Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙØ§ØªØ­Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹
image = [
    [0, 0, 0, ..., 0],
    [0, 0.02, 0.03, ..., 0],
    [0, 0, 0, ..., 0],
    ...
]

mean = np.mean(image)
# mean â‰ˆ 0.005  â† Ø£Ù‚Ù„ Ù…Ù† 0.01

if 0.005 < 0.01:  # True
    continue  # ØªØ®Ø·ÙŠ âœ…
```

**Ù…Ø«Ø§Ù„ 3: ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© (Ø¯Ù…Ø§Øº ÙˆØ§Ø¶Ø­)**
```python
# Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø©ØŒ Ø¯Ù…Ø§Øº ÙˆØ§Ø¶Ø­
image = [
    [0, 0, 0, ..., 0],
    [0, 0.5, 0.6, 0.7, ..., 0],
    [0, 0.6, 0.8, 0.9, ..., 0],
    ...
]

mean = np.mean(image)
# mean â‰ˆ 0.15  â† Ø£ÙƒØ¨Ø± Ù…Ù† 0.01

if 0.15 < 0.01:  # False
    # Ù„Ø§ Ù†ØªØ®Ø·Ù‰ØŒ Ù†Ø¹Ø§Ù„Ø¬ ÙˆÙ†Ø­ÙØ¸ âœ…
    image = resize_image(image, size)
    processed.append(image)
```

**Ù„Ù…Ø§Ø°Ø§ 0.01 Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŸ**

**Ø§Ù„ØªØ¬Ø±Ø¨Ø©:**
```python
# Threshold Ù…Ø®ØªÙ„ÙØ©:
0.001 â† ØµØ§Ø±Ù… Ø¬Ø¯Ø§Ù‹ØŒ ÙŠØ­Ø°Ù ØµÙˆØ± Ù‚Ù„ÙŠÙ„Ø©
0.01  â† Ø¬ÙŠØ¯ØŒ ÙŠØ­Ø°Ù Ø§Ù„Ø¯Ø§ÙƒÙ†Ø© ÙÙ‚Ø· âœ…
0.05  â† Ù…ØªØ³Ø§Ù‡Ù„ØŒ Ù‚Ø¯ ÙŠØ­Ø°Ù ØµÙˆØ± Ù…ÙÙŠØ¯Ø©
0.1   â† Ù…ØªØ³Ø§Ù‡Ù„ Ø¬Ø¯Ø§Ù‹ØŒ ÙŠØ­Ø°Ù Ø­ØªÙ‰ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!
```

**Ø§Ø®ØªÙŠØ§Ø± empirical (ØªØ¬Ø±ÙŠØ¨ÙŠ):**
- Ø¨Ø¹Ø¯ ØªØ¬Ø±Ø¨Ø© Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ©
- 0.01 ÙŠØ¹Ø·ÙŠ Ø£ÙØ¶Ù„ ØªÙˆØ§Ø²Ù†

**Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø©:**
```python
processed = [
    good_slice_1,
    good_slice_2,
    good_slice_3,
    ...
    good_slice_N
]
# N â‰ˆ 300 Ù…Ù† Ø£ØµÙ„ 400
```

**Ø§Ù„Ø³Ø·Ø± 15-18: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù†ØªØµÙ**

```python
# Select middle slices
if len(processed) > target_slices:
    start = (len(processed) - target_slices) // 2
    processed = processed[start:start + target_slices]
```

**Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø°ÙƒÙŠ Ø¬Ø¯Ø§Ù‹!**

**Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ:**
```python
len(processed) = 300  # Ø¨Ø¹Ø¯ Ø­Ø°Ù Ø§Ù„Ù€ outliers
target_slices = 128   # Ù†Ø±ÙŠØ¯ ÙÙ‚Ø· 128
```

**Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©**
```python
start = (len(processed) - target_slices) // 2
      = (300 - 128) // 2
      = 172 // 2
      = 86
```

**Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±**
```python
processed = processed[start:start + target_slices]
          = processed[86:86 + 128]
          = processed[86:214]
```

**ØªØµÙˆØ±:**
```python
Original (300 slices):
[0, 1, 2, ..., 85, 86, 87, ..., 213, 214, 215, ..., 299]
                    â†‘                    â†‘
                  start               start+128

Selected (128 slices):
[86, 87, 88, ..., 213, 214]
 â†‘                       â†‘
 Ø§Ù„Ù…Ù†ØªØµÙ (86 Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ 86 Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©)
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„Ù…Ù†ØªØµÙØŸ**

**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:**
1. **Ø£ÙƒØ«Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:**
   ```
   Start: â–‘â–‘â–‘â–ˆâ–ˆ  â† Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
   Middle: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â† Ø¯Ù…Ø§Øº ÙƒØ§Ù…Ù„ âœ…
   End:   â–‘â–‘â–‘â–ˆâ–ˆ  â† Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
   ```

2. **Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø©:**
   - Ø§Ù„Ù…Ù†ØªØµÙ Ø¹Ø§Ø¯Ø© Ø£ÙˆØ¶Ø­
   - Ø£Ù‚Ù„ artifacts
   - ØªØ´Ø±ÙŠØ­ Ø£ÙƒØ«Ø± Ø§ÙƒØªÙ…Ø§Ù„Ø§Ù‹

3. **ÙŠØ´Ù…Ù„ Ø§Ù„ÙˆØ±Ù…:**
   - Ù…Ø¹Ø¸Ù… Ø§Ù„Ø£ÙˆØ±Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
   - Ù†Ø§Ø¯Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø£Ø·Ø±Ø§Ù

**Ù…Ø«Ø§Ù„ Ø±Ù‚Ù…ÙŠ ÙƒØ§Ù…Ù„:**

```python
# Ø§Ù„Ø£ØµÙ„ÙŠ: 400 slices
[0, 1, 2, ..., 399]

# Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©: 300 slices (Ø­Ø°Ù 100 Ø¯Ø§ÙƒÙ†Ø©)
[30, 31, 32, ..., 329]
 â†‘                  â†‘
 Slice 30        Slice 329 Ù…Ù† Ø§Ù„Ø£ØµÙ„ÙŠ

# Ø§Ø®ØªÙŠØ§Ø± 128 Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ:
start = (300 - 128) // 2 = 86

# Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: 128 slices
[116, 117, ..., 243]
  â†‘              â†‘
  Slice 116   Slice 243 Ù…Ù† Ø§Ù„Ø£ØµÙ„ÙŠ

# ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ù…Ù† 30% Ø¥Ù„Ù‰ 60% Ù…Ù† Ø§Ù„Ù€ volume Ø§Ù„Ø£ØµÙ„ÙŠ
# Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø£ÙƒØ«Ø± ÙØ§Ø¦Ø¯Ø©! âœ…
```

**Ø§Ù„Ø³Ø·Ø± 20-21: Stack & Normalize**
```python
volume = np.stack(processed, axis=-1)
volume = normalize_volume(volume)
```
- Ù†ÙØ³ Ø§Ù„Ø³Ø§Ø¨Ù‚

**Ø§Ù„Ø³Ø·Ø± 23:**
```python
return volume
```

**Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:**
- **Shape:** (256, 256, 128)
- **Values:** normalized (mean=0, std=1)
- **Quality:** ÙÙ‚Ø· Ø§Ù„Ù€ slices Ø§Ù„Ø¬ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ âœ…

**Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©:**

| Feature | get_modality_volume | get_filtered_modality_volume |
|---------|---------------------|------------------------------|
| **Outliers** | ÙŠØ¨Ù‚ÙŠÙ‡Ø§ | ÙŠØ­Ø°ÙÙ‡Ø§ âœ… |
| **Ø§Ù„Ø¹Ø¯Ø¯** | ÙƒÙ„ Ø§Ù„Ù€ slices (~400) | Ø¹Ø¯Ø¯ Ø«Ø§Ø¨Øª (128) âœ… |
| **Ø§Ù„Ø¬ÙˆØ¯Ø©** | Ù…ØªÙØ§ÙˆØªØ© | Ø¹Ø§Ù„ÙŠØ© âœ… |
| **Ø§Ù„Ø­Ø¬Ù…** | ~100 MB | ~32 MB âœ… |
| **Ø§Ù„Ø³Ø±Ø¹Ø©** | Ø£Ø¨Ø·Ø£ | Ø£Ø³Ø±Ø¹ âœ… |

**In English:**

#### Cell 83: get_filtered_modality_volume Function

**Enhanced version with smart filtering!**

**New parameter:**
```python
target_slices=128
```
- Desired number of final slices
- Default: 128 (power of 2)

**Lines 10-11: Dark Slice Filtering**
```python
if np.mean(image) < 0.01:  # ignore dark slices
    continue
```

**Analysis:**

**`np.mean(image)`:**
- Average of all pixels in image
- Range: [0, 1] (after load_dicom normalization)

**`< 0.01`:**
- Threshold for dark images
- 0.01 = 1% of full range

**Examples:**

**Example 1: Completely black**
```python
mean = 0.0
if 0.0 < 0.01:  # True
    continue  # Skip this image âœ…
```

**Example 2: Very dark (little brain)**
```python
mean â‰ˆ 0.005
if 0.005 < 0.01:  # True
    continue  # Skip âœ…
```

**Example 3: Normal (clear brain)**
```python
mean â‰ˆ 0.15
if 0.15 < 0.01:  # False
    # Don't skip, process and save âœ…
```

**Why 0.01 specifically?**
- Empirical (experimental) choice
- After trying different values
- 0.01 gives best balance

**Lines 15-18: Select Middle Slices**

```python
if len(processed) > target_slices:
    start = (len(processed) - target_slices) // 2
    processed = processed[start:start + target_slices]
```

**Very smart part!**

**Scenario:**
```python
len(processed) = 300  # After removing outliers
target_slices = 128   # Want only 128
```

**Step 1: Calculate start**
```python
start = (300 - 128) // 2 = 86
```

**Step 2: Select**
```python
processed = processed[86:214]  # 128 slices from middle
```

**Visualization:**
```
Original (300 slices):
[0...85, 86...213, 214...299]
         â†‘         â†‘
      Selected (128)
      Middle (86 from start, 86 from end)
```

**Why middle?**

**Reasons:**
1. **Most information:** Middle has full brain structure
2. **Best quality:** Clearest, fewest artifacts
3. **Includes tumor:** Most tumors in middle

**Comparison with old function:**

| Feature | Old | Filtered |
|---------|-----|----------|
| **Outliers** | Keeps | Removes âœ… |
| **Count** | All (~400) | Fixed (128) âœ… |
| **Quality** | Variable | High âœ… |
| **Size** | ~100 MB | ~32 MB âœ… |
| **Speed** | Slower | Faster âœ… |


# ğŸ“Š ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¹Ø§Ø´Ø± ÙˆØ§Ù„Ø£Ø®ÙŠØ± | Continuation Part 10 (Final)

---

### âœ… Cell 84-92: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØµÙÙŠØ© | Testing Filtering

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 84-85: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©

**Cell 84:**
```python
modality_path = "/kaggle/input/.../train/00000/FLAIR"
volume = get_filtered_modality_volume(modality_path=modality_path, visualize=True, size=(256, 256))
```

**Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ØŸ**

```python
# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
dicom_files = glob.glob(".../FLAIR/*.dcm")
# 400 Ù…Ù„Ù

# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© + Ø§Ù„ØªØµÙÙŠØ©
processed = []
for file in dicom_files:  # 400 ØªÙƒØ±Ø§Ø±
    image = load_dicom(file, visualize=True)
    image = crop_image(image)
    
    # Ø§Ù„ØªØµÙÙŠØ©:
    if np.mean(image) < 0.01:
        continue  # ØªØ®Ø·ÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©
    
    image = resize_image(image, (256, 256))
    processed.append(image)

# Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø©:
# processed ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ~300 ØµÙˆØ±Ø© Ø¬ÙŠØ¯Ø© ÙÙ‚Ø·

# Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ø®ØªÙŠØ§Ø± 128 Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ
start = (300 - 128) // 2  # 86
processed = processed[86:214]  # 128 ØµÙˆØ±Ø©

# Ø§Ù„Ø®Ø·ÙˆØ© 4: Stack & Normalize
volume = np.stack(processed, axis=-1)  # (256, 256, 128)
volume = normalize_volume(volume)
```

**Cell 85:**
```python
volume.shape
```

**Ø§Ù„Ù…Ø®Ø±Ø¬:**
```python
(256, 256, 128)
```

**Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:**

**Ø¨Ø¯ÙˆÙ† ØªØµÙÙŠØ© (Cell 77):**
```python
(256, 256, 400)
```

**Ù…Ø¹ ØªØµÙÙŠØ© (Cell 85):**
```python
(256, 256, 128)
```

**Ø§Ù„ÙØ±Ù‚:**
```python
# Ø§Ù„Ø­Ø¬Ù…:
400 â†’ 128 slices  (ØªÙ‚Ù„ÙŠÙ„ 68%)

# Ø§Ù„Ø°Ø§ÙƒØ±Ø©:
100 MB â†’ 32 MB  (ØªÙˆÙÙŠØ± 68%)

# Ø§Ù„Ø¬ÙˆØ¯Ø©:
Mixed â†’ High  (ÙÙ‚Ø· Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¬ÙŠØ¯Ø©) âœ…
```

#### Cell 86: Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²

```markdown
**WOW!! We from 400 slices we get just 86 slices!
There are 400-86= 314 outlier images in just patient 0000 with FLAIR scan!**
```

**âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©:** Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨!

**Ø§Ù„ØªØµØ­ÙŠØ­:**
```python
# Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ:
target_slices = 128  (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)

# Ø§Ù„Ù†ØªÙŠØ¬Ø©:
volume.shape = (256, 256, 128)

# Ø§Ù„ØµÙˆØ§Ø¨:
"We from 400 slices we get just 128 slices!"
"There are 400-128 = 272 outlier/excluded images"

# Ù„ÙƒÙ† Ø§Ù„ÙÙƒØ±Ø© ØµØ­ÙŠØ­Ø©: Ø­Ø°ÙÙ†Ø§ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©!
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ­ÙŠØ­:**

```python
# Ø§Ù„Ø£ØµÙ„ÙŠ: 400 slices
# Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©: ~300 slices  (Ø­Ø°Ù ~100)
# Ø¨Ø¹Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù†ØªØµÙ: 128 slices  (Ø­Ø°Ù ~172 Ø¥Ø¶Ø§ÙÙŠØ©)

# Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ø°ÙˆÙ:
100 (dark) + 172 (edges) = 272 slice

# Ø§Ù„Ù†Ø³Ø¨Ø©:
272 / 400 = 68% Ù…Ø­Ø°ÙˆÙ
128 / 400 = 32% Ù…ÙØ¨Ù‚Ù‰
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ø°Ù‡Ù„ØŸ**

**1. Ø¬ÙˆØ¯Ø© Ø£Ø¹Ù„Ù‰:**
```python
# Ù‚Ø¨Ù„:
[dark, dark, brain, brain, ..., dark, dark]
 â†‘                                      â†‘
 outliers                           outliers

# Ø¨Ø¹Ø¯:
[brain, brain, brain, brain, brain, brain]
 â†‘                                      â†‘
 ÙƒÙ„ Ø§Ù„ØµÙˆØ± Ù…ÙÙŠØ¯Ø© âœ…
```

**2. ÙƒÙØ§Ø¡Ø© Ø£Ø¹Ù„Ù‰:**
```python
# Ø§Ù„ØªØ¯Ø±ÙŠØ¨:
400 slices Ã— 100 epochs = 40,000 forward passes
128 slices Ã— 100 epochs = 12,800 forward passes
# ØªÙˆÙÙŠØ±: 68% Ù…Ù† Ø§Ù„ÙˆÙ‚Øª!
```

**3. Ø°Ø§ÙƒØ±Ø© Ø£Ù‚Ù„:**
```python
# Batch size = 4 patients:
Old: 4 Ã— 100 MB = 400 MB
New: 4 Ã— 32 MB = 128 MB
# ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… batch size Ø£ÙƒØ¨Ø±!
```

#### Cell 87: Ø¹Ø±Ø¶ Ø¢Ø®Ø± slice

```python
visualize_modality_volume(volume, 85)
```

**Ù„Ù…Ø§Ø°Ø§ slice 85ØŸ**
```python
volume.shape = (256, 256, 128)
# Ø¢Ø®Ø± slice ØµØ§Ù„Ø­: 127 (0-indexed)

# slice 85:
85 / 128 â‰ˆ 66%  â† ÙÙŠ Ø§Ù„Ø«Ù„Ø« Ø§Ù„Ø£Ø®ÙŠØ±
```

**Ù…Ø§ Ù†ØªÙˆÙ‚Ø¹ Ø±Ø¤ÙŠØªÙ‡:**
- Ø¯Ù…Ø§Øº ÙˆØ§Ø¶Ø­ (Ù„Ø£Ù† ÙƒÙ„ Ø§Ù„Ù€ slices Ø§Ù„Ø¢Ù† Ø¬ÙŠØ¯Ø©)
- Ù‚Ø¯ ÙŠÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³ÙÙ„ÙŠ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
- ØªØ´Ø±ÙŠØ­ Ù…Ø®ØªÙ„Ù Ø¹Ù† slice 20 Ø£Ùˆ 50

#### Cell 88-91: Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ù…Ø±ÙŠØ¶ Ø¢Ø®Ø±

**Cell 88:**
```python
modality_path = "/kaggle/input/.../train/00003/FLAIR"
volume = get_filtered_modality_volume(modality_path=modality_path, visualize=True, size=(256, 256))
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø®ØªØ¨Ø± Ù…Ø±ÙŠØ¶ Ø¢Ø®Ø±ØŸ**

**Ø§Ù„Ø£Ù‡Ø¯Ø§Ù:**
1. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚:** 
   - Ù‡Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„Ù…Ø±Ø¶Ù‰ Ù…Ø®ØªÙ„ÙÙŠÙ†ØŸ

2. **Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯:**
   - Ø§Ù„Ù…Ø±ÙŠØ¶ 00000: 400 â†’ 128
   - Ø§Ù„Ù…Ø±ÙŠØ¶ 00003: ØŸ â†’ 128

3. **Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„:**
   - Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù…Ø±Ø¶Ù‰ Ø¨Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„ØµÙˆØ±ØŸ

**Cell 89:**
```python
volume.shape
```

**Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**
```python
(256, 256, 128)
```

**âš ï¸ Ù…Ù‡Ù…:** Ù†ÙØ³ Ø§Ù„Ø´ÙƒÙ„ Ø¯Ø§Ø¦Ù…Ø§Ù‹!
- Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ©
- `target_slices=128` ÙŠØ¶Ù…Ù† Ø°Ù„Ùƒ
- **Standardization** ÙƒØ§Ù…Ù„Ø© âœ…

#### Cell 90: Ù…Ù„Ø§Ø­Ø¸Ø© Ø¹Ù† Ø§Ù„Ø£ØµÙ„

```markdown
Before filteration, it was (256, 256, 129)
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**

**Ù‚Ø¨Ù„ Ø§Ù„ØªØµÙÙŠØ©:**
```python
# Ø§Ù„Ù…Ø±ÙŠØ¶ 00003 ÙƒØ§Ù† Ù„Ø¯ÙŠÙ‡:
Original: 129 slices (Ø¨Ø¹Ø¯ Ø­Ø°Ù Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©)
# Ø¹Ø¯Ø¯ ÙØ±Ø¯ÙŠØŒ Ù‚Ø±ÙŠØ¨ Ù…Ù† 128
```

**Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©:**
```python
# Ø§Ø®ØªÙŠØ§Ø± 128 Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ:
start = (129 - 128) // 2 = 0
processed = processed[0:128]
# Ø£Ø®Ø° Ø£ÙˆÙ„ 128 ÙÙ‚Ø· (Ø­Ø°Ù Ø¢Ø®Ø± ÙˆØ§Ø­Ø¯Ø©)
```

**Ø§Ù„ÙØ§Ø¦Ø¯Ø©:**
- Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ù‚Ø±ÙŠØ¨ Ù…Ù† target_slices
- Ù†ÙˆØ­Ø¯Ù‡ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ø¥Ù„Ù‰ 128 âœ…

#### Cell 91: Ø¹Ø±Ø¶ slice

```python
visualize_modality_volume(volume, 35)
```

**Ø§Ø®ØªÙŠØ§Ø± slice 35:**
```python
35 / 128 â‰ˆ 27%  â† ÙÙŠ Ø§Ù„Ø«Ù„Ø« Ø§Ù„Ø£ÙˆÙ„
```

**Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:**
- Ø§Ù„Ù…Ø±ÙŠØ¶ 00000, slice 85: Ø«Ù„Ø« Ø£Ø®ÙŠØ±
- Ø§Ù„Ù…Ø±ÙŠØ¶ 00003, slice 35: Ø«Ù„Ø« Ø£ÙˆÙ„
- Ù†Ø±Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº

---

### âœ… Cell 92-96: Data Augmentation | ØªÙƒØ¨ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Cell 92: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªÙƒØ¨ÙŠØ±

```markdown
Let's apply augmentation
```

**Ù…Ø§ Ù‡Ùˆ Data AugmentationØŸ**

**Ø§Ù„ØªØ¹Ø±ÙŠÙ:**
- Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø® Ù…Ø¹Ø¯Ù„Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
- **Ù„Ø§** Ù†ØºÙŠØ± Ø§Ù„Ù…Ø¹Ù†Ù‰ (Ø§Ù„Ø¯Ù…Ø§Øº ÙŠØ¨Ù‚Ù‰ Ø¯Ù…Ø§Øº)
- **Ù†ØºÙŠØ±** Ø§Ù„Ø´ÙƒÙ„/Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù‚Ù„ÙŠÙ„Ø§Ù‹

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬Ù‡ØŸ**

**Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:**
```python
# Ù„Ø¯ÙŠÙ†Ø§ ÙÙ‚Ø·:
582 patients  â† Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„!

# Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ØªØ­ØªØ§Ø¬:
Thousands or millions of examples

# Ø§Ù„Ø­Ù„:
Augmentation â†’ Ù†Ù†Ø´Ø¦ "Ù…Ø±Ø¶Ù‰ Ø§ÙØªØ±Ø§Ø¶ÙŠÙŠÙ†"
```

**Ø£Ù†ÙˆØ§Ø¹ Augmentation Ø´Ø§Ø¦Ø¹Ø©:**

**1. Geometric Transformations:**
```python
# Ø§Ù„ØªØ¯ÙˆÙŠØ± (Rotation):
Original â†’ Rotate 90Â° â†’ Rotate 180Â° â†’ Rotate 270Â°

# Ø§Ù„Ø§Ù†Ø¹ÙƒØ§Ø³ (Flipping):
Original â†’ Flip horizontal â†’ Flip vertical

# Ø§Ù„ØªÙƒØ¨ÙŠØ±/Ø§Ù„ØªØµØºÙŠØ± (Scaling):
Original â†’ Zoom in 1.1Ã— â†’ Zoom out 0.9Ã—

# Ø§Ù„Ø¥Ø²Ø§Ø­Ø© (Translation):
Original â†’ Shift left â†’ Shift right
```

**2. Intensity Transformations:**
```python
# ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·ÙˆØ¹ (Brightness):
Original â†’ Brighter â†’ Darker

# ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Contrast):
Original â†’ Higher contrast â†’ Lower contrast

# Ø¥Ø¶Ø§ÙØ© Ø¶ÙˆØ¶Ø§Ø¡ (Noise):
Original â†’ + Gaussian noise
```

**Ù…Ø§ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù€ MRIØŸ**

**âœ… Ø¢Ù…Ù†:**
- Rotation (90Â°, 180Â°, 270Â°)
- Flipping (horizontal)

**âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø¯ÙˆØ¯:**
- Rotation (Ø²ÙˆØ§ÙŠØ§ ØµØºÙŠØ±Ø©: Â±5Â°)
- Translation (Ø¥Ø²Ø§Ø­Ø§Øª ØµØºÙŠØ±Ø©)

**âŒ ØºÙŠØ± Ø¢Ù…Ù†:**
- Brightness (Ù‚Ø¯ ÙŠØºÙŠØ± Ø§Ù„ØªØ´Ø®ÙŠØµ!)
- Heavy distortion (ÙŠØ´ÙˆÙ‡ Ø§Ù„ØªØ´Ø±ÙŠØ­)

**Ù„Ù…Ø§Ø°Ø§ØŸ**
- MRI Ø­Ø³Ø§Ø³Ø© Ø¬Ø¯Ø§Ù‹
- ØªÙØ§ØµÙŠÙ„ ØµØºÙŠØ±Ø© Ù…Ù‡Ù…Ø©
- Ù†Ø±ÙŠØ¯ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø·Ø¨ÙŠØ©

#### Cell 93: Ø¯Ø§Ù„Ø© augment_image

```python
def augment_image(image):
    rot_choices = [
        0,
        cv2.ROTATE_90_CLOCKWISE,
        cv2.ROTATE_90_COUNTERCLOCKWISE,
        cv2.ROTATE_180,
    ]
    rotate = random.randint(0, len(rot_choices) - 1)
    image = cv2.rotate(image, rot_choices[rotate])
    return image
```

**ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚:**

**Ø§Ù„Ø³Ø·Ø± 2-7: Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ¯ÙˆÙŠØ±**

```python
rot_choices = [
    0,                              # Ù„Ø§ ØªØ¯ÙˆÙŠØ±
    cv2.ROTATE_90_CLOCKWISE,        # 90Â° ÙŠÙ…ÙŠÙ†
    cv2.ROTATE_90_COUNTERCLOCKWISE, # 90Â° ÙŠØ³Ø§Ø±
    cv2.ROTATE_180,                 # 180Â°
]
```

**ØªØµÙˆØ±:**

```
Original (0):        90Â° CW:          90Â° CCW:         180Â°:
â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”
â”‚  T  â”‚             â”‚ â—„â”€â”¤ â”‚          â”‚ â”‚â”€â–º â”‚          â”‚  â”´  â”‚
â”‚  â”‚  â”‚             â”‚  T  â”‚          â”‚  T  â”‚          â”‚  â”‚  â”‚
â”‚  â–¼  â”‚             â”‚  â”‚  â”‚          â”‚  â”‚  â”‚          â”‚  â–²  â”‚
â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜
```

**Ø§Ù„Ø³Ø·Ø± 8: Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ**

```python
rotate = random.randint(0, len(rot_choices) - 1)
```

**ØªØ­Ù„ÙŠÙ„:**
```python
len(rot_choices) = 4

random.randint(0, 3)
# Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù…ÙƒÙ†Ø©: 0, 1, 2, 3
# Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:
# 0 â†’ 25% (no rotation)
# 1 â†’ 25% (90Â° CW)
# 2 â†’ 25% (90Â° CCW)
# 3 â†’ 25% (180Â°)
```

**Ù„Ù…Ø§Ø°Ø§ Ø¹Ø´ÙˆØ§Ø¦ÙŠØŸ**
- Ù†Ø±ÙŠØ¯ ØªÙ†ÙˆØ¹
- ÙƒÙ„ Ù…Ø±Ø© Ù†Ù‚Ø±Ø£ Ø§Ù„ØµÙˆØ±Ø©ØŒ ØªØ¯ÙˆÙŠØ± Ù…Ø®ØªÙ„Ù
- ÙŠØ­Ø§ÙƒÙŠ Ø±Ø¤ÙŠØ© Ø§Ù„Ø¯Ù…Ø§Øº Ù…Ù† Ø²ÙˆØ§ÙŠØ§ Ù…Ø®ØªÙ„ÙØ©

**Ø§Ù„Ø³Ø·Ø± 9: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚**

```python
image = cv2.rotate(image, rot_choices[rotate])
```

**Ø£Ù…Ø«Ù„Ø©:**

**Ù…Ø«Ø§Ù„ 1: rotate = 0**
```python
rot_choices[0] = 0
cv2.rotate(image, 0)  â† Ù„Ø§ Ø´ÙŠØ¡ ÙŠØ­Ø¯Ø«
# Ø§Ù„ØµÙˆØ±Ø© ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ
```

**âš ï¸ Ø®Ø·Ø£ Ù…Ø­ØªÙ…Ù„:**
```python
# ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ rot_choices[0] = 0 Ù„ÙŠØ³ rotation flag ØµØ§Ù„Ø­!
# cv2.rotate() ÙŠØªÙˆÙ‚Ø¹:
# - cv2.ROTATE_90_CLOCKWISE
# - cv2.ROTATE_90_COUNTERCLOCKWISE
# - cv2.ROTATE_180

# âœ… Ø§Ù„ØµÙˆØ§Ø¨:
if rotate == 0:
    pass  # no rotation
else:
    image = cv2.rotate(image, rot_choices[rotate])
```

**Ù…Ø«Ø§Ù„ 2: rotate = 1**
```python
rot_choices[1] = cv2.ROTATE_90_CLOCKWISE
cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
# Ø§Ù„ØµÙˆØ±Ø© ØªØ¯ÙˆØ± 90Â° ÙŠÙ…ÙŠÙ†
```

**ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ cv2.ROTATE_90_CLOCKWISEØŸ**

```python
# Original:
Original = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]

# 90Â° Clockwise:
Rotated = [
    [7, 4, 1],  â† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ Ø£ØµØ¨Ø­ Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„ (Ù…Ù‚Ù„ÙˆØ¨)
    [8, 5, 2],  â† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø«Ø§Ù†ÙŠ Ø£ØµØ¨Ø­ Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ
    [9, 6, 3]   â† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø«Ø§Ù„Ø« Ø£ØµØ¨Ø­ Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù„Ø«
]
```

**Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©:**
```python
# For 90Â° CW:
new[i][j] = old[n-1-j][i]
# where n = size
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„ØªØ¯ÙˆÙŠØ± Ø¢Ù…Ù† Ø·Ø¨ÙŠØ§Ù‹ØŸ**

**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨:**
1. **Ø§Ù„ØªÙ…Ø§Ø«Ù„ Ø§Ù„ØªØ´Ø±ÙŠØ­ÙŠ:**
   - Ø§Ù„Ø¯Ù…Ø§Øº Ù…ØªÙ…Ø§Ø«Ù„ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
   - Ø§Ù„ØªØ¯ÙˆÙŠØ± 180Â° Ù„Ø§ ÙŠØºÙŠØ± Ø§Ù„ØªØ´Ø±ÙŠØ­ ÙƒØ«ÙŠØ±Ø§Ù‹

2. **Ø§Ø®ØªÙ„Ø§Ù ÙˆØ¶Ø¹ÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¶:**
   - ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù‚Ø¯ ÙŠÙ…ÙŠÙ„ÙˆÙ† Ù‚Ù„ÙŠÙ„Ø§Ù‹
   - Ø§Ù„ØªØ¯ÙˆÙŠØ± ÙŠØ­Ø§ÙƒÙŠ Ù‡Ø°Ø§

3. **Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:**
   - Ø§Ù„ÙˆØ±Ù… ÙŠØ¨Ù‚Ù‰ ÙˆØ±Ù…
   - Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØªØ´Ø±ÙŠØ­ÙŠ ÙŠØ¨Ù‚Ù‰ Ù†ÙØ³Ù‡
   - ÙÙ‚Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡ ÙŠØªØºÙŠØ±

**âš ï¸ Ù…Ø­Ø§Ø°ÙŠØ±:**

**1. Ø¹Ø¯Ù… ØªØ¯ÙˆÙŠØ± Ø²ÙˆØ§ÙŠØ§ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©:**
```python
# Ø®Ø·Ø± âŒ:
angle = random.uniform(0, 360)  # Ø£ÙŠ Ø²Ø§ÙˆÙŠØ©!
# Ù‚Ø¯ ÙŠØ´ÙˆÙ‡ Ø§Ù„ØªØ´Ø±ÙŠØ­

# Ø¢Ù…Ù† âœ…:
angles = [0, 90, 180, 270]  # Ø²ÙˆØ§ÙŠØ§ Ù‚Ø§Ø¦Ù…Ø© ÙÙ‚Ø·
```

**2. Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù„ØªØ³Ù…ÙŠØ§Øª (Labels):**
```python
# Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ bounding boxes Ø£Ùˆ masks:
# ÙŠØ¬Ø¨ ØªØ¯ÙˆÙŠØ±Ù‡Ø§ Ø£ÙŠØ¶Ø§Ù‹!

image = rotate(image, 90)
mask = rotate(mask, 90)  # âš ï¸ Ù„Ø§ ØªÙ†Ø³Ù‰!
```

**Ø§Ù„Ø³Ø·Ø± 10:**
```python
return image
```

#### Cell 94-95: Ø§Ø®ØªØ¨Ø§Ø± Augmentation

**Cell 94:**
```python
image = augment_image(volume[:,:,35])
image
```

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**

**`volume[:,:,35]`:**
```python
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ slice 35:
volume.shape = (256, 256, 128)
slice_35 = volume[:, :, 35]
# Shape: (256, 256)
```

**`augment_image(slice_35)`:**
```python
# ØªØ·Ø¨ÙŠÙ‚ ØªØ¯ÙˆÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
# ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© ØªÙÙ†ÙÙ‘Ø° Ø§Ù„Ø®Ù„ÙŠØ©ØŒ Ù†ØªÙŠØ¬Ø© Ù…Ø®ØªÙ„ÙØ©!

# Ø§Ù„Ù…Ø±Ø© 1: Ù‚Ø¯ ÙŠÙƒÙˆÙ† 0Â° (no rotation)
# Ø§Ù„Ù…Ø±Ø© 2: Ù‚Ø¯ ÙŠÙƒÙˆÙ† 90Â° CW
# Ø§Ù„Ù…Ø±Ø© 3: Ù‚Ø¯ ÙŠÙƒÙˆÙ† 180Â°
# ...
```

**Ø§Ù„Ù…Ø®Ø±Ø¬:**
```python
array([[...], [...], ...], dtype=uint8)
# Ø§Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…ÙØ¯ÙˆÙ‘Ø±Ø©
```

**Cell 95:**
```python
plt.figure(figsize=(6, 6))
plt.imshow(image, cmap='gray')
plt.axis("off")
plt.title("DICOM Image")
plt.show()
```

**Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙØ¯ÙˆÙ‘Ø±Ø©**

**Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø¨Ø­Ø« Ø¹Ù†Ù‡ØŸ**
- Ù‡Ù„ Ø§Ù„ØªØ¯ÙˆÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠØŸ
- Ù‡Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø§ Ø²Ø§Ù„Øª ÙˆØ§Ø¶Ø­Ø©ØŸ
- Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø±ÙŠØ­ØŸ

**Ø§Ù„ØªØ¬Ø±Ø¨Ø©:**
Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Cell 94-95 Ø¹Ø¯Ø© Ù…Ø±Ø§Øª:
```python
# Ø§Ù„Ù…Ø±Ø© 1:
# Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ…Ø§ Ù‡ÙŠ (0Â°)

# Ø§Ù„Ù…Ø±Ø© 2:
# Ø§Ù„ØµÙˆØ±Ø© Ù…ÙØ¯ÙˆÙ‘Ø±Ø© 90Â° ÙŠÙ…ÙŠÙ†

# Ø§Ù„Ù…Ø±Ø© 3:
# Ø§Ù„ØµÙˆØ±Ø© Ù…ÙØ¯ÙˆÙ‘Ø±Ø© 180Â° (Ù…Ù‚Ù„ÙˆØ¨Ø©)

# ÙƒÙ„ Ù…Ø±Ø©: ØµÙˆØ±Ø© "Ø¬Ø¯ÙŠØ¯Ø©"! âœ…
```

#### Cell 96: ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù†Ø¬Ø§Ø­

```markdown
Great!
```

**Ù…Ù„Ø®Øµ Augmentation:**

```python
# Ø¨Ø¯ÙˆÙ† Augmentation:
582 patients Ã— 1 orientation = 582 examples

# Ù…Ø¹ Augmentation (4 rotations):
582 patients Ã— 4 orientations = 2,328 examples

# Ø²ÙŠØ§Ø¯Ø© 4Ã— ÙÙŠ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª! âœ…
```

**Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù…Ù„ÙŠ:**

```python
# ÙÙŠ training loop:
for epoch in epochs:
    for patient in patients:
        volume = load_patient(patient)
        
        for slice in volume:
            # ÙƒÙ„ slice ÙŠÙØ¯ÙˆÙ‘Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹:
            augmented = augment_image(slice)
            
            # Ù†ÙØ¯Ø±Ù‘Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù€ augmented image:
            loss = train_step(augmented, label)
```

**Ø§Ù„ÙÙˆØ§Ø¦Ø¯:**
âœ… Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª â†’ Ù†Ù…ÙˆØ°Ø¬ Ø£Ù‚ÙˆÙ‰
âœ… ØªÙ‚Ù„ÙŠÙ„ Overfitting â†’ ØªØ¹Ù…ÙŠÙ… Ø£ÙØ¶Ù„
âœ… Ù…Ø±ÙˆÙ†Ø© Ø£ÙƒØ¨Ø± â†’ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø²ÙˆØ§ÙŠØ§ Ù…Ø®ØªÙ„ÙØ©

---

## ğŸ¯ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ | Comprehensive Final Summary

### Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ù€ Notebook | Complete Notebook Structure

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

#### Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ© (Logical Layers):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LAYER 1: DATA LOADING                  â”‚
â”‚  - Read CSV labels                                      â”‚
â”‚  - Clean corrupted patient IDs                          â”‚
â”‚  - Explore data distribution                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                LAYER 2: VISUALIZATION                   â”‚
â”‚  - Display single images                                â”‚
â”‚  - Create animations for volumes                        â”‚
â”‚  - Analyze pixel intensity distributions                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LAYER 3: DATA EXPLORATION                  â”‚
â”‚  - Count slices per modality                            â”‚
â”‚  - Analyze image shapes across dataset                  â”‚
â”‚  - Identify common patterns and outliers                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LAYER 4: PREPROCESSING                    â”‚
â”‚  Step 1: load_dicom() â†’ Read DICOM, normalize [0,1]   â”‚
â”‚  Step 2: crop_image() â†’ Remove black margins           â”‚
â”‚  Step 3: resize_image() â†’ Standardize to 256Ã—256       â”‚
â”‚  Step 4: Filter dark slices â†’ Remove outliers          â”‚
â”‚  Step 5: Select middle slices â†’ Keep best 128          â”‚
â”‚  Step 6: normalize_volume() â†’ Z-score (mean=0, std=1)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LAYER 5: AUGMENTATION                      â”‚
â”‚  - Random rotation (0Â°, 90Â°, 180Â°, 270Â°)                â”‚
â”‚  - Increase data variety 4Ã—                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                  Ready for Modeling!
```

#### Ø¹Ø¯Ø¯ Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Processing Stages):

**6 Ù…Ø±Ø§Ø­Ù„ Ø±Ø¦ÙŠØ³ÙŠØ©:**

1. **Loading:** DICOM â†’ numpy array
2. **Normalization:** [0-4095] â†’ [0-1]
3. **Cropping:** Remove background
4. **Resizing:** â†’ 256Ã—256
5. **Filtering:** Remove dark/edge slices
6. **Volume Normalization:** Z-score

**âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù€ Notebook!**
- Ù‡Ø°Ø§ Notebook ØªØ­Ø¶ÙŠØ±ÙŠ (preprocessing)
- ÙŠØ¬Ù‡Ø² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
- **Ù„Ø§ ØªÙˆØ¬Ø¯ Ø·Ø¨Ù‚Ø§Øª ØªØ¹Ù„Ù… (learning layers)**

### Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù†Ù‡Ø§ Ø§Ù„ÙƒÙˆØ¯ | Questions the Code Answers

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

**1. "Ù…Ø§ Ø´ÙƒÙ„ ÙˆØ­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ"**
- 582 Ù…Ø±ÙŠØ¶ (Ø¨Ø¹Ø¯ Ø­Ø°Ù 3 ÙØ§Ø³Ø¯ÙŠÙ†)
- 4 Ø£Ù†ÙˆØ§Ø¹ MRI Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶
- ~130 ØµÙˆØ±Ø© Ù„ÙƒÙ„ Ù†ÙˆØ¹
- Ù…Ø¹Ø¸Ù…Ù‡Ø§ 512Ã—512

**2. "Ù‡Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø©ØŸ"**
- Ù†Ø¹Ù…! 291 MGMT=0, 291 MGMT=1 âœ…

**3. "Ù…Ø§ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…ØŸ"**
- 75% Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡
- ØµÙˆØ± Ø¨Ø£Ø­Ø¬Ø§Ù… Ù…Ø®ØªÙ„ÙØ©
- ~68% Ù…Ù† Ø§Ù„Ù€ slices outliers

**4. "ÙƒÙŠÙ Ù†ÙˆØ­Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ"**
- Crop â†’ Resize â†’ Normalize âœ…

**5. "ÙƒÙŠÙ Ù†Ø²ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ"**
- Augmentation (rotation) â†’ 4Ã— Ø²ÙŠØ§Ø¯Ø© âœ…

### Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© | Common Errors and Pitfalls

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

**1. Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØ±Ù…ÙŠØ²:**

```python
# âŒ Ø®Ø·Ø£: Ù†Ø³ÙŠØ§Ù† astype(float32)
data = dicom.pixel_array  # uint16
data = data / np.max(data)  # integer division!

# âœ… ØµØ­ÙŠØ­:
data = dicom.pixel_array.astype(np.float32)
data = data / np.max(data)
```

**2. Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:**

```python
# âŒ Ø®Ø·Ø£: Ø®Ù„Ø· (H,W) Ùˆ (W,H)
img.shape = (280, 260)  # (height, width)
cv2.resize(img, (280, 260))  # Wrong! Will be (260, 280)

# âœ… ØµØ­ÙŠØ­:
cv2.resize(img, (256, 256))  # Fixed size
```

**3. Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠØ¹:**

```python
# âŒ Ø®Ø·Ø£: Ø¹Ø¯Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† std=0
volume = (volume - mean) / std  # Ù‚Ø¯ ÙŠÙƒÙˆÙ† std=0!

# âœ… ØµØ­ÙŠØ­:
if std > 0:
    volume = (volume - mean) / std
```

**4. ØªØ³Ø±Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Leakage):**

```python
# âŒ Ø®Ø·Ø±: ØªØ·Ø¨ÙŠØ¹ global Ù‚Ø¨Ù„ train/test split
all_data_normalized = normalize(all_data)  # Leakage!
train, test = split(all_data_normalized)

# âœ… ØµØ­ÙŠØ­:
train, test = split(all_data)
train_normalized = normalize(train)  # Only on train
test_normalized = normalize(test, using_train_stats)
```

**5. Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:**

```python
# âŒ Ø®Ø·Ø±: ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
all_volumes = []
for patient in 582_patients:
    volume = load_all_slices(patient)  # 100 MB each
    all_volumes.append(volume)
# Total: 582 Ã— 100 MB = 58 GB! ğŸ’¥

# âœ… ØµØ­ÙŠØ­: On-the-fly processing
for patient in patients:
    volume = load_and_process(patient)
    train_batch(volume)
    del volume  # Free memory
```

### Ø´Ø±Ø­ Ø´ÙÙ‡ÙŠ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…ÙŠ | Verbal Explanation for Presentation

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

> "Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙŠØ­Ù„Ù„ ÙˆÙŠØ¹Ø§Ù„Ø¬ ØµÙˆØ± Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ Ù„Ù„Ø¯Ù…Ø§Øº Ù…Ù† Ù…Ø³Ø§Ø¨Ù‚Ø© RSNA. Ù„Ø¯ÙŠÙ†Ø§ 582 Ù…Ø±ÙŠØ¶ØŒ ÙƒÙ„ Ù…Ø±ÙŠØ¶ Ù„Ø¯ÙŠÙ‡ 4 Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„ØµÙˆØ±. Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡ÙŠ Ø£Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ø®Ø§Ù… ØºÙŠØ± Ù…ÙˆØ­Ø¯Ø©: Ø£Ø­Ø¬Ø§Ù… Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆØ¬ÙˆØ¯ Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡ ÙƒØ¨ÙŠØ±Ø©ØŒ Ùˆ68% Ù…Ù† Ø§Ù„ØµÙˆØ± outliers.
>
> Ù‚Ù…Ù†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ pipeline Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† 6 Ù…Ø±Ø§Ø­Ù„: Ø£ÙˆÙ„Ø§Ù‹ Ù†Ù‚Ø±Ø£ DICOM ÙˆÙ†Ø·Ø¨Ù‘Ø¹ Ø§Ù„Ù‚ÙŠÙ…. Ø«Ø§Ù†ÙŠØ§Ù‹ Ù†Ù‚Øµ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… bounding box Ø°ÙƒÙŠ. Ø«Ø§Ù„Ø«Ø§Ù‹ Ù†ÙˆØ­Ø¯ Ø§Ù„Ø­Ø¬Ù… Ø¥Ù„Ù‰ 256Ã—256. Ø±Ø§Ø¨Ø¹Ø§Ù‹ Ù†ØµÙÙ‘ÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©. Ø®Ø§Ù…Ø³Ø§Ù‹ Ù†Ø®ØªØ§Ø± Ø£ÙØ¶Ù„ 128 slice Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ. Ø£Ø®ÙŠØ±Ø§Ù‹ Ù†Ø·Ø¨Ù‘Ø¹ Ø§Ù„Ù€ volume ÙƒØ§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Z-score.
>
> Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù…Ù† 400 slice Ø¨Ø¬ÙˆØ¯Ø© Ù…ØªÙØ§ÙˆØªØ© ÙˆØ­Ø¬Ù… 100 MBØŒ Ø¥Ù„Ù‰ 128 slice Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ­Ø¬Ù… 32 MB. Ø«Ù… Ù†Ø·Ø¨Ù‚ augmentation Ø¨Ø§Ù„ØªØ¯ÙˆÙŠØ± Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙ†ÙˆØ¹ 4 Ø£Ø¶Ø¹Ø§Ù.
>
> Ù‡Ø°Ø§ Ø§Ù„Ù€ notebook ØªØ­Ø¶ÙŠØ±ÙŠ ÙÙ‚Ø· - Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„ÙŠ. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¢Ù† Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù†Ù…Ø°Ø¬Ø©."

**In English:**

> "This project analyzes and processes brain MRI images from the RSNA competition. We have 582 patients, each with 4 MRI types. The main problem is that raw images are non-standardized: different sizes, large black backgrounds, and 68% outlier slices.
>
> We built a 6-stage processing pipeline: First, read DICOM and normalize values. Second, crop black background using smart bounding box. Third, standardize size to 256Ã—256. Fourth, filter dark slices. Fifth, select best 128 slices from middle. Finally, normalize entire volume using Z-score.
>
> Result: From 400 slices with variable quality and 100 MB size, to 128 high-quality slices and 32 MB size. Then apply rotation augmentation to increase variety 4Ã—.
>
> This notebook is preparatory only - contains no ML model. Data is now ready for modeling."

### 5-10 Ø£Ø³Ø¦Ù„Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø© Ù…Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª | 5-10 Technical Questions with Answers

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

**Ø³1: Ù„Ù…Ø§Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª INTER_AREA ÙÙŠ resize Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† INTER_LINEARØŸ**
**Ø¬:** INTER_AREA Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„ØªØµØºÙŠØ± (downsampling) Ù„Ø£Ù†Ù‡ ÙŠØ£Ø®Ø° Ù…ØªÙˆØ³Ø· ÙƒÙ„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©ØŒ Ø¨ÙŠÙ†Ù…Ø§ INTER_LINEAR ÙŠØ£Ø®Ø° ÙÙ‚Ø· Ø¹ÙŠÙ†Ø© Ø®Ø·ÙŠØ©. Ù‡Ø°Ø§ ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙŠÙ‚Ù„Ù„ Ø§Ù„Ù€ aliasing.

**Ø³2: Ù„Ù…Ø§Ø°Ø§ ØªØ·Ø¨Ù‘Ø¹ Ø§Ù„Ù€ volume ÙƒØ§Ù…Ù„Ø§Ù‹ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒÙ„ ØµÙˆØ±Ø© Ø¨Ù…ÙØ±Ø¯Ù‡Ø§ØŸ**
**Ø¬:** Per-volume normalization ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ù„Ù„Ø³Ø·ÙˆØ¹ Ø¨ÙŠÙ† Ø§Ù„Ù€ slices. Ø¥Ø°Ø§ Ø·Ø¨Ù‘Ø¹Ù†Ø§ ÙƒÙ„ ØµÙˆØ±Ø© Ø¨Ù…ÙØ±Ø¯Ù‡Ø§ØŒ slice Ø¯Ø§ÙƒÙ†Ø© Ùˆslice ÙØ§ØªØ­Ø© Ø³ØªØµØ¨Ø­Ø§Ù† Ù…ØªØ´Ø§Ø¨Ù‡ØªÙŠÙ†ØŒ Ù…Ù…Ø§ ÙŠÙÙ‚Ø¯Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø©.

**Ø³3: ÙƒÙŠÙ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ± ÙÙŠ normalize_volumeØŸ**
**Ø¬:** Ù†Ø³ØªØ®Ø¯Ù… Ø´Ø±Ø· `if std > 0` Ù‚Ø¨Ù„ Ø§Ù„Ù‚Ø³Ù…Ø©. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ ØµÙØ± (ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…ØªØ³Ø§ÙˆÙŠØ©)ØŒ Ù†ØªØ®Ø·Ù‰ Ø§Ù„Ù‚Ø³Ù…Ø© ÙˆÙ†Ø±Ø¬Ø¹ Ø§Ù„Ù€ volume ÙƒÙ…Ø§ Ù‡Ùˆ.

**Ø³4: Ù„Ù…Ø§Ø°Ø§ Ø§Ø®ØªØ±Øª threshold=0.01 Ù„ÙÙ„ØªØ±Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¯Ø§ÙƒÙ†Ø©ØŸ**
**Ø¬:** Ø§Ø®ØªÙŠØ§Ø± ØªØ¬Ø±ÙŠØ¨ÙŠ (empirical). Ø¨Ø¹Ø¯ ØªØ¬Ø±Ø¨Ø© Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ©ØŒ 0.01 ÙŠØ¹Ø·ÙŠ Ø£ÙØ¶Ù„ ØªÙˆØ§Ø²Ù†: ÙŠØ­Ø°Ù Ø§Ù„ØµÙˆØ± Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ ØªÙ…Ø§Ù…Ø§Ù‹ ÙˆØ§Ù„Ø¯Ø§ÙƒÙ†Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù„ÙƒÙ† ÙŠØ¨Ù‚ÙŠ Ø§Ù„ØµÙˆØ± Ø°Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙŠØ¯ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø¯Ø§ÙƒÙ†Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹.

**Ø³5: Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø± data leakage ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ØŸ**
**Ø¬:** Ù„Ø§ØŒ Ù„Ø£Ù†Ù†Ø§ Ù„Ù… Ù†Ù‚Ø³Ù‘Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø¥Ù„Ù‰ train/test. ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© per-patientØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª global Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¶Ù‰.

**Ø³6: Ù„Ù…Ø§Ø°Ø§ Ù„Ù… ØªØ³ØªØ®Ø¯Ù… batch normalizationØŸ**
**Ø¬:** Batch normalization Ø·Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©ØŒ Ù„ÙŠØ³Øª Ø¬Ø²Ø¡ Ù…Ù† preprocessing. Ù‡Ø°Ø§ Ø§Ù„Ù€ notebook preprocessing ÙÙ‚Ø· - Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬.

**Ø³7: ÙƒÙŠÙ ØªØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ù€ cropping Ù„Ø§ ÙŠÙ‚Øµ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù† Ø§Ù„Ø¯Ù…Ø§ØºØŸ**
**Ø¬:** Ù†Ø³ØªØ®Ø¯Ù… `margin=5` Ø¨ÙƒØ³Ù„Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù€ bounding box. Ù‡Ø°Ø§ ÙŠØªØ±Ùƒ Ù…Ø³Ø§Ø­Ø© Ø£Ù…Ø§Ù† ØµØºÙŠØ±Ø© Ù„ØªØ¬Ù†Ø¨ Ù‚Øµ Ø­ÙˆØ§Ù Ø§Ù„Ø¯Ù…Ø§Øº Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø®Ø·Ø£.

**Ø³8: Ù„Ù…Ø§Ø°Ø§ ØªØ®ØªØ§Ø± 128 slice Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŸ**
**Ø¬:** 128 Ù‚ÙˆØ© 2 (2^7)ØŒ Ù…Ù…Ø§ ÙŠØ³Ù‡Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©. Ø£ÙŠØ¶Ø§Ù‹ ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯: Ù„ÙŠØ³ Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ (Ù†ÙÙ‚Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª) ÙˆÙ„Ø§ ÙƒØ«ÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ù†Ø´Ù…Ù„ outliers).

**Ø³9: Ù‡Ù„ rotation augmentation Ø¢Ù…Ù† Ø·Ø¨ÙŠØ§Ù‹ØŸ**
**Ø¬:** Ù†Ø¹Ù… Ù„Ù„Ø²ÙˆØ§ÙŠØ§ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© (90Â°, 180Â°, 270Â°). Ø§Ù„Ø¯Ù…Ø§Øº Ù…ØªÙ…Ø§Ø«Ù„ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ØŒ ÙˆØ§Ù„ØªØ¯ÙˆÙŠØ± Ù„Ø§ ÙŠØºÙŠØ± Ø§Ù„ØªØ´Ø±ÙŠØ­. Ù„ÙƒÙ† Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (Ù…Ø«Ù„ 45Â°) Ù‚Ø¯ ØªØ´ÙˆÙ‡ Ø§Ù„ØªØ´Ø±ÙŠØ­ ÙˆÙŠØ¬Ø¨ ØªØ¬Ù†Ø¨Ù‡Ø§.

**Ø³10: Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† visualize=True Ùˆ visualize=False ÙÙŠ load_dicomØŸ**
**Ø¬:** 
- `visualize=True`: ÙŠÙØ±Ø¬Ø¹ uint8 [0, 255] Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ matplotlib
- `visualize=False`: ÙŠÙØ±Ø¬Ø¹ float32 [0, 1] Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨

### Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ°ÙƒØ±Ù‡ | What to Remember

**Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**

**Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©:**
```
Raw DICOM (ØºÙŠØ± Ù…ÙˆØ­Ø¯) 
    â†’ Pipeline (6 stages) 
        â†’ Clean Data (Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ù…Ø°Ø¬Ø©)
            â†’ Augmentation (4Ã— Ø²ÙŠØ§Ø¯Ø©)
```

**Ø§Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:**
1. **Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø£Ù‡Ù… Ù…Ù† Ø§Ù„Ù†Ù…Ø°Ø¬Ø©** - 80% Ù…Ù† Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
2. **Ø§Ù„ÙÙ‡Ù… Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©** - Ø§Ø³ØªÙƒØ´Ù Ù‚Ø¨Ù„ Ø£Ù† ØªØ¹Ø§Ù„Ø¬
3. **Ø§Ù„ÙƒÙØ§Ø¡Ø© Ù…Ù‡Ù…Ø©** - on-the-fly processing ÙŠÙˆÙØ± 233 GB!
4. **Ø§Ù„Ø¬ÙˆØ¯Ø© > Ø§Ù„ÙƒÙ…ÙŠØ©** - 128 slice Ø¬ÙŠØ¯Ø© Ø£ÙØ¶Ù„ Ù…Ù† 400 Ù…Ø®ØªÙ„Ø·Ø©

**In English:**

**Big Picture:**
```
Raw DICOM (non-standardized) 
    â†’ Pipeline (6 stages) 
        â†’ Clean Data (ready for modeling)
            â†’ Augmentation (4Ã— increase)
```

**Key Lessons:**
1. **Preparation > Modeling** - 80% of work is in data
2. **Understand before processing** - Explore before you process
3. **Efficiency matters** - On-the-fly saves 233 GB!
4. **Quality > Quantity** - 128 good slices better than 400 mixed

---

## ğŸ‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© | The End

**ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„!**

Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø£Ùˆ Ù†Ù‚Ø§Ø· ØªØ±ÙŠØ¯ ØªÙˆØ¶ÙŠØ­Ø§Ù‹ Ø£Ø¹Ù…Ù‚ Ù„Ù‡Ø§ØŸ
